[{"title":"Instruction Set and CPU Structure","url":"/categories/cs/cpu/","content":"\r\n","categories":["cs"]},{"title":"Analog Integrated Circuits","url":"/categories/electronics/analog_integrate/","content":"1. MOSFET\r\n1.1 MOSFET in CMOS\r\nA typical fabrication of NMOS is shown in the figure below.\r\n\r\n\r\n\r\n\r\nSketch of NMOS Fabrication\r\n\r\nThe NMOS is grown on a p-substrate, which contains  as\r\nmajority carrier.\r\nFirst discuss cases of \r\n\r\n\r\n\r\nObviously nothing happens in the NMOS, no electric field. NMOS is\r\ncut-off and at equilibrium.\r\n\r\n\r\n\r\nPositive voltage at gate produces a vertical  from\r\n to p-sub. Some of the minority  are attracted\r\nto the surface while  are pushed away. The attracted\r\n gather near the surface of p-sub. However, the\r\n is still weak. It’s not enough to attract adequate\r\n to deplete all , hence there’s still no\r\nfree . The MOSFET is cut-off.\r\n\r\n\r\n\r\nNow  is strong enough to attract adequate\r\n to generate a channel for free . The\r\nMOSFET is on. On the surface of p-sub, the\r\n’s are fully depleted, the remaining  become\r\nfree , forming a conductive channel.\r\nThe threshold voltage of  to turn on the channel is\r\ndenoted .\r\n\r\n\r\n\r\n\r\nThree Working Region of NMOS\r\n\r\nThe threshold voltage is related with doping, material and process.\r\nGenerally,\r\n\n\n\r\n and  are determined by material and doping concentration of substrate, respectively.  is the charge of depletion zone. , gate oxide capacitance, is determined by thickness of oxide and material.\r\nPMOS has one more step than NMOS in fabrication. There’s an n-well on\r\nthe p-sub, then 2 p-semiconductor S and D.\r\n1.2. Large Signal Behavior.\r\n\r\nCut-off: .\r\n.\r\nWeak Inversion / Sub-threshold Region.\r\n, but .\r\n\r\nWith depletion region, there’s no drift current, only diffusion\r\ncurrent.\r\nDiffusion current depends on the concentration density of\r\n\n. \n\n\r\nThe gradient satisfies \n\n\r\nIt can be deduced.\r\n\n\n\r\nwhere  depends on the process.  is called\r\nthermal voltage.\r\nAt room temp. .\r\n\r\nLinear Region. , .\r\n\r\nWe can define overdrive voltage \n\n as how much\r\n exceeds .\r\nWe apply a voltage at D, generating a  gradient.\r\n\r\n\r\n\r\n\r\nLinear Region of NMOS\r\n\r\nThe charge at x cross\r\nsection is (to simplify, regard ) \n\n where\r\n is capacitance of oxide of unit area. Current is charge\r\nover time\r\n\n\n\r\n\n\n\r\nIntegrate over entire channel length \n\n\n\r\nHence \n\n\r\n\r\nSaturation Region\r\n\r\nIf  is very large, while , maybe\r\n, causing the pinch-off of the channel between\r\n and . The effective channel length may be\r\nshortened. In this case, the integral over channel length no longer has\r\nupper bound if . Let . we get\r\n\n\n\r\n1.3 Channel Length Modulation.\r\nConsidering pinch-off in saturation region.\r\n\n\n\r\n is an empirical parameter equivalent to modifying  to . \nWe assume  approximately:\r\n\n\nIn fact \n\nwhere  only depends on the process, \ncalled Early voltage.\n\r\nSince pinched-off channel is shorter relative to lower device, hence\r\nthe longer the channel, the weaker channel length modulation is.\r\n1.4 Small Signal Analysis\r\nWith a specific DC bias (large signal), the small signal analysis\r\nevaluates how much effect a small variation based on DC bias causes.\r\n\r\n\r\n\r\n\r\nLarge Signal and Small Signal\r\n\r\nThe MOSFET small signal equivalent circuit is shown below.\r\n\r\n\r\n\r\n\r\nSmall Signal Model of MOSFET\r\n\r\nUnder DC bias , ,  and\r\n, take a full differential\r\n\n\n\r\nApproximate  (small signal) with .\r\n\n\n\r\nthe three terms represents three current branches. Hence there must\r\nbe three branches between  and . The\r\nparameters are determined by the derivatives,\r\n\n\n\r\nPlug these parameters in\r\n\n\n\r\nIn saturation\r\n\n\n\r\nSo\r\n\n\n\r\nNote that . It looks like a conflict that whether\r\n is positively or negatively related to .\r\nBut note that  is a constant while  is not.\r\nSo the correct answer is\r\n\n\n\r\nSince bulk/substrate is always connected to the GND (NMOS) or VDD\r\n(PMOS), to simplify problems, we regard , so\r\n will cause no effect. But when the drain (NMOS) or source\r\n(PMOS) is not connected to GND or VDD, this term must be taken into\r\nconsideration.\r\nAlso, the impedances\r\n\n\n\r\nWe hope to have large  and large . But\r\nthey conflict each other when \r\n\n\n\r\nSo we have to balance the two parameters. Define intrinsic gain\r\n\n\n\r\nAlso\r\n\n\n\nFrom the deduction above, \nwe can have another understanding of the small signal model.\n turns the input voltage  into the current , \nand then  turns  back to voltage.\nIn fact almost all amplification circuits can be decomposed into the two steps.\n\nSo the total voltage gain is\n\n\r\nWe know the small signal voltages are approximation of differential,\r\nhence in small signal model, we only take variation into consideration.\r\nAll constant voltages (DC flatten) in large signal model are\r\nconsidered as AC-GND, including GND, VDD, bias, and so on.\r\n1.5 Complete MOSFET Small\r\nSignal Model\r\nWith high freq. input, the capacitance between different parts can no\r\nlonger be ignored.\r\n\r\n\r\n\r\n\r\nComplete Small Signal Model of MOSFET\r\n\r\n2. Single-Stage Amplifiers\r\n2.1 Insight of Amplification\r\nRecall the intrinsic gain of MOSFET small signal model. For more\r\namplification circuit, it can be expanded\r\n\n\n\r\nSo why MOSFET can amplify? For MOSFET.\r\n\n\n\r\n\n and  are always large enough. And the model tells us the two parameters are independently controlled by  and . Meanwhile, recall the curve .\n\r\n\r\n\r\n\r\n\r\nDC Resistance and AC Resistance\r\n\r\nThe resistance of large and small signals are separated. The large\r\nsignal resistance must be finite, while small signal resistance may be\r\ninfinite (in saturation, approximately). So the separation of DC and AC\r\nresistance is also an important reason to amplify.\r\nAn opposite example is resistors. Also follow the two steps, voltage\r\nto current, current to voltage. For resistors\r\n\n\n\r\nThey’re not independent, and . Finally, the gain is\r\n\n\n\r\nNo amplification.\r\n2.2 Amplifier Concepts\r\nThe voltage gain: \r\nThe current gain: \r\nThe power gain: \r\nThere are 4 types of amplifiers.\r\n\r\n\r\n\r\n\r\n4 Types of Amplifiers\r\n\r\n2.3\r\nThree configurations of MOSFET Single-Stage Amplifier.\r\nThere are 3 configurations:\r\n\r\nCommon Source: input G, output D\r\nCommon Gate: input S, output D\r\nCommon Drain: input G, output S\r\n\r\nNote that for all configurations the MOSFET should always be in\r\nsaturation region.\r\nA MOSFET is a 4-port device. Ignore the bulk, we can list the\r\nimpedance of the remaining 3 ports (relative to AC-GND)\r\n\r\nGate: , , control current source.\r\nSource:  (low impedance), , control\r\ncurrent source.\r\nDrain:  (high impedance), , can’t\r\ncontrol current source.\r\n\r\nOriginally there should be 6 types of configuration:\r\n\r\nG → S\r\n\r\nG → D\r\n\r\nS → G\r\n\r\nS → D\r\nD → G\r\nD → S\r\n\r\nNow we analyze each of them\r\n\r\n\r\n\r\ncan control,  can output, able to amplify.\r\n\r\n\r\ncan control,  can output, able to amplify.\r\n\r\n\r\ncan control, , unable to amplify.\r\n\r\n\r\ncan control, , able.\r\n\r\n\r\ncan’t control, unable.\r\n\r\n\r\ncan’t control, unable.\r\n\r\nIf the current source cannot work, the MOSFET become a resistor. We\r\nhave known a resistor can’t be used for amplification, so the current\r\nsource must be actively controlled..\r\nHence there’re only 3 configuration remaining, available for\r\namplifier. Check the small model, ( impedance relative to\r\n)\r\n\r\n\r\n\n\n\r\n\r\n\n\n\r\n\r\n\n\n\r\n\r\nAccording to Thevenin’s theorem, the G→S configuration exhibits large\r\ninput impedance and small output impedance. This results in higher\r\nvoltage division at the input terminal and greater voltage transfer to\r\nthe load. Therefore, G→S is suitable for voltage input and voltage\r\noutput applications.\r\nSimilarly, G→D configuration is appropriate for voltage input and\r\ncurrent output operation, while S→D configuration works well for current\r\ninput and current output scenarios.\r\n2.4 Common-Source Amplifier\r\nThe input is applied to gate, output is taken from D. The source is\r\nthe common reference point.\r\n\r\n\r\n\r\n\r\nCommon-Source Amplifier and AC Model\r\n\r\n\r\nDC Analysis.\r\n\r\nAt . MOSFET cut-off, . If\r\n is float (no load). Current on  is zero.\r\nHence .\r\nIncrease  over . \r\nincreases, , With  rising, \r\ngradually decreases. At this time , MOSFET is in\r\nsaturation.\r\nWhen  increases more (approaching ),\r\nthere must be a time that . The MOSFET enters linear\r\nregion.\r\n\r\n\r\n\r\n\r\nDC VTC of Common Source Amplifier\r\n\r\nThe slope reflects the intrinsic gain.\r\n\n\n\r\nFrom the figure, the slope is negative, hence .\r\nCommon-source amplifier is a inverse-phase amplifier.\r\n\r\nAC Analysis\r\n\r\nNow it comes to the small signal (AC). Replace the DC voltage sources\r\nwith GND.\r\n\r\n\r\n\r\n\r\nAC Model of Common Source\r\n\r\nBy KCL:\r\n\n\n\r\nIt gives\r\n\n\n\r\nAlso, we can also say\r\n\n\n\r\nIn practice, we assume . Apply this assumption.\r\n\n\n\r\nTo raise the gain, we can raise  or .\r\nWe know\r\n\n\n\r\n\n\n\r\nPlug in\r\n\n\n\r\nHence to raise , we can increase , or\r\njust increase , and decrease .\r\nBut trade off is necessary here. Such optimization will bring larger\r\ndevice size and capacitance, causing lower speed.\r\nIf the MOSFET works in the linear region ( too\r\nlarge).\r\n\n\n\r\nIt has no amplification function.\r\nBack to the saturation, since , the external\r\nresistance becomes a critical factor to limit the gain.\r\n\r\nMOS to replace .\r\n\r\nThis MOS is configured in the linear region.\r\n\n\n\r\n\r\n\r\n\r\n\r\nResistance Replaced by MOSFET\r\n\r\nIn process some parameters may drift. By using a PMOS to replace\r\n, the drift may occur together, which may cancel each\r\nother and the external property may remain unchanged.\r\nHowever such cancellation is not stable. So there’s another\r\nmethod.\r\n\r\n\r\n\r\n\r\nDiode Connection of MOSFET Load\r\n\r\nThe MOS to replace  is configured as “Diode\r\nconnection”. In the view of source, the impedance is\r\n\n\n\r\nSince , the MOSFET works in saturation region. The\r\ntotal impedance is\r\n\n\n\r\nGain\r\n\n\n\r\nIf the two MOSFETs are all NMOS, the parameter drift in process is\r\nalmost the same, so they cancel better and are more precise. In fact the\r\nsecond NMOS serves as a DC current source. Then in AC analysis,\r\n, the  current source vanishes. The total\r\nAC model is simplified to\r\n\r\n\r\n\r\nHowever, for the second MOS, , so we must take\r\n into consideration.\r\n\r\n\r\n\r\n\r\nAC Model Considering Bulk Bias\r\n\r\nThe resistance\r\n\n\n\r\nTo cancel bulk bias, use PMOS to replace NMOS\r\n\r\n\r\n\r\n\r\nAmplifier Replaced by PMOS\r\n\r\nBulk of PMOS is connected to VDD, which is the same as source. Hence\r\nthe bulk bias is cancelled.\r\nBut this circuit has some fatal problems:\r\n\r\nOnly half of the MOSFETs are used to amplify, causing a waste of\r\ncurrent\r\nIt is sensitive to process corner (asymmetry between NMOS and\r\nPMOS)\r\nIt can only deal with a very small range of signal.\r\n\r\nThe total DC model can be plotted.\r\n\r\n\r\n\r\n\r\nI-V Curve of Two MOSFETs\r\n\r\n2.5 CMOS Inverter\r\nTo reduce waste, we need to place the crossing point into two\r\nsaturation points. However, forcing two lines almost parallel is very\r\ndifficult. In most case at least one MOSFET works in linear region.\r\nThe circuit must be changed. We connect  to the\r\n terminal.\r\n\r\n\r\n\r\n\r\nCMOS Inverter\r\n\r\nThis is two amplifiers. When NMOS amplifies, PMOS serves as its load\r\nand vice versa.\r\nIn fact this is a digital inverter. Its DC characteristic is shown\r\nbelow.\r\n\r\n\r\n\r\n\r\nCMOS VTC\r\n\r\nFor AC model,\r\n\r\n\r\n\r\n\r\nAC Model of CMOS Inverter\r\n\r\nThe transconductance is . The gain\r\n\n\n\r\nWith  rises from 0 to VDD, NMOS changes from\r\nsaturation to linear, PMOS changes from linear to saturation. Once one\r\nof the MOSFET enters linear region,  decreases, causing a\r\nlarge decay of gain (since ).\r\n2.6 Source Degeneration\r\nAt this time the problem of current waste is solved. But the\r\n range of double saturation is still too narrow (even\r\nnarrower than before). This problem is solved by source\r\ndegeneration.\r\n\r\n\r\n\r\n\r\nSource Degeneration\r\n\r\n\r\n\r\n\r\n\r\nAC Model of Source Degeneration\r\n\r\nConnect D with GND, by KCL:\r\n\n\n\r\nEliminate :\r\n\n\n\r\nThe effective transconductance:\r\n\n\n\r\nThe key to widen the range of input voltage is negative feedback.\r\nWithout the source resistor, when the input voltage increases, the\r\ngate-source voltage increases, leading to a rise in drain current and a\r\ndrop in drain-source voltage. This makes it easy for the transistor to\r\nenter the linear region.\r\nHowever, with the source resistor present, when the drain current\r\nincreases, the source voltage also increases. This compensates for the\r\nvariation in gate-source voltage, preventing significant changes in\r\ngate-source voltage. As a result, the drain current remains relatively\r\nstable, and the drain-source voltage also maintains stability, keeping\r\nthe MOSFET in the saturation region.\r\n\r\n\r\n\r\n\r\nFeedback Process\r\n\r\nNow it comes to impedance. Apply voltage source , and\r\nconnect  to \r\n\n\n\r\n\n\n\r\nEliminate :\r\n\n\n\r\n\n\n\r\nFinally we have the gain of common source with source\r\ndegeneration:\r\n\n\n\r\nIt remains constant! Source degeneration only changes the range of\r\ndouble saturation.\r\nSource degeneration has other advantages. In original amplifier:\r\n\n\n\r\nThe circuit parameter is influenced by the input signal, generating\r\nnew frequency components, introducing non-linear distortion, but with\r\nsource degeneration:\r\n\n\n\r\nIt is independent of the input (in saturation).\r\n\r\n\r\n\r\n\r\nParameter w/ and w/o Source Degeneration\r\n\r\n","categories":["electronics"]},{"title":"Digital Integrated Circuits","url":"/categories/electronics/digital_integrate/","content":"1. Fundamental Concepts\r\n1.1 Voltage Transfer\r\nCharacteristic (VTC)\r\nRelationship between input voltage  and output voltage .\r\nExample:\r\n\r\n\r\n\r\n\r\nVTC of a CMOS Inverter\r\n\r\nThere’re some symbols need to be explained:\r\n\r\n\r\n= minimum HIGH input voltage\r\n\r\n=maximum LOW input voltage\r\n\r\n=minimum HIGH output voltage\r\n\r\n=maximum LOW output voltage\r\n\r\n is a function of .\r\nIf we connect the output end and the input end, we get a\r\n\r\nThe  is called switching threshold\r\nvoltage.\r\n1.2 Noise Margin\r\nDifference between the minimum HIGH output voltage of driving gate\r\nand the minimum HIGH input voltage recognized by receiving gate.\r\n\r\nFor low flatten, the same.\r\n\r\n\r\n\r\n\r\nNoise Margin of a Inverter\r\n\r\n1.3 Regenerative Property\r\nThe ability of a CMOS inverter to recover the input unideal digital\r\nsignal to an approximate ideal signal.\r\n\r\n\r\n\r\n\r\nWhat is Regenerative Property\r\n\r\nThis characteristic derives from two main reasons:\r\n\r\nVTC:  curve is very stiff in undefined region. Small\r\ndisturbance will cause the signal to leave the undefined region.\r\nPush-Pull Output: CMOS inverter has high input resistance (MOS Gates\r\nare insulated) and low output resistance (MOS Source-Drain resistance in\r\nsaturation area is small). The output end is connected with VDD or GND\r\nthrough a very small resistance.\r\n\r\n1.4 Delay Definitions\r\n\r\n: response time of the gate for a low to high (or positive) output transition.\r\n: response time of the gate for a high to low (or negative) output transition.\r\n\r\n\nNote that the LH or HL is determined by the output direction.\ni.e.,\nIf the output falls from high to low, \nthe delay time is , \nand vice versa.\n\r\n\r\n\r\n\r\n\r\n and \r\n\r\n\r\nPropagation delay time: \r\nContamination delay time: \r\nFall time: 90% - 10% voltage point\r\nRise time: 10% - 90% voltage point\r\n\r\n\r\n\r\n\r\n\r\nFall Time and Rise Time\r\n\r\n2. Diodes\r\n2.1 PN Junction\r\n\r\n\r\n\r\n\r\nDiode on a Silicon\r\n\r\n\r\n\r\n\r\n\r\nPN Junction and Depletion Zone\r\n\r\nThe abrupt gradient causes diffusion, causing the depletion zone. In\r\nthe depletion zone, there will be a build-in field, causing drift.\r\nThe build-in field has a potential: \n\nwhere  is the thermal voltage, \n are separately the accepter concentration and donor concentration.\n\r\n2.2 Bias\r\n\r\nForward Bias:\r\n\r\n\r\nExternal field overcomes build-in field, large forward current\r\n(diffusion)\r\nDepletion width decreases\r\n\r\nReverse Bias:\r\n\r\n\r\nExtremely small reverse current (drift)\r\nDepletion width increases (external &amp; build-in same\r\ndirection)\r\n\r\n\r\n2.3 Current\r\n\n\n\r\n2.4 Capacitance\r\n\r\n\r\n\r\n\r\nPN Junction Capacitance\r\n\r\nThere’re charge on the sides of depletion zone, forming a parallel\r\ncapacitance.\r\n\n\nwhere  is the cross-sectional area.\nFor abrupt junction, ;\nand for linear junction, .\n\r\n3. MOSFET-Static\r\nIn a Metal-Oxide-Semiconductor (MOS) structure, the basic principle\r\nof conduction relies on an electric field controlling a channel. When a\r\nvoltage is applied to the gate terminal, it creates an electric field\r\nthat penetrates through the insulating oxide layer and into the\r\nsemiconductor substrate. This field attracts or repels charge carriers\r\n(electrons or holes) in the semiconductor, forming a conductive channel\r\njust below the oxide.\r\nFor an N-channel MOS transistor, a positive gate voltage attracts\r\nminority carriers (electrons) to the surface of the p-type silicon,\r\ncreating a thin n-type inversion layer. This layer acts as a channel\r\nthat connects the source and drain regions, allowing current to flow\r\nwhen a voltage is applied between them. The size and conductivity of\r\nthis channel are directly controlled by the gate voltage, enabling the\r\ndevice to function as a switch or an amplifier.\r\n3.1 Concepts:\r\n\r\n\r\n\r\nSource and drain are completely symmetric in fabrication. The only\r\ndifference is the voltage.\r\n\r\nNMOS: Low voltage for source, high for drain\r\nPMOS: High voltage for source, low for drain\r\n\r\n3.2 Parameters\r\n\r\n (length): Length of the (transistor) channel.\n  The minimum channel length is the most critical physical parameter \n  that characterizes the advancement of a CMOS technology.\r\n (width): The (transistor) channel width.\n  Minimum width is fixed by the technology.\n  The parameter that the circuit designers decide\n  — Adjusted to tune the driving strength of a transistor\n  Theoretically, no limit for maximum.\r\n\r\n3.3 VTC\r\n\r\n\r\n\r\n\r\nNMOS VTC\r\n\r\n\r\n\r\n\r\n\r\nPMOS VTC\r\n\r\nTake NMOS for example:\r\nBuilding on the principles of MOS transistor operation, the device\r\ncan operate in three distinct regions—cutoff,\r\nlinear, and saturation—\r\n\nbased on the applied gate () and drain-source () voltages. These regions determine the transistor's function as a switch or an amplifier.\n\r\nCutoff Region\r\nThe transistor is in the cutoff region when the\r\ngate voltage () is less than the threshold voltage (). \nIn this state, the electric field from the gate isn't strong enough to attract enough minority carriers (e.g., electrons for an n-channel device) to form a conductive channel at the silicon surface. Without this channel, the source and drain terminals are effectively isolated. As a result, no drain current () can flow, regardless of the voltage applied across the drain and source. The transistor acts like an open switch, blocking current flow.\r\nLinear Region (or Triode Region)\r\nThe transistor operates in the linear region when\r\n is greater than  and the drain-source voltage () is small.\n\nThe gate voltage is now high enough to form a complete, continuous channel between the source and drain. Because  is small, the voltage drop along the channel is negligible, and the channel's thickness is relatively uniform. This makes the channel's resistance nearly constant. Consequently, the drain current () is approximately proportional to , behaving like a voltage-controlled resistor. The value of this resistance decreases as  increases.\r\nSaturation Region\r\nThe transistor enters the saturation region when\r\n is greater than  and  increases to a certain point.\r\n\nAs  rises, the voltage at the drain end of the channel, which is equal to , decreases. When this voltage drops below the threshold voltage, the part of the channel near the drain terminal is \"pinched off.\" The channel effectively disappears at this point, and the current flows through a high-field region to the drain.\n\nDespite the pinch-off, the current does not stop. Instead, it becomes nearly constant because the current is primarily determined by the carrier density at the entrance of the channel. In this region, the MOS transistor acts as a voltage-controlled constant current source, which is the key operational mode for amplifier circuits.\r\nMathemathecally - Cut-off: , no current - Linear:\r\n, \n   and  are approximately linear.\n  \n   \n      : Electron mobility, which describes how easily electrons move through the semiconductor channel. It is measured in .  \r\n\n      : Gate oxide capacitance per unit area, which depends on the thickness and material of the gate oxide layer. It is measured in  or .\n  \r\n\r\nSaturation: , \n   almost remains constant.\n  \n  \r\n\r\n3.4 Channel-Length Modulation\r\n\nWhen MOSFET reaches saturation, \npart of the channel will be pinched-off, \ncausing a shrink of the effective channel length as  increase.\n\r\n\r\n\r\n\r\n\r\nSketch of Pinch-off\r\n\r\n\nGiven this effect, \nthe  formula should be repaired:\n\nThe  is a empirical parameter, \nreflecting the effect of channel modulation.\n\r\n3.5 Velocity Saturation\r\nWhen the electrical field along the channel reaches a critical value,\r\nthe carrier velocity tends to saturate due to scattering effects\r\n(collisions suffered by the carriers).\r\n\nSince the carrier velocity has reaches a constant, \nthe current will no longer increase as  \r\nincreases. This phenomenon is called velocity saturation.\r\n\nSuch effect only matters in short channel devices.\nFor long channel ones,  is large, \neven if  is large, \nthe electrical field  is still small, \nwhich is not enough to saturate the velocity.\nBut in short channel ones, \n is small.\nIt's easy to form a large electrical field.\nHence, velocity saturation is important in short channel devices.\n\r\nThe essense of velocity saturation is electrical field.\r\n\nShort channel MOSFET enters saturation at  before  reaches , \nexperiencing an extended saturation region.\nThen the amount of current is reduced for a given control voltage.\n\r\n\r\n\r\n\r\n\r\nEffect of Velocity Saturation on Short Channel Devices\r\n\r\nGiven velocity saturation and other short-channel effects, we need a\r\nunified current model that works seamlessly across all operating\r\nregions. The key insight is to recognize that the current is always\r\nlimited by the smallest “voltage headroom” available.\r\n\nWe define  as the gate overdrive voltage, and introduce a unified model:\n\nwhere\n\n\r\nThis model elegantly handles all three regions: -\r\nCutoff: When , , and  - Linear\r\nregion: When  is the smallest, , recovering the linear region equation - Saturation:\r\nWhen  or  is the smallest,  becomes constant, giving saturation behavior\n\nThe parameter  represents the drain saturation voltage considering velocity saturation:\n\nwhere  is the critical electric field for velocity saturation.\n\nThis unified approach eliminates discontinuities at region boundaries, making circuit simulation more robust and accurate.\n\r\n\r\n\r\n\r\n3.6 Equivalent Resistance\r\nIn digital circuit analysis, we often need to estimate the\r\ncharging/discharging time of capacitive loads. For this purpose, we\r\ndefine an equivalent resistance  that approximates the nonlinear I-V behavior of a MOSFET with a single resistor value.\n\nThe equivalent resistance is defined as the average resistance during a output transition from  to  (for charging) or from  to  (for discharging). For a capacitor being charged through a PMOS transistor:\n\n\n\nwhere , and  is the drain current when the output voltage is .\n\r\nIn saturation region (considering channel-length modulation), the\r\ncurrent is approximately: \n\n\nSubstituting and integrating from  to :\n\n\n\nFor small , we can expand using Taylor series:\n\nThis equivalent resistance allows us to estimate delay as , greatly simplifying digital timing analysis.\n\r\n3.7 Source and Drain\r\nTake NMOS for example:\r\n\r\n\r\n\r\n\r\nExample for Source Voltage\r\n\r\n\nIn the first figure, \nonce the gate voltage rised to , \n, \nNMOS is turned-on.\nS is charged by D.\nBut with time going, \n rised.\nCan it reach ?\nIf , \n, \nthe NMOS will be cut-off.\nSo to make NMOS keep turning on, \n has a maximum value of .\n\r\nThen in the second figure, \nD still charged S.\nBut since current is from D to S, \nwith the resistance of , \nit's obvious that .\nMeanwhile, \neven at , \nthe NMOS is still on, \nso  is reachable.\n\r\n\nTo recap, \nthe two cases can be joined into one equation:\n\n\r\n\r\n\r\n\r\n4. MOSFET-Dynamic\r\n4.1 Capacitance Components\r\n\r\n\r\n\r\nThere are three sources of capacitance in a MOSFET.\r\n\r\nOverlap cap   \r\nParasitic capacitance between the gate and source/drain caused by\r\nlateral diffusion extending beneath the oxide layer.\r\n\n  \n  \r\nGate-Source and Gate-Drain\r\n\r\n: cap per unit area presented by gate oxide\r\n: overlap cap per unit transistor width\r\n: lateral diffusion length\r\n: transistor width\r\n: oxide permittivity\r\n: oxide thick\r\n\r\n\r\n\r\n\r\nGate-to-channel capacitances   \r\nGenerates the channel charge required for transistor operation and\r\ndepends on the transistor’s operating region.\r\nGate-Body, Gate-Source, Gate-Drain\r\n\r\n\r\n\r\nJunction cap: PN junction cap  .\r\n\r\n\r\n\r\n\n  Junction / diffusion cap:\n  \n\n  diffusion cap of source to body:\n  \n\n  diffusion cap of drain to body:\n  \n  \r\n\r\n: bottom-plate junction cap\r\n: side-wall junction cap\r\n: junction cap/unit area\r\n: side-wall junction cap/unit perimeter\r\n: area of bottom plate\r\n: perimeter of side wall\r\n: length of source region\r\n: length of drain region\r\n: transistor width\r\n\r\n\r\n4.2 Body Effect\r\nWhen the voltage of substrate changed, the threshold voltage will\r\nalso change.\r\n\n\n\nFor NMOS\n\nFor PMOS\n\nwhere  is the substrate bias between source and body; \n is threshold voltage for  (determined by manufacturing).\n is called the body effect coefficient, \nexpressing the impact of .\nAnd \n\nPositive for PMOS and negative for NMOS.\n\r\n4.3 Sub-threshold Condition\r\n\nMeaning .\nIn most digital ICs, sub-threshold current is not desirable (leakage), \r\nwhich will cause heat and waste of power.\r\n\nIn sub-threshold region, \nthe slope of  ——  is \n\n\r\n\r\n\r\n\r\n\nIn this situation, \n\nwhere\n\n\r\nLeakage in sub-threshold region is the most impotant useless power\r\nsource in modern IC.\r\n5. Inverters\r\n5.1 Static CMOS Inverter\r\n\r\n\r\n\r\n\r\nCOMS Invertor\r\n\r\nPMOS passes a strong 1 and a weak 0, while NMOS passes a strong 0 and\r\na weak 1. When input is 0, PMOS is conducted and NMOS cut, output is a\r\nstrong 1. And when input is 1, the output is a strong 0.\r\nSince MOS has a equivalent resistance , \nso an inverter can only load finite load.\nThe maximum load is determined by when the output voltage with load\nexceeds the noise margin.\n\r\n\nIn an inverter, \nwhen the output is float, \nby KCL we have\n\nThe input  and  is in fact the  of NMOS and PMOS, respectively.\nAccording to the MOSFET property, \nwe can list a table of the two MOSFETs:\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\nTo find the DC response, \nsince the currents have the same value but in opposite direction, \nwe place the  curve in different  in one coordinate with some transformation.\nThe crossing points of NMOS and PMOS in the same  is the working point.\nConnect all such points to get the DC response curve.\n\r\n\r\n\r\n\r\n\r\nFinding the VTC of CMOS Invertor\r\n\r\nAccording to this VTC curve, we will further research the state of\r\nMOSFETs in different region.\r\n\r\n\r\n\r\n\r\nCMOS Invertor VTC in Different Region\r\n\r\n\r\nIn region A: \n\nInverter in region A outputs a high-quality 1.\nWe can say .\nObviously, the NMOS is cut-off.\nand\n\nSince\n\nThen the PMOS is in the linear region.\n\r\n\r\nSimilarly, we can get a table:\r\n\r\n\r\n\r\nTo recap, CMOS inverters have such good properties: - High noise\r\nmargins: Output voltage swing is equal to the supply\r\nvoltage（rail-to-rail full voltage swing）\r\n\r\nRatioless circuit structure: The output logic levels are\r\nindependent of the relative device sizes\r\nLow output impedance: Always exists a path with finite resistance\r\nbetween the output and either VDD and GND in steady state\r\nHigh input resistance: The input of static CMOS circuit only\r\nconnects to transistor gate with no DC input current\r\nLow power: No direct path between VDD and GND in steady\r\nstate\r\n\r\n5.2 Switching Behavior\r\n\nIn an inverter,\ntwo MOSFETs must satisfy\n\nplug the MOSFET current formula in,\n\n\nThe switching threshold voltage  is a crucial parameter defined as the point where . At this bias point, both NMOS and PMOS transistors are in saturation (for long-channel devices), and we can equate their currents.\n\nAt :\n\r\n\r\nFor NMOS: , \r\nFor PMOS: , \r\n\r\n\nUsing the saturation current formula (ignoring channel-length modulation for simplicity):\n\n\nSetting  (currents are equal in magnitude but opposite in direction):\n\n\nTaking square roots and defining the mobility ratio :\n\n\nSolving for :\n\n\nFor the more general case including velocity saturation, we use the unified current model from section 3.5. When both transistors are in saturation,  for each device, giving:\n\n\nSolving this equation yields:\n\nwhere  is the relative strength ratio between PMOS and NMOS.\n\nThis result shows that  can be tuned by adjusting the transistor sizing ratio , which is fundamental to circuit optimization.\n\r\n5.3 Gain\r\n\nThe gain of a CMOS inverter is a critical parameter that determines its noise immunity and signal regeneration capability. It is defined as the slope of the voltage transfer characteristic at the switching threshold:\n\n\n\nTo derive the gain, we start from the current balance equation at the output node. The currents through NMOS and PMOS must sum to zero:\n\n\n\nUsing the unified current model with channel-length modulation, and assuming both transistors are in saturation at :\n\n\n\nWe differentiate this equation implicitly with respect to :\n\n\n\nAt the switching point , we evaluate the partial derivatives:\n\r\n\r\nTransconductance terms: \n  \r\nOutput conductance terms:\r\n\n  \n\nSubstituting into the differentiated equation:\n\n\n\nSolving for the gain :\n\n\n\nThis can be rewritten using the drain current  at the switching point:\n\n\n\nA useful approximation for circuit design is:\n\n\n\nwhere  is the relative strength ratio.\n\nThis high gain (typically &gt; 20 for modern CMOS) is what gives CMOS inverters their excellent regenerative property and sharp switching characteristics.\n\r\n\r\n\r\n\r\n\r\nWhen the voltage is too low, both MOSFETs are leaking current and\r\ncannot be cut-off or connected completely. Hence, though lower voltage\r\nis helpful for decreasing power consumption, but it will also reduce the\r\ngain if exceeding reasonable range.\r\n5.4 Noise Margin\r\n\nThe concept has been declared in chapter 1.\nWe figure out why the  and  are the points of .\n\nThe noise margins  and  represent the circuit's immunity to noise. To maximize the overall noise immunity, we want to maximize the sum:\n\n\n\nFor an ideal CMOS inverter with rail-to-rail swing,  and , so:\n\n\n\nThe parameters  and  are defined as the points where the slope of the VTC curve equals -1. This definition arises from optimizing noise immunity. Imagine drawing rectangles between the VTC curve and the ideal binary thresholds. The noise margins represent the dimensions of these \"noise immunity rectangles.\" To maximize their total area, the corners of these rectangles should lie where the VTC curve has slope -1. In another view, consider  as the point where a small input noise  causes an equal output noise  (unity noise gain). At this boundary condition:\n\n\n\nThe same reasoning applies to . Therefore, the optimal noise margins occur when:\n\n\n\nThis elegant result means that the points of maximum noise immunity naturally occur where the VTC curve has a slope of -1, providing a clear and practical way to determine  and  from the voltage transfer characteristic.\n\r\nThere is an approach to simplify the calculation. This simplified\r\nmethod uses a piecewise linear model to approximate the voltage transfer\r\ncharacteristic (VTC) curve. The VTC curve is approximated as three\r\nstraight line segments:\r\n\r\nHigh-level output segment: When the input\r\nvoltage is low, the output voltage remains at the high level\r\n\r\nLow-level output segment: When the input voltage\r\nis high, the output voltage remains at the low level\r\n\r\nTransition region: Near the switching threshold voltage , the curve is approximated as a straight line with slope , passing through the point , satisfying the equation:\n   \nBased on this linearized approximation, we can derive the key parameters:\n\r\n\r\n\r\nHigh-level input threshold\r\n: Obtained by solving the intersection point of the transition region line and the horizontal line \n  \r\nLow-level input threshold : Obtained by solving the intersection point of the transition region line and the horizontal line \n  \n\r\nFinally, calculate the noise margins:\r\nHigh-level noise margin: \r\nLow-level noise margin: \r\n\r\n","categories":["electronics"]},{"title":"WORK ASSIGNMENT AFTER FINAL TERM DISASTER","url":"/categories/news/assignment/","content":"[Shanghai, June 20th, 2025]\r\nFollowing the severe impact of the recent final-term diaster, we\r\nhereby establish reconstruction and adjustment framework. This plan\r\noutlines out assignment in the following month to reduce the negative\r\nimpact caused by the disaster and try to make preparation for\r\ndevelopments and challenges in the future.\r\nWith the publishing of this report, our schedule is\r\nunfrozen.\r\nWe will focus on the following items in a month:\r\n\r\nReconstruct the systems destroyed in the disaster.\r\nArchive our digital resources.\r\nDevelop our website.\r\nDevelop our algebra technology.\r\nContinue and restart the research on digital IC.\r\nMaking preparations for our investigation in Germany.\r\n\r\nThis plan will remain valid until our investigation to Germany at Aug\r\n3rd.\r\n","categories":["news"]},{"title":"Let's Read hbird_e203 CPU Together","url":"/categories/electronics/read_e203/","content":"Note: This article is merely a thought process. Please do not\r\nuse it as a handout, as it does not provide enough information for\r\nsystematic learning. Using it as a reference is OK, though.\r\nAs a start to deep dive into the world of CPU, hbird_e203 is a good\r\nproject.\r\nThe following is the project link: Hummingbird\r\nE203\r\n1. Instruction Set\r\nArchitecture (ISA)\r\nhbird_e203 applies RISC-V ISA, which is designed simply. Instrictions\r\nin RISC-V ISA are ordered strictly.\r\n\r\n\r\n\r\n\r\nFormat of RISC-V Instructions\r\n\r\nRISC-V only supports little-endian. What is little-endian and\r\nbig-endian? Well, let’s use a figure to interpret:\r\n\r\n\r\n\r\n\r\nInterpretation of Little-Endian and Big-Endian\r\n\r\nIf the ISA applies little-endian, the data stored is\r\n0x78563412. If big-endian, it should be\r\n0x12345678.\r\n2. Standard DFF Registers\r\nhbird_e203 uses modulized, standard DFF module to construct\r\nregisters, instead of using always block. wire flg_r; // output signalwire flg_nxt = ~flg_r; // input signalwire flg_ena = (ptr == ('E203_OITF_DEPTH-1) &amp; ptr_ena);// instantiating standard DFF modulessirv_gnrl_dfflr #(1) flg_dfflrs(flg_ena, flg_nxt, flg_r, clk, rst_n);\r\nIn another module, sirv_gnrl_dfflr is designed\r\nbelow:\r\nmodule sirv_gnrl_dfflr # (    parameter DW = 32 // This is a 32bit width DFF, by transmitting parameter DW can be vaired.) (    input               lden,     input      [DW-1:0] dnxt,    output     [DW-1:0] qout,    input               clk,  // clock    input               rst_n // negative valid reset, synchronous);    reg [DW-1:0] qout_r;    always @(posedge clk or negedge rst_n)    begin : DFFLR_PROC // DFFLR_PROC is just a label of this always block. No effect on functions.        if (rst_n == 1'b0)            qout_r &lt;= {DW{1'b0}}; // reset, output pull down        else if (lden == 1'b1)            qout_r &lt;= #1 dnxt;            // #1: delay of 1 time-unit. No effect in synthesis. Debug method in simulation.            // dnxt: data to be assigned    end    assign qout = qout_r;    // introduce a checker, conditional compile.    // only used in simulation, no hardware circuit produced    `ifndef FPGA_SOURCE//{    `ifndef DISABLE_SV_ASSERTION//{    //synopsys translate_off        sirv_gnrl_xchecker # (        .DW(1)        ) sirv_gnrl_xchecker(        .i_dat(lden),        .clk  (clk)        );    //synopsys translate_on    `endif//}    `endif//}endmodule\r\nUsing standard modules, it’s convenient to replace type of registers\r\nor insert delay globally. The xchecker module captures\r\nundefined state. Once detected, it reports an error and aborts\r\nsimulation.\r\n3. if-else and\r\nassign\r\nThis project recommends replacing if-else with\r\nassign. Because if-else has two major\r\ndrawback:\r\n\r\nif-else cannot transmit undefined state\r\nX. if(flg)    out = in1;else    out = in2; If flg == X, verilog will\r\nequate this to flg == 0, the final output will be\r\nout = in2, which did not transmit X state.\r\nHowever, if assign used assign out = flg ? in1 : in2; The X\r\nstate will be transmitted. Such transmission will make debug\r\neasier.\r\nif-else will be synthesised as a priority MUX, which\r\nbrings large area and worse timing. Let’s take the MUX below for\r\nexample: if (sel1)    out = in1[3:0];else if (sel2)    out = in2[3:0];else if (sel3)    out = in3[3:0];else    out = 4'b0; After synthesis, this code turns to\r\n\r\n\r\n\r\n\r\nPriority MUX\r\n\r\n\r\n3 MUX will obviously occupy larger area. But if we use\r\nassign: assign out =   ({4{sel1}} &amp; in1[3:0])             | ({4{sel2}} &amp; in2[3:0])             | ({4{sel3}} &amp; in3[3:0]); This is a parallel, gating MUX. The\r\nsel signals act as gating controls, which are parallel for\r\nthe three in signals. It will be synthesised as\r\n\r\n\r\n\r\n\r\nParallel MUX\r\n\r\n4. Data Hazard\r\n\r\nRAW (Read After Write) Suppose instruction j needs\r\nan operation number, which should be provided by instruction\r\ni. Hence, WB of i must be executed before the\r\nregister reading of j.\r\nFor example: i: ADD x1, x2, x3 ; (x2 + x3 -&gt; x1)j: SUB x4, x1, x5 ; (x1 - x5 -&gt; x4)\r\nIn a pipeline, when j is executing ID, i\r\nmight be still executing EX, the result hasn’t been written into the\r\nregister file yet. In this situation, j will read a wrong\r\noperation number.\r\nTo solve the problem, the pipeline can apply a stalling to pause the\r\nfollowing instructions, waiting for WB of i. But the most\r\nusual method is Data Forwarding. CPU will send the result of EX or MEM\r\nof i to j directly, instead of waiting for\r\ni. This method increases efficiency compared to\r\nstalling.\r\nWAR (Write After Read) Instruction j tries to write\r\nin a register, but another instruction i needs to read the\r\noperation number in this register. Reading of i must be\r\ncompleted before writing of j.\r\nExample: i: SUB x4, x1, x5  ; read x1j: ADD x1, x2, x3  ; write x1k: MUL x6, x1, x7  ; read x1\r\nIf the pipeline is in-order, it has no problem. However, in an\r\nout-of-order pipeline, if x2 and x3 are ready\r\nahead of time, j may finish executing before\r\ni. Then i will give a wrong result.\r\nTo solve, CPU will rename the registers. i: SUB x4, P1, x5   ; // P1 for old x1 valuej: ADD P2, x2, x3   ; // P2 for new x1 value, unrelated with P1k: MUL x6, P2, x7   ; // Use new value\r\nTo achieve renaming, the CPU will create a map from the external\r\nregister file (ISA registers) to the internal registers. Then the\r\nwriting and reading will not influence each other anymore.\r\nWAW (Write After Write)\r\nTwo instructions, i and j, both needs to write\r\na number into the same register. The correct order is i\r\nfirst and j second. The same, WAW also occurs in an\r\nout-of-order pipeline. If j finishes first, the final\r\nresult should be that of i, which is wrong.\r\nThe solution is also Renaming.\r\n\r\n5. Instruction Fetch (IF)\r\nThe final target of IF is to be ‘fast’ and ‘continuously’.\r\nITCM\r\nTo make IF faster, we need to make the read-delay of the memory\r\nsmaller. General memory may have a delay of dozens of clock period,\r\nwhich is far from meeting our requirements.\r\nIn general, a modern CPU creates a small memory (dozens of KB) used\r\nfor store instructions, which is physically close to the core. This\r\nmemory is called ITCM (Instriction Tightly Coupled Memory).\r\nITCM is not DDR or cache. It is just a small memory with specific\r\naddress. The delay is predictable compared to cache. Hence, in\r\nsituations with high performance requirements, engineers prefer to use\r\nITCM.\r\nNon-aligned Instructions\r\nRISC-V support compressed instructions (C expansion). CPU has to deal\r\nwith a mixture of 32-bit and 16-bit instructions. So how CPU know it is\r\na 32-bit or a 16-bit instruction?\r\nThe least significant two bits of the opcode for a 32-bit\r\nRISC-V instruction must be 0b11.\r\nCPU distinguishes the instructions according to the least two\r\nsignificant bits (call it LS2B below). If the LS2B is 0b11,\r\nit is 32bit; If not, it is 16bit.\r\nSo how CPU deal with them? Let’s clarify the flow in detail.\r\n\r\nComponents\r\n\r\nFetch Width: For efficiency, CPU will take more than a halfword from\r\nITCM a time. It usually takes more, for example 32bits.\r\nInstruction Prefetch Queue (IPQ): A FIFO between IFU and the\r\ndecoder.\r\nRISC-V Rule: If LS2B = 0b11, it is a 32bit instruction;\r\nor it is a 16bit one\r\n\r\nWorking Flow\r\n\r\nAccording to PC value, IFU takes a word (32bit) from ITCM and\r\ninserts them into the bottom of IPQ.\r\nID get a halfword (16bit) from the top of IPQ, then judge whether it\r\nis a compressed instruction.\r\n\r\nSituation A: It is a 16bit compressed instruction\r\nID consumes the first 16bits in IPQ and sends them to the following\r\nsections as a entire instruction. Pointer of IPQ moves 2 bytes.\r\nSituation B: It is part of a 32bit instruction\r\nID needs more data. It consumes the first 32bits in IPQ and then\r\nsends them to the following section. Pointer of IPQ moves 4\r\nbytes.\r\n\r\nThese steps will be repeated. When the data in the IPQ is less than\r\n32 bits, the IFU will perform the next 32-bit read operation and pad the\r\ndata to the end of the IPQ.\r\n\r\n\r\nBranch Instructions\r\nThere are two types of branch instructions in RISC-V.\r\n\r\nUnconditional Jump: Judgement conditions unnecessary. There are also\r\ntwo types of unconditional jump。\r\n\r\nDirect: Target address can be directly calculated by\r\nimm in the instruction.\r\nExample: jal x5, imm, imm is 20 bit, jump\r\nto address 2*imm + PC.\r\nIndirect: Target address needs to be calculated from data in the\r\nregister file.\r\nExample: jalr x1, x6, imm, imm is 12 bit,\r\njump to address imm + x6.\r\n\r\nConditional Jump: Jump with conditions Still, two types: Direct and\r\nIndirect. But there are no indirect instructions in RISC-V.\r\n\r\nBranch Prediction\r\nSolve two problems: - Whether to jump (Direction) - What’s the target\r\naddress (Address)\r\nStatic prediction: Always predicting the same outcome or following a\r\nfixed pattern. (BTFN)\r\nJump direction: Target PC &lt; Present PC, called back; otherwise\r\ncalled forward.\r\nDynamic:\r\n\r\n1 bit Saturating: Using the last direction to predict. Modify\r\nwhen meeting a mistake.\r\n2 bit Saturating:\r\nLook at the state machine:\r\n\r\n\r\n\r\n\r\n\r\n2 bit Saturating Counter FSM Chart\r\n\r\n2 bit Saturating is effective in predicting one instruction. But for\r\nmany instructions (At different PC address) not. (They will conflict)\r\nIdeally each jump instruction should have its own predictor, which will\r\nbring unacceptable hardware cost. So in practice there’re only finite\r\npredictors forming a table (Branch Prediction Table).\r\nAccurate prediction procedure: Indexing\r\n\r\nOne instruction enters the pipeline, with\r\nPC = 0x12345678.\r\nCPU take lower significant sevral bits (10 for example), index\r\n0x678 = 0d1656.\r\nCPU accesses BPT with index 0d1656, find a 2 bit\r\nsaturating predictor.\r\nPrediction run by predictor, update states, …\r\n\r\nIn fact the instructions number is far larger than that of\r\npredictors. Hence many different instructions have to use the same\r\npredictor. This problem is called Aliasing.\r\nThere is a more complicated method, but with better performance,\r\ncalled Correlation-Based Branch Predictor.\r\n\r\nWhy we need it\r\n\r\nConsider a code\r\nif (a &gt; 10) { // branch A  Hajimi nameiluduo axigaaxi;}Dingdongji Dingdongji, Dagou Dagou Jiaojiaojiao;if (b &gt; 20 &amp;&amp; a &gt; 10) { // branch B  Axiga Yakunalu Hajimi Haji;}\r\nWhether B jumps dependes on both b &gt; 20 and the\r\nresult of branch A. If A did not jump, B must not jump. A single\r\npredictor table cannot deal with such a situation.\r\nTwo components:\r\n\r\nGlobal History Register (GHR): Width N, record results\r\nof recent N instructions.\r\nPattern History Table (PHT): An array composed by 2bit\r\ncounters.\r\n\r\nIndex method: PC ^ GHR. The 2bit counters record “when\r\nglobal history is in certain pattern, how branch B will act”, instead of\r\nhistory of branch B itself.\r\nProcedure:\r\nSuppose GHR has 2bit width. Initial state 00.\r\n\r\nExecution 1: suppose a = 5, A not jump, recorded as\r\n0, GHR left shift, fill 0 to the LSB of GHR.\r\nGHR = 00. B not jump, recorded as 0, GHR left\r\nshift, fill 0 to the LSB of GHR.\r\nExecution 2: suppose a = 15, b = 25, A jump, GHR =\r\n01. Before execute B, produce index,\r\nidx = Hash(PC_B, 01). Find a 2bit counter (suppose initial\r\nstate to be 11, indicates when the last branch jumped, B\r\ntends not to jump).\r\nMake prediction: B will not jump.\r\nActual result: Prediction failed! Counter: 11 -&gt; 10.\r\nGHR: 01 -&gt; 11.\r\n\r\nThese steps repeat.\r\n6. E200 IFU Implementation\r\nRISC-V put the length indicator on the lowest significant bits.\r\nHence, the IF logic can recognize the length right after it fetches the\r\nlowest bits. What’s more, since the compressed instruction set is\r\noptional, if the CPU is not designed to support compressed set, it can\r\ndirectly ignore the lowest bits, which can save about 6.25 I-cache\r\ncost.\r\nOverall Design Concept\r\nIFU module has such a micro-architechture:\r\n\r\n\r\n\r\n\r\nIFU Micro-Architechture\r\n\r\nIt tries to fetch instructions “fast” and “continuously”. E203\r\nassumes most instructions are stored in ITCM, for it is designed for\r\nultra-low power, embedded cases, it will never load long codes. Usually\r\nthe codes can all be loaded in ITCM.\r\nIF module can fetch an instruction in just one period, which has\r\nreached the requirement of fast. When it need to fetch instructions from\r\nBIU, there will be more delay but such cases are much less than ITCM, so\r\nE203 made no optimization for these cases (for higher performance, maybe\r\nsuch optimization is necessary, though).\r\nFor “continuously”, each time IF should predict the next\r\nPC value. IF partly decodes the fetched instruction and\r\njudges whether it needs to jump. If yes, branch predictor runs in the\r\nsame period, and IF uses the result and decoded information to generate\r\nnext PC.\r\nMini-decode\r\nThis module just needs to judge whether it is a general instruction\r\nor branch instruction. To simplify design process, this module is\r\nimplemented by instantiating a complete decode module with unrelated\r\ninput grounded and output unconneted. Synthesis tools optimize the\r\nredundant logics and finally achieve a mini-decode.\r\n`include \"e203_defines.v\"module e203_ifu_minidec(  // The IR stage to Decoder  input  [`E203_INSTR_SIZE-1:0] instr,  // The Decoded Info-Bus  output dec_rs1en,  output dec_rs2en,  output [`E203_RFIDX_WIDTH-1:0] dec_rs1idx,  output [`E203_RFIDX_WIDTH-1:0] dec_rs2idx,  output dec_mulhsu,  output dec_mul   ,  output dec_div   ,  output dec_rem   ,  output dec_divu  ,  output dec_remu  ,  output dec_rv32, // indicate bits of instruction (16bits or 32bits)  output dec_bjp,  // general or branch instructions  output dec_jal,  // whether jal   output dec_jalr, // whether jalr  output dec_bxx,  // whether conditional jump instructions  output [`E203_RFIDX_WIDTH-1:0] dec_jalr_rs1idx, // index of rs1 register for jalr  output [`E203_XLEN-1:0] dec_bjp_imm  // imm of conditional jump  );  // a complete decode module  e203_exu_decode u_e203_exu_decode(  .i_instr(instr),  .i_pc(`E203_PC_SIZE'b0),  // ......  );endmodule\r\nWe will research on the decode module in detail in the following\r\nsections, not here.\r\nReady/Valid Handshake\r\nReady/valid handshake is a protocal to ensure correct transition of\r\ndata between two equipments.\r\n\r\n\r\n\r\n\r\nSketch of Handshake Protocal\r\n\r\nThe rules are straightforward: data transfer only happens when both\r\nready and valid are ‘1’ during\r\nthe same clock cycle. The handshake is a stateless protocol. Neither\r\nparty needs to remember what happened in previous clock cycles to\r\ndetermine if a data transfer occurs in the current cycle. Furthermore,\r\nboth parties must operate synchronously and read the control signals on\r\nthe same clock edge. Because of that, ready/valid isn’t appropriate for\r\nclock domain crossing (CDC).\r\nSimple BPU Branch Predictor\r\nTo achieve low power, E203 applies the most simple stationary\r\nprediction. For conditional direct jump instructions, a backward jump is\r\npredicted to require a jump; otherwise, it is predicted not to require a\r\njump. Meanwhile BPU generates next PC by\r\nPC + offset.\r\nThe file is in module e203_ifu_litebpu.v\r\n`include \"e203_defines.v\"module e203_ifu_litebpu(  // Current PC  input  [`E203_PC_SIZE-1:0] pc,  // The mini-decoded info   input  dec_jal,   // whether jal  input  dec_jalr,  // whether jalr  input  dec_bxx,   // where conditional jump  input  [`E203_XLEN-1:0] dec_bjp_imm,  // imm of conditional jump, target address is PC + imm  input  [`E203_RFIDX_WIDTH-1:0] dec_jalr_rs1idx, // index of rs1 in jalr  // The IR index and OITF status to be used for checking dependency  input  oitf_empty,  // whether OITF is empty  input  ir_empty,    // whether IR is empty  input  ir_rs1en,    // whether to use rs1 in IR  input  jalr_rs1idx_cam_irrdidx, // whether index of rs1 matches that of IR  /*     If the instruction in IR and the next jalr use the same register,     jalr_rs1idx_cam_irrdidx = 1    This signal is used to prevent RAW hazard.  */    // The add op to next-pc adder  output bpu_wait,    /*     If dependency detected, bpu_wait = 1,    pause the pipeline and wait until instruction of IR completes execution.  */  output prdt_taken,  // result, predict to jump -&gt; high  output [`E203_PC_SIZE-1:0] prdt_pc_add_op1,    output [`E203_PC_SIZE-1:0] prdt_pc_add_op2,  // predicted_pc = prdt_pc_add_op1 + prdt_pc_add_op2  input  dec_i_valid, // valid for decode  // The RS1 to read regfile  output bpu2rf_rs1_ena, // enable register file to read rs1  input  ir_valid_clr, // clear IR valid  // when instruction in IR is cleared, dependency can be removed.  input  [`E203_XLEN-1:0] rf2bpu_x1,  // read x1 register ()  input  [`E203_XLEN-1:0] rf2bpu_rs1, // other registers for jalr  // clock and reset (negative valid)  input  clk,  input  rst_n  );  // The JAL and JALR is always jump, bxxx backward is predicted as taken    assign prdt_taken   = (dec_jal | dec_jalr | (dec_bxx &amp; dec_bjp_imm[`E203_XLEN-1]));    /*  `E203_XLEN is the width of register  BPU predicts to jump, if instr is jal or jalr.  For conditional, MSB of dec_bjp_imm determines the symbol of offset.  1 for negative number, backward jump, predict as taken;  0 for positive number, forward jump, predict as not-taken.  */  // The JALR with rs1 == x1 have dependency or xN have dependency  wire dec_jalr_rs1x0 = (dec_jalr_rs1idx == `E203_RFIDX_WIDTH'd0);  // assigned as 1 when jalr uses x0 (value is always 0), a special unconditional jump  wire dec_jalr_rs1x1 = (dec_jalr_rs1idx == `E203_RFIDX_WIDTH'd1);  // assigned as 1 when jalr uses x1 (used as return address register)  wire dec_jalr_rs1xn = (~dec_jalr_rs1x0) &amp; (~dec_jalr_rs1x1);  // assigned as 1 when jalr uses other registers (general registers)  // check dependency  wire jalr_rs1x1_dep = dec_i_valid &amp; dec_jalr &amp; dec_jalr_rs1x1 &amp; ((~oitf_empty) | (jalr_rs1idx_cam_irrdidx));  /*  dependency occured at register x1:  decode success, is a jalr instr., jalr and IR both use x1, OITF is not empty.  */  wire jalr_rs1xn_dep = dec_i_valid &amp; dec_jalr &amp; dec_jalr_rs1xn &amp; ((~oitf_empty) | (~ir_empty));  /*  If the value of xn can only be determined until EXU, it is considered dependency exists.  It is considered to have dependency only if IR is not empty, instead of comparing the register number.  This is in fact a conservative method.   Such design simplifies hardware logic and reduces the cost of area and power,   though decreases performance at the same time.  But since E203 is designed for ultra-low power,   a little loss of performance is acceptable.  */  wire jalr_rs1xn_dep_ir_clr = (jalr_rs1xn_dep &amp; oitf_empty &amp; (~ir_empty)) &amp; (ir_valid_clr | (~ir_rs1en));  /*   If only depend to IR stage (OITF is empty), then if IR is under clearing,   or it does not use RS1 index, then we can also treat it as non-dependency  */  // a FSM, determine the time to read value of rs1 to provide correct value for jalr  wire rs1xn_rdrf_r; // indicates whether CPU needs to read value from rs1  wire rs1xn_rdrf_set = (~rs1xn_rdrf_r) &amp; dec_i_valid &amp; dec_jalr &amp; dec_jalr_rs1xn &amp; ((~jalr_rs1xn_dep) | jalr_rs1xn_dep_ir_clr);  wire rs1xn_rdrf_clr = rs1xn_rdrf_r;  wire rs1xn_rdrf_ena = rs1xn_rdrf_set |   rs1xn_rdrf_clr;  wire rs1xn_rdrf_nxt = rs1xn_rdrf_set | (~rs1xn_rdrf_clr);  sirv_gnrl_dfflr #(1) rs1xn_rdrf_dfflrs(rs1xn_rdrf_ena, rs1xn_rdrf_nxt, rs1xn_rdrf_r, clk, rst_n);  assign bpu2rf_rs1_ena = rs1xn_rdrf_set;  assign bpu_wait = jalr_rs1x1_dep | jalr_rs1xn_dep | rs1xn_rdrf_set;  assign prdt_pc_add_op1 = (dec_bxx | dec_jal) ? pc[`E203_PC_SIZE-1:0]                         : (dec_jalr &amp; dec_jalr_rs1x0) ? `E203_PC_SIZE'b0                         : (dec_jalr &amp; dec_jalr_rs1x1) ? rf2bpu_x1[`E203_PC_SIZE-1:0]                         : rf2bpu_rs1[`E203_PC_SIZE-1:0];    assign prdt_pc_add_op2 = dec_bjp_imm[`E203_PC_SIZE-1:0];  endmodule\r\n","categories":["electronics"]},{"title":"NOTICE OF LOWERING THE ALERT LEVEL","url":"/categories/news/emerge_cancel/","content":"[Shanghai, Oct 11th, 2025]\r\nGiven that the current situation has eased and external threat has\r\nreduced. The policy center has decided to lower the alert level from\r\nDEFCON 2 to DEFCON 3 and cancel the emergency state released on\r\nSep. 8th, 2025.\r\n\r\n鉴于当前形势已趋缓和，外部威胁有所降低。 政策中心决定将戒备等级\r\n从DEFCON 2调降至DEFCON 3， 并解除2025年9月8日发布的紧急状态。\r\n\r\nAngesichts der Entspannung der aktuellen Lage und des Rückgangs\r\nexterner Bedrohungen hat das Policy Centre beschlossen, die Alarmstufe\r\nvon DEFCON 2 auf DEFCON 3 herabzustufen und den am 8. September 2025\r\nausgerufenen Ausnahmezustand aufzuheben.\r\n","categories":["news"]},{"title":"DECLARATION OF EMERGENCY STATE","url":"/categories/news/emerge_declare/","content":"[Dandong, Sep 8th, 2025]\r\n中文版本请见下方\r\nDie Deutsche Version finden Sie unten.\r\n\r\nThe investigation in Deutschland and Switzerland has ended on Sep\r\n4th. In view of the current threat assessment, we are elevating our\r\nreadiness condition to DEFCON 2.\r\n\r\n\r\n\r\n\r\nOur Readiness Level\r\n\r\nThis alert level will remain in effect for a minimum of 6 months. It\r\nwill not be lowered until an official announcement is made.\r\n\r\n在德国和瑞士的调查已经结束。 考虑到当前局势，\r\n我们将当前戒备水平提升为DEFCON 2.\r\n\r\n\r\n\r\n\r\n我们采用的戒备等级\r\n\r\n该水平将至少持续6个月。 在发出正式通知之前， 该水平不会被降低。\r\n\r\nDie Ermittlungen in Deutschland und in der Schweiz worden am 4 Sep\r\nabgeschlossen. Angesichts der aktuellen Bedrohungslage erhöhen wir\r\nunsere Alarmstufe auf DEFCON 2.\r\n\r\n\r\n\r\n\r\nUnsere Alarmstufe\r\n\r\nDiese Stufe bleibt mindestens 6 Monate lang in Kraft. Vor der\r\nAusstellung einer formellen Mitteilung darf diese Stufe nicht\r\nherabgesetzt werden.\r\n","categories":["news"]},{"title":"ELECANNONIC HAS SUCCESSFULLY CREATED ITS WEBSITE","url":"/categories/news/website_create/","content":"[Shanghai, May 28th, 2025]\r\nWe, EleCannonic, proudly announces the official launch of its\r\nbrand-new website, marking a significant milestone in its digital\r\njourney. Built from scratch, the project was guided by experienced\r\nspecialists who generously shared their technical experience to turn the\r\nvision into reality.\r\nThe website, designed to streamline communication and display our\r\ninitiatives, features a clean layout and user-friendly navigation.\r\nNotably, EleCannonic has opened a “Friendship Links” section to connect\r\nwith like-minded individuals and organizations. Friends who wish to\r\ncollaborate or exchange personal/blog links are warmly invited to join\r\nthis growing network.\r\nWe started with zero resources, but collective effort made it\r\npossible. This is just the beginning. We hope to foster more connections\r\nthrough this platform.”\r\nVisit our website to\r\nexplore EleCannonic’s new online home. Those interested in adding\r\nfriendship links may submit requests via the friendship link.\r\n","categories":["news"]},{"title":"Electrodynamics","url":"/categories/physics/electro/","content":"1. Vector Algebra\r\nWe will neglect very simple operations.\r\n1.1 Triple Products:\r\n  \n\n\n\r\n1.2 Nabla Operator:\r\n\nFor a unit vector,\ndifferentiation means how  changes\nas  increases by an infinitesimal amount.\nThat is \n\n\r\nDescartes Coordinate:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSpherical Coordinate:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\nAnd similarly for others.\n\r\nBased on these tables, we can derive\r\ngradient: \n\n\ndivergence:\n\n\r\nTo recap:\r\n\r\nGradient:   \n\n\r\nDivergence:   \n\n\r\nCurl:   \n\n\r\nLaplace:   \n\n\r\n\r\n1.3 Properties of Nabla\r\nOperators\r\n\r\n  \r\n\r\nPf.\r\n  \n\n\nSince  is analytic, \nmixed parital derivative is equal.\nHence .\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \n\n\n\r\n\r\n  \r\n\r\nPf. Consider only one component (others are cylic).\r\n  \n\n\nSame for other components and join them together, proven\n\n\n\r\n\r\n  \r\n\r\nPf. Consider only one component (others are cylic).\r\n  \n\n\nSame for other components and join them together, proven\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \nWe have a triple product formula: .\nReplace these vectors with  and .\n\nWe have to explain the .\nIn this term you should not regard  as a common vector and this is not a common numerical product.\nInstead,  is an operator acting on scalar .\nWith the operator understanding, \nit will be easy to interpret why it becomes a gradient operator instead of a common product.\n\r\nDifferential operators require a function to act upon. The\r\nnabla operator always acts on the function to its right.\r\n  \n\n\r\n\r\n  \r\n\r\nPf.\r\n  \nLike this process:\n\nThe same, \n\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \n\nBecause\n\nthen\n\nSeparately simplify the four terms:\n\nFinally\n\n\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \n\nSimilarly for .\nFinally remove all  and join results from  and .\nThe formula will be proven.\n\n\n\r\n\r\n  \r\n\r\nPlease note that the   here is a dyadic tensor, not\r\ngradient.\r\nPf.\r\n  \n\n\n\n\n\n\n\n\n\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \n\n\n\n\n\r\n\r\n  \r\n\r\nPf.\r\n  \nThe components\n\nTake divergence\n\nJoin them together, then\n\n\n\n\r\n1.4 Dyadic Tensor\r\n  \n\n\r\nThis is in fact tensor product. The final space is   \n\n The\r\nresult is a 2-order tensor.\r\nTo interpret in components:   \n\n\r\n1.5 Levi-Civita Symbol\r\n  \n\nThen \n\r\n1.6 Two Theorems\r\nTheorem: If . then , \r\nPf. \n\nSuppose .\n\n\n\nsince , but .\n\nThen for .  2 terms .\n\n.\n\n\r\nTheorem: If , then . .\r\nPf. \n\n\r\n1.7 Application Examples\r\nWe offer some application examples here:\r\n \n\n\r\n\r\n   \r\n\r\n \n\n\nsince\n\n\n\nThen\n\n\n\r\n\r\n  \\left(dyadic\\right) \r\n\r\n\n\n\r\nSince\r\n\n\n\r\nThen \n\n\r\n\r\n  \\left(dyadic\\right) \r\n\r\n\n\n\nSince  then \n\n\n\r\n2. Static Field\r\n2.1 Static Electric Field\r\n\n\nNote that  is the absolute position of the measuring point, while  is the position of the charges.\n\r\nThe formula above can be written in a compact form. \n\n\r\nwhere \n\n is defined as electric potential,\r\nwhich is a scalar potential field.\r\n\nNote that the  does not operate , for  operates space. For a small field \n\n is a constant vector (fixed charge).\n\r\nNext we can calculate the curl and divergence \n\n\r\n\n\n\nTake an arbitrary volume including .\n\n\nAt , , but the integral is finite. Hence .\n Finally we get \n\n\r\n\r\nFrom the derivations above, we can get some important\r\nconclusions.\r\n\r\nPoisson’s Equation for static field: \r\nStatic electric field has a source but no curl.\r\n\r\nFurther, there’s Earnshaw’s law.\r\nEarnshaw’s law: Point charge cannot be in stable\r\nequilibrium in static electric field.\r\nPf. stable equilibrium .\r\nHowever , there’s a conflict.\r\n2.2 Static Magnetic Field.\r\nTo show direction of current, we can define current\r\ndensity .\n\n\n By charge conservation, in a closed\r\nvolume, \n\n Then \n\n\n\r\nThere’s an experimental law: Biot - Savart Law.\r\nBiot - Savart Law: For 2 current elements\r\n and  , the force between them is\n\n And magnetic flux density \n\n\nFurther show  in a compact form.\n\n\n\nSimilar to electric field, we can define a potential  for magnetic field. But  is a vector.\n\n\n\nFurther inspect .\n\n\n\n\r\n\n\n\nFor any function , \n\r\nHence  and \n\n\r\n\nFor the 1st term, because of \n \nThis term includes the entire space, so we can take a sphere with \n\nSince  at infinite far away points, the 1st term collapses to 0.\n\r\nMeanwhile  (no charge here), hence \r\nFor the 2nd term, \n\n Finally, \n\n\r\n2.3 Lorentz Force\r\n\nUnder all inertial systems, Lorentz force takes the same form.\n\n\n\nFor current\n\n\n\nIn general, \ncurrent density  is propotional to the force per unit charge:\n\n is the Lorentz force (mentioned in 2.3).\nPlug in force,\n\nOrdinarily the velocity of charges is sufficiently small and the term of magnetic field can be ignored.\n\nwhere  is called conductivity.\n\r\nThis formula is called Ohm’s law.\r\n\nIt may be confusing that we know the electrofield inside a conductor is 0.\nBut there is current inside connecting wires made of metal,\nwhich seems to violates Ohm's law.\nHowever, \nthis conclusion is just an approximation.\n of a conductor is so large that only a very small field can drive large current.\nHence in most cases, \neven if the charge are moving, \nthe field inside can be ignored.\n\r\n2.4 Multipole Expansion\r\nWhen you’re very far away from a localized charge distribution, if\r\n, it can be approximated as a point charge. But if\r\n, to make things more precise, the charge distribution\r\nshould be expanded as multipole.\r\n\r\n\r\n\r\n\r\nDifferent Multipoles\r\n\r\nWe now deduce the potential of multipoles. At certain point\r\n, the potential is given by\r\n\n\n\r\nUsing the law of cosines.\r\n\n\n\r\nDefine , then .\r\n\r\n\r\n\r\n\r\nField of a Charge Distribution\r\n\r\nApply Taylor expansion\r\n\n\n\r\nPlug  into the expansion,\r\n\n\n\r\nSurprisingly, the coefficient of the  terms are\r\nLegendre polynomial. We conclude that\r\n\n\n\r\nThe terms in the expansion above indicate the contributions of\r\nmultipoles.\r\n term is the monopole contribution.  term is dipole,  quadrupole,  octopole, etc.\r\nOrdinarily, the expansion is dominated by monopole, which is just a\r\npoint charge.\r\nIf , the dominant term will be the dipole,\r\n\n\n\r\nWe define\r\n\n\n\r\nas the dipole moment. The dipole contribution simplifies to\r\n\n\n\r\nGenerally, if you move the origin of the coordinate, \r\nwill also change\r\n\n\n\r\nThe field of a dipole is\r\n\n\n\r\nAlso, we can extract the contribution of quadrupoles.\r\n\n\n\r\nThe two integrations can be combined into one term.\r\n\n\n\r\nThe quadrupole moment tensor is defined as\r\n\n\n\r\nObviously , hence , this is a symmetric\r\ntensor.\r\nIt’s easy to verify  to be traceless.\r\n\n\n\r\nFurther, to get a more compact form, expand  with a\r\nmore compact form of Taylor expansion.\r\n\n\n\r\nIn static potential\r\n\n\n\r\nThe contribution of quadrupole\r\n\n\n\r\nFor the operator,\r\n\n\n\r\nPlug this term into .\r\n\n\n\r\nwhere \n\n\r\nThe electric field produced by a quadrupole is:\r\n\n\n\r\nConsider .\r\n\n\n\r\nFirst calculate gradient of quadratic form \n\n\r\nThen \n\n\r\nIf a quadrupole is placed in the external electrical field\r\n, the energy should be\r\n\n\n\r\nThe monopole (total charge) and dipole of a quadrupole are both 0.\r\nHence there’s only one term left.\r\n\n\n\r\n3. Dynamic Field\r\n3.1 Dynamic Electrical Field\r\n\nFaraday discovers that the variation of  will generate electromotive potential.\n\n\n\nwhere  is the magnetic flux.\n\n\n\nWe can deduce the differential form.\n\n\n\nRemove , we have\n\n\n\r\n3.2 Dynamic Magnetic Field\r\n\nIf we take variation of  into consideration, we will find a conflict.\n\n\n\nBut take divergence to both sides.\n\n\n\nThis means , but it's not necessary in physics. Charge conservation indicates that\n\n\n\nTo cancel this conflict, we need a new term.\n\n\n\n\nwe have\n\n\n\nthen\n\n\n\n\n\n\n\r\n3.3 Maxwell Equation in Vacuum\r\nFrom the derivation above, we can summarize Maxwell’s equations:\r\n\n\n\n\nNote that in the last integral form, \nthe term \n \nis often written as , \nwhere  is the electric displacement field. \nSimilarly, \nthe magnetic field integral equation is sometimes expressed using the magnetic field intensity \n instead of , \nwhere  in a vacuum. \nThe image shows the latter using .\n\r\n3.4 Completeness of Maxwell\r\nEquation\r\nCompleteness: Given the initial and boundary condition, the\r\n are uniquely determined.\r\nPf. Given initial condition. \n\n\n\nboundary condition\n\n\n\nDefine\n\n\n\nThen\n\n\n\nAccording to Maxwell's Equation\n\n\n\nDefine\n\n\n\nThen\n\n\n\nWith the initial condition, . Hence .\n\r\n4. Fields in Matter\r\n4.1 Polarization\r\nA body with equal positive and negative charge (totally neutral) is\r\ncalled a dipole. When a dipole is placed in , it will be\r\npolarized.\r\nThe dipole has a dipole moment of:\r\n\n\n\r\nis called polarizability, depending on the structure of the dipole.\r\nIf the distance between positive and negative charge is\r\n, then the moment can also be written as:\r\n\n\n\r\nThe direction of  is from negative to positive.\r\nTake a polarized atom for example:\r\n\r\n\r\n\r\n\r\nPolarized Atom\r\n\r\nThe field produced by negative charge is:\r\n\n\n\r\nThe positive remains stationary at equilibrium. Hence:\r\n\n\n\n\n\r\nThe polarizability is therefore:\r\n\n\n\r\nFor more complicated molecules,  and \r\nmay not be in the same direction. The  is expanded as a\r\ntensor:\r\n\n\n\r\nThe torque by external field is:\r\n\n\n\nIn uniform field \\left(or  is very small\\right):\n\n\n\r\n\r\n\r\n\r\n\r\nForce on a Dipole\r\n\r\nIn non-uniform field, assume  is very small. The total\r\nforce is:\r\n\n\n\r\nThen torque in non-uniform field:\r\n\n\n\r\nAnd the energy of a dipole in field  is (suppose\r\n at the center):\r\n\n\n\r\nIn uniform field:\r\n\n\n\r\n4.2 Potential and Force of a\r\nDipole\r\nNow it comes to the field produced by a dipole.\r\nThe potential is:\r\n\n\n\r\nand\r\n\n\n\r\nExpand with Taylor series (for ):\r\n\n\n\r\nIgnore second-order term:\r\n\n\n\r\nThen:\r\n\n\n\r\nAnd electric field:\r\n\n\n\r\nExample: Two interacting dipoles \r\nConsider , \r\n\n\n\r\n4.3 Macroscopic Polarization\r\nIf an object contains many dipoles,  is defined as the\r\ntotal polarization:\r\n\n\n\r\nThe potential becomes:\r\n\n\n\r\nwhere  are the positions of field point and source\r\nrespectively.\r\nReassign it:\r\n\n\n\r\nThe two terms are separately the potential produced by surface and\r\nvolume charge. Hence \n\n\r\nWe did not take any free charge into consideration. All charge\r\nmentioned above is generated by polarization. They’re trapped in the\r\nobject and cannot move freely. Such charge is called bounded charge.\r\n4.4 Electric Displacement\r\nWe now take free charge into consideration.\r\n\n\n\r\nBy Gaussian law:\r\n\n\n\r\nFor convenience, define:\r\n\n\n\r\nas the electric displacement, then Gaussian law is simplified:\r\n\n\n\r\nIn integral form:\r\n\n\n\r\nFor curl (ignore  first):\r\n\n\n\r\nFor many substances,  is proportional to\r\n:\r\n\n\n\r\nThen:\r\n\n\n\r\nIn another view, permittivity changes with substance:\r\n\n\n\r\nIf the substance is not isotopic, the  in different\r\ndirections will change. Then  will be expanded as a\r\ntensor:\r\n\n\n\n\n\r\nThe energy should be:\r\n\n\n\r\n4.5 Magnetization\r\nAll magnetic phenomena are due to electric charges in motion. Like\r\nelectric polarization, when a magnetic field is applied, a net alignment\r\nof magnetic dipoles (produced by polarized atom) occurs, then the medium\r\nis magnetized.\r\nThe magnetization is not always the same as external .\r\nSome are parallel (paramagnets), some are opposite (diamagnets). A few\r\nsubstances retain the magnetization even after the external field has\r\nbeen removed.\r\nA magnetic dipole is a small current loop. The simplest example is a\r\nrectangular dipole.\r\n\r\n\r\n\r\n\r\nRectangular Magnetic Dipole\r\n\r\nThe torque in a uniform field in the z direction:\r\n\n\n\r\nwhere  is the normal vector of the dipole plane. For\r\nnon-rectangular dipoles, we can fill the area with many rectangular\r\nsides.\r\nAdjacent sides always have opposite current, so they cancel each\r\nother.\r\nThe sides next to the edge compose the original current loop. Then\r\nthe torque can be expanded:\r\n\n\n\r\n\r\n\r\n\r\n\r\nNon-rectangular Dipole\r\n\r\nwhere ,  is the area.\r\nFor a current element in , force applied by field\r\nis:\r\n\n\n\r\nThen for a macroscopic current:\r\n\n\n\r\nFor a current loop in uniform field:\r\n\n\n\r\nIn non-uniform field, suppose an infinitesimal circular wire ring of\r\nradius  carrying a current :\r\n\n\n\r\nHence the energy:\r\n\n\n\r\nAlso, it comes to the dipole’s field and perturbation.\r\nFor current density :\r\n\n\n\r\nIn a dipole, :\r\n\n\n\r\nSince :\r\n\n\n\r\nAnd field:\r\n\n\n\r\n4.6 Macroscopic Magnetization\r\nFor an object, define:\r\n\n\n\r\nas the total magnetization. With , the potential of\r\nthe object:\r\n\n\n\r\n(due to formula )\r\nThe first term looks like the effect of volume:\r\n\n\n\r\nThe second term looks like the effect of surface:\r\n\n\n\r\nThen:\r\n\n\n\r\nThe  and  are bounded current in the\r\nvolume and surface, respectively.\r\nNow it’s time to take free current into consideration:\r\n\n\n\r\nBy Ampere’s law:\r\n\n\n\r\nDefine:\r\n\n\nas an auxiliary field, then:\n\n\nIn integral form:\n\n\n\r\nThe divergence:\r\n\n\n\r\nonly when , \r\nSimilar as electric field,  is always linear with\r\n:\r\n\n\n\r\nThen:\r\n\n\n\n is called permeability, and  is called susceptibility.\n\nWhen the substance is not isotropic,  is always expanded as a tensor:\n\n\n\r\n4.7 Maxwell Equation in Matter\r\nWe first list Maxwell equation in vacuum: \n\nIn matter, \n and  will be replaced with  and .\nAccording to chapter 5 and chapter 6, \nGaussian law is replaced with\n\nAmpere law of loop is replaced with\n\n\r\n4.8 Boundary Conditions\r\nAt the boundary of different substances,  are not\r\ncontinuous.\r\nTake a thin cylinder crossed by the boundary, with bottom area\r\n, infinitesimal height . According to\r\nGaussian theorem:\r\n\n\nand\n\n\r\nAnd:\r\n\n\n\r\nTake an Ampere loop with infinitesimal width , crossed\r\nby the boundary, length .\r\nBy Faraday’s theorem:\r\n\n\nthen\n\n\r\nBy Ampere’s law of loop:\r\n\n\n\r\nThis gives:\r\n\n\nfinally\n\n\r\n5. Energy\r\n5.1 Energy of Electric Field\r\n\nFirst we research on discrete charge cases. For 2 charges, the work to move  from far away to the position of  \\left(let  to be the original point\\right) is:\n\n\n\nBring more charge in.\n\n\n\nThe final work done should be the sum of all works.\n\n\n\nRemove repeated terms.\n\n\n\nwhen the discrete charges become continuous,\n\n\n\nwhere  is the field point and  is the position of field source.\n\nApply Maxwell equation.\n\n\n\nSo\n\n\nBut , then\n\n\nWe're integrating over all space and at infinite far away .\nSo the first term\nthen\n\r\n5.2 Energy of Magnetic Field\r\n\nSuppose there're two loops at rest.\nRun a stady current on loop 1, \nit produces a magnetic field .\nThe Biot-Savart law gives\n\nThe magnetic field is proportional to current .\nAnd so too is the flux of loop 2\n\n is called the mutual inductance from loop 1 to loop 2.\nNow plug in the magnetic potential:\n\nExtract \n\nWe can find an amazing result:\nthe formula of  is symmetric to  and .\nHence, we can say \n\nWe then drop the subscripts and call both of them .\n\r\n\nWith the inductance, \naccording to Faraday's law, \nmagnetic field produced by loop 1 generates emf in loop 2:\n\nFurthermore, \nthe current applied on loop 1 no longer generates electromotive force in loop 2, \nbut also generates in loop 1 itself.\nOnce more, it is also propotional to current:\n\nThis new coefficient  is called self-inductance.\n\nIn an inductor, the electromotive force can be expressed with\n\nTotal work done per unit time is\n\nSo\n\n\r\n\nFurthermore,\n\nHence\n\nBy Maxwell's equation,\n\nSo\n\nSimilarly,  at infinite far away.\n\n\r\n5.3 Poynting Theorem\r\nand Energy Conservation\r\nExtract 2 equations from Maxwell equations:\r\n\n\n\n\r\nMultiply as follows, \n\n\n Subtract \n\n\r\nReassign this equation we get \n\n We define \n\n\r\nto be the energy density of EMF (Electromagnetic\r\nField). And \n\n to be the Poynting vector,\r\nrepresenting the energy flow density.\r\n\nThe  can also be assigned as follows\n\n\r\nHence Poynting’s theorem can also be expressed by .\n\n\n is obviously the work done by Lorentz force. This equation indicates that\n\n\n\r\n5.4 Energy Flow in Matter\r\nWhen in matter, repeat the process above but with permittivity and\r\npermeability replaced:\r\n\n\n\n\r\nMultiply as follows, \n\n\n Subtract \n\n\r\nReassign this equation we get \n\nThis is the form of Poynting theorem.\nThe Poynting vector\n\nand the energy density\n\n\r\n6. Momentum\r\n6.1 Maxwell’s Stress Tensor\r\n\nWe have known the force density\n\ntotal force\n\n\r\nWe propose to express it with field alone. \n\n\nTo simplify , introduce Maxwell Stress Tensor \n\n\n\r\nHence\r\n\n\n\r\n\nwith component\n\n\r\nThe force density then simplified to \n\n Integrate to get\r\ntotal force \n\n\r\n6.2 Conservation of Momentum\r\n\nWe denote  as mechanical momentum. \nIn an isolated space\n\nThis is a standard form of conservation function.\n\\left(variation of density = Flux out of the volume\\right)\n\nSo the term\n\nmust also be a momentum. \nIt is carried by EMF. Define\n\nas the density of EMF momentum. \nThen we have\n\n\r\n6.3 Angular Momentum\r\n\nBased on momentum, introduce angular momentum,\n\n\r\n","categories":["physics"]},{"title":"Quantum Mechanics","url":"/categories/physics/quantum/","content":"Note:\r\n\r\nBefore you read this article, please make sure you have learned\r\nLinear Algebra, Classical Mechanics and\r\nprobability.\r\n\r\nQuantum Mechanics is built upon five fundamental postulates. These\r\npostulates often pose a significant challenge for beginners because they\r\nare not derived from any prior principles—they are axiomatic. Why do we\r\nadopt them? The ultimate justification is empirical: all\r\ntheoretical predictions derived from these postulates show remarkable\r\nagreement with experimental results. Therefore, while their\r\ninternal “why” may be elusive, we accept them as the foundation of the\r\ntheory.\r\n1. State and Superposition\r\n  \nIn classical mechanics, \nthe state of a particle is accurately described by the position and momentum .\nIn quantum mechanics, however,\nexperimental results indicate that states is inherently probabilistic.\nWe can only describe the probability of finding a particle at a given location.\n\r\nTo make the particle mathematically processable, state\r\nvectors are introduced. To represent these vectors, we use\r\nDirac symbols.\r\n  \nA quantum state is represented by a ket , \n is just a symbol to indicate which state it is.\nA ket is a vector in Hilbert space.\nIts Hermitian conjugate denoted as , called a bra\n(Imagine transposing a column vector and taking the complex conjugate of its elements).\n\r\nIf you don’t know what is a Hilbert space, just check this link: CHAPTER 3 -\r\nHilbert Spaces - MIT\r\nFor example:   \nSchrödinger explains superposition principle with a cat. \nBefore the box is opened, \nwe can say the cat is under a state of \n\r\nSuch a ket vector exists in a Hilbert space, with infinite\r\ndimensions. Inner product is defined as:   \n\n The result is a\r\nscalar.\r\n  \nKet vectors may be multiplied by complex numbers and added together to get another ket vector:\n\nwhere  and  are two complex numbers. \n\r\nPostulate of State: Each state of a dynamical system\r\nat a particular time corresponds to a ket vector in Hilbert space. If a\r\nstate results from the superposition of certain other states, its ket is\r\nrepresentable by the component kets, and vice versa.\r\nWe need to emphasize the superposition principle:\r\nsuperposition Principle:   \nIf  and  are possible state of a system, \nthen the superposition of the states\n\nis also a possible state.\n\r\nTake the example above,   \n is the superposition of  and .\n\r\nThis postulate leads to superposition symmetry: When two or more\r\nstates are superposed, the order in which they occur in the\r\nsuperposition process is unimportant.\r\nproposition:   \n and  ( and ) are the same state.\n\r\nWhy? Any complex number can be decomposed according to Euler’s\r\nformula:  \n\n The magnitude component will finally be cancelled\r\nby normalizing. Meanwhile, for phase component, since all physically\r\nmeasurable results are related to the probability modulus  \n\n,\r\nwhere  \n\n is another state, representing the eigenstates of\r\nthe measured quantity. (You will know it in the following chapters)\r\nSuppose state A is imposed a global phase, this phase term\r\n will be squared and take the magnitude, whose result is\r\nfinally 1, bringing no effect.\r\nBy the way,   \nif , \nbut ,\nthe result will be nothing at all.\nThen the two components have cancelled each other by an interference effect.\n\r\n2. Operators\r\nFunctions in Hilbert space are represented as operators.\r\n  \n\n You can understand it by imagining matrices and vectors\r\n(but remember operators Hilbert space are not always linear). We make\r\nthe rule that the ket must be put on the right of the operator.\r\nFor linear operators:   \n \n\n\n\n\n\r\n  \nIn general .\nDefine commutator:\n\nWhen , \nthe two operators commute.\n\r\nWe have known inner product  .\nNow take .\nThis is another linear operator that can operate on other kets\n(imagine , the result is also a ket.)\n\r\nOperators can also be conjugated:   \n\n\n\r\n  \nIf ,  is called a Hermitian operator.\nHermitian operators have real eigenvalues and can be diagonalized under their eigenbasis.\n\r\nThere is a theorem:\r\nTheorem: If  is a real linear operator and\r\n, then .\r\n  \nProof: Take first case when .\n\nThis gives .\n\r\n  \nWhen  and let \n\nThen . Apply result from , we get\n\nRepeat these steps and we will get the final result.\n\r\n3. Observables\r\n  \nIf a linear operator  and a value  satisfies\n\nThen  is the eigenstate of \nand  is the corresponding eigenvalue.\n\r\nIt’s obvious that physical results of observation results must be\r\nreal. We assume that once we measure a quantity the state\r\ninstantly collapses to one of the operator’s eigenstates, randomly, and\r\nwe get the corresponding eigenvalue as the measurement result.\r\nThen we can know all observables are Hermitian\r\noperators.\r\nNow we will develop the theory for Hermitian operators.\r\n\r\nThe eigenvalues are all real numbers\r\n\r\nPf.   \nSuppose .\nMultiply  on the left.\n\nTake conjugation,\n\nThen ,  is real.\n\r\n\r\nThe eigenvalues associated with eigenkets is the same as the\r\neigenbras.\r\nThis conclusion is obvious to be deduced from the first one.\r\nNeglected here.\r\nThe conjugate imaginary if any eigenkeet is an eigenbra belonging\r\nto the same eigenvalue, and conversely.\r\n\r\nInstantly we can deduce: If we have two or more eigenstates of a real\r\ndynamical variable belonging to the same eigenvalue, then any state\r\nformed by superposition will also be an eigenstate.\r\nWe therefore have a theorem:\r\nTheorem: Two eigenvectors of a real dynamical variable\r\nbelonging to different eigenvalues are orthogonal.\r\nPf.   \nLet  be two eigenkets of the real dynamical variable .\n\nTo prove .\nSeparately multiply  on the left.\n\nTake the conjugation of the second equation, \n\nThen we get\n\nSince , then \n\r\nThis theorem indicates that an observable has a set of orthonormal\r\neigenstates. Furthermore, these eigenstates are not only orthonormal,\r\nbut also complete, which means any state in Hilbert state can be\r\nrepresented by this set of eigenstates (eigenbasis).\r\nThis means   \n\n\r\nWe just leave this equation here, temporarily.\r\nNow we can introduce postulate 2:\r\nPostulate of Measurement: Each physical quantity is\r\nrelated to an observable, which is mathematically a Hermitian operator.\r\nWe assume that once we measure a quantity the state instantly collapses\r\nto one of the operator’s eigenstate, randomly, and we get the\r\ncorresponding eigenvalue as the measurement result.\r\n  \nNote that measurement can change the state.\nIf we measure a state with a Hermitian operator , \nsuppose we get the eigenvalue  and state collapses to .\nBut when we measure it with  again, \nwe will definitely get the same result.\n\r\n4. Probability\r\n4.1 Postulate of Probability\r\n  \nRecall the formula:\n\nBased on this formula, \nBohn raised the postulate of probability:\n\r\nPostulate of Probability:   \nIn one measurement of , \nthe probability to get the value of  is \n\n\r\n \nLet's explain the physical meaning of .\nAccording to postulate of probability, \n\n\r\nHence, the coefficient is in fact the square root of the probability\r\nthat the corresponding eigenvalue is measured.\r\n \nWe can also extract an equation:\n\nThen any state can be written only by the eigenbasis of a Hermitian operator:\n\nReassign it, \n\nThere is an amazing result:\nState  remained unchanged after operated by \n\nObviously, this is a unit operator .\n\nIf we take continuous part into account:\n\n\r\nThis is called Spectral Decomposition Theorem.\r\n \nFurthermore, one term of the spectral Decomposition theorem, \n\nis called a projection operator.\nIt projects a state to the direction ,\nwith a coefficient of .\n\r\nStill, we can also expand the spectral Decomposition theorem to any\r\nHermitian operator.  \nWith continuous spectral, \n\nApply  on \n\nSince  is randomly taken, \n\n\r\n4.2 Probability Current\r\nLike electric current indicates the variation of charge in a closed\r\nvolume in a unit time, probability current indicates that of probability\r\ndensity.  \nElectric current satisfies continuity equation,\nthen similarly, \nprobability equation also satisfies \n(since charge and probability density remain constant,\nthey neither arise from nothing nor vanish into nothingness) \n\n\nSchrödinger equation (in chapter 9) gives\n\ntake conjugation\n\n\ncalculate ：\n\nplug in the Schrödinger equation and its conjugation\n\nterms with  cancel each other\n\nthen\n\nnote that\n\nhence\n\ncompared with continuity equation  proven.\n\r\n \nThe proof above gives the calculation of probability current:\n\n\r\n5. Expectation\r\nIf we take multiple measurements on a state , we may\r\nget different results each time. So what’s the mean number? This\r\nintroduces the expectation.\r\n \nA quantity, , has expectation, \nwhich is the weighted sum of each result.\n\n\r\nThe variance of an observable, , measures the spread or uncertainty in the measurement results. \nIt is defined as the expectation value of the squared deviation from the mean:\n\n\r\nA crucial result in quantum mechanics is that if a system is in an\r\neigenstate of an observable, the variance of a measurement of that\r\nobservable is zero. This means there is no uncertainty; the measurement\r\nwill always yield the corresponding eigenvalue. For example, if a system\r\nis in an energy eigenstate  of the Hamiltonian \nthe variance of its energy measurement is zero.\nThis is because, in an eigenstate, \nthe value of the physical quantity is precisely defined. \nThe following calculation demonstrates this for the Hamiltonian operator:\n\n\r\nThis result indicates that an isolated quantum system has\r\nfixed, precisely determined energy.\r\n6. Uncertainty\r\n\nRecall commutators:\n\nWe must first prove commuting operators have the same orthonormal eigenbasis.\r\nSuppose .\r\n\r\nStep1: Non-degenerate eigenstates of  (one eigenvalue corresponds to only one eigenstate)\n\nSuppose  is an eigenstate of .\nLet  operate on this state, \nand then let  operate on it again:\n\nThis equation indicates that  is also an eigenstate of , \nwith the same eigenvalue .\nSince  is non-degenerate, \nthis implies that no other state, \napart from a constant multiple, \nis an eigenstate of  with eigenvalue . \nTherefore, \nthe new state  must be identical to  \n(differing only by a constant).\n\r\nStep2: Degenerate State\r\n\r\n\n  Suppose  is -multiple degenerate,\n  corresponding to .\n  These states span to be a -dim subspace, called .\n  Any state  in this state satisfies \n\n  The same as before, consider .\n\n  \n\n   is still an eigenstate with eigenvalue a. Hence it must also belong to . We can say  keeps .\n\n  Since  is Hermitian, then in  there is a new set of basis, which is also a set of eigenstates of . Let them to be . They satisfy\n\n  \n\n  Meanwhile . Proven.\n\r\nThen we can have the uncertainty principle:\r\nUncertainty Principle: States for any pair of physical\r\nproperties whose operators do not commute, it is impossible to know both\r\nproperties with perfect accuracy at the same time.\r\nPf. \nFirst, we define the standard deviation.\n\nNow take two standard deviations and multiply them together. In mathematics, we have the Cauchy-Schwarz inequality.\n\nIn Hilbert space, it still holds.\n\nNow, let  and .\n\nSimplifying this, we get:\n\nBecause of:\n\n is an anti-commutator. Since  and .\nThen  is real and  is imaginary. We finally have:\n\n\r\nThe essence to this is that if two operators do not commute, they\r\ndon’t have same eigenstates. So when you apply these two operators, the\r\nstate does not know which state to collapse. This “confusion” brings the\r\nuncertainty. It is not measurement error. Instead, it is a critical\r\ndifference between quantum world and classical world.\r\nExample: Momentum and Position\r\n\nA free particle satisfies Schrödinger equation\n\nSolve the equation, \nwe can find that a free particle can be described by a planar wave:\n\nDe Brogile told us \n\nIgnore the time (stationary), \n\nThe momentum operator should satisfy the eigenfunction:\n\nTake derivate for both sides:\n\nReassign, \n\nHence we finally have\n\n\nThere is a most famous example of uncertainty between  and .\n\nThe two operators do not commute, \nso they cannot be measured at the same time.\n\n\r\n7. Functions of Observables\r\n\nIn this chapter, \nto unify discrete and continuous eigenstates, \nwe will use  to indicate eigenstates of \ninstead of .\n\r\nIf  is an observable, \nthen any function, , \nis also an observable.\nWe do not need to require  to be real, \nfor we can separate  to two parts: real part and imaginary part, \nwhich is two real operators.\nWhen  is measured, \nthe two parts are also measured, automatically.\n\r\n\nBased on the discussion above, \na postulate can be raised naturally.\n\nOr we say an operator and its function have the same eigenstates.\nThe root reason is that when the operator is measured, \nthe function is automatically measured.\nThe function itself does not measure states, \ninstead it just receives result from .\n\r\nWe try to raise a theorem:\r\nTheorem: If  is an observable, then any function  is also an observable.\r\nPf. \nWe can suppose  is real safely.\n(reason is above) \nExpand  with spectral theorem\n\nThen  can be defined as\n\nWe need to prove  is Hermitian.\n\n\r\nThere is also an important theorem called Hellmann-Feynman\r\nTheorem:\r\nTheorem: \n is a Hamiltonian depending on a continuous parameter . \n is its eigenstate, with eigenvalue of . \nThen\n\n\nPf.\n\nSince , \n\n\n\n\r\n8. Wave Function\r\nUnder specific representation (e.g. displacement), abstract state\r\nvector can be represented by wave function.\r\n\nLet's take displacement operator  for example.\n\nUnlike previous operators, \n can take continuous values,\nso \n\r\npossesses a continuous eigenbasis.\r\n\nUnder displacement representation, \nthe wave function can be represented by\n\nSimilar to the  before, \n, \nUnder continuous eigenbasis, \naccording to postulate of probability, \n becomes the probability density\nof finding the particle at .\n\r\n\nWith wave function, \nthe inner product and expectation can also be written in integral:\n\nHere  operates on .\n\n\r\nThe normalization condition turns to \n\n\r\nPhysically, wave function has some conditions:\r\n\r\nWave function must be continuous\r\nThis condition originates from the physical requirement. If the wave\r\nfunction jumps at a certain point, the probability density at that point\r\ncannot be determined. This is unacceptable in physics.\r\nFirst derivative of wave function must also be continuous:\r\nThis condition originates from Schrödinger equation. If this\r\ncondition is not satisfied, there will be a delta function. When the\r\npotential is a finite function, the equation cannot hold.\r\n\r\nExample: One-dimension Infinite Potential Well\r\n\nOne-dimension Infinite Potential Well means a potential well that\n\nInside the well there is an electron.\nObviously, the electron will never appear outside the well.\nThen\n\nSince the wave function is continuous, \nit must be 0 at the boundary, \nor the wave function will take a non-zero value outside the well, \nwhich leads to a conflict.\nSo\n\nInside the well, Schrödinger equation is simplified to \n\nThis equation has a general solution:\n\nApply the boundary condition,\n\nWe can then get the eigenvalue of energy:\n\nNormalize the wave functions, \nwe have \n\n\r\nThis example indicates that boundary conditions (standing\r\nwave) must lead to quantization.\r\nThere is also a question: Why the electron in the well must be in\r\nmotion, instead of remaining stationary? \nRecall the principle of uncertainty.\nIf the electron remains stationary, \nthe momentum  must not be zero, \nwhich forces the electron to move.\nHowever, \nonce in motion the displacement  \nis driven away from 0.\nThis causes a conflict.\n\r\n9. Time Evolution\r\n9.1 Time-dependent\r\nSchrödinger Equation\r\nWe mentioned that states must remain normalized before, which is\r\n\n\nSo, with time evolution, \nthe states must also remain normalized.\nThis introduces a postulate:\n\r\nPostulate of Evolution: Time evolution does not\r\nchange the magnitude of states.\r\n\nIt means a state at time  depends on the initial state .\nRecall the superposition principle, \ntake two initial states  and , \nthen the linear combination\n\nis also a valid initial state.\n\r\n\nFurther, time evolution is a process to map initial state \nto a state at a later time.\nWe can use an operator to represent evolution.\n\nOperate this operator on the combined state:\n\nSo the time evolution operator is a linear operator.\n\r\n\nWhat's more, \nby the postulate of evolution, \nthe magnitude before and after evolution remains one.\nThen\n\nso \n\nThis indicates that time evolution operator is unitary.\n\r\n\nWe pass to the infinitesimal case by making  and assume the limit\n\nexist.\nThis limit is just a derivative\n\nIn this infinitesimal evolution, \nthe limit operator is close to unit operator .\nWe can write the operator as \n\nSince  is unitary, \nplug in the unitary condition.\n\nIgnore the high order term, \nand cancel ,\n\nHence  is anti-Hermitian.\nNote that the limit operator is just , \nso the equation turns to \n\nSince  is anti-Hermitian, \nit only has pure imaginary eigenvalues.\nWe multiply by a pure imaginary number  to get a real operator.\n\n is then a Hermitian operator.\nThe equation becomes\n\n\r\nThis operator is called time-dependent Schrödinger\r\nequation.\r\n\nThere is still a problem yet to be solved:\nWhat is the physical meaning of the operator ?\nIt is Hermitian,\nso maybe it should be a physical observable.\nIn fact experiments have verified that  is the total energy of the system (Hamiltontian).\n\r\n\nA system with time-independent Hamiltonian is the most trivial case.\nSolving the equation in this case, \nwe finally get the solution\n\nwhere the time evolution operator\n\n\r\n9.2 Stationary Schrödinger\r\nEquation\r\nFrom the time-dependent Schrödinger equation, we can deduce the\r\ntime-independent one under displacement representation.\r\n\nTo derive the time-independent form, we assume the Hamiltonian  is time-independent. This allows us to separate the time and spatial parts of the state vector. We can express the state vector as the product of a time-independent ket  and a time-dependent scalar function :\n\n\nSubstituting this form into the time-dependent Schrödinger equation:\n\n\nSince  and  are time-independent, we can simplify this expression:\n\n\nTo separate the variables, we divide both sides by . The left side depends only on time, while the right side depends only on the state. For this to hold true, both sides must equal a constant, which we call the energy  of the system.\n\n\nThe time-independent part gives us the abstract time-independent Schrödinger equation:\n\n\nTo get the equation in the displacement representation, we left-multiply by the position eigenstate :\n\n\nBy definition,  is the time-independent wavefunction in position space.\n\nThe Hamiltonian is the total energy operator, consisting of kinetic and potential energy:\n\n\nIn the position representation, the momentum operator  becomes a differential operator, and the position operator  becomes a multiplicative operator. Therefore, the Hamiltonian operator takes the form:\n\n\nSubstituting this into our equation, we obtain the time-independent Schrödinger equation in the position representation:\n\n\nThis differential equation describes the stationary states of a quantum system with a time-independent Hamiltonian. Its solutions, , are the wavefunctions of these states, and the eigenvalues, , are the corresponding possible energy values.\n\r\nExample: Spin of Electron\r\n\nAn electron has two spin states:\n toward  and  toward , \nwith energy of  and , \nrespectively.\nGiven the initial state \n\nThis state is in fact .\nWith time evolves, \n\nLet's find the probability at  after time .\nWe need to figure out the coefficient first:\n\nThen\n\nBy the way, \nsince  and  can form a complete orthonormal eigenbasis, \nit's natural to find that  and  can also form a complete orthonormal eigenbasis (symmetry).\nIf we take  and  as the eigenbasis, \naccording to the completeness, \nwe know the probability of  by subtracting  from 1:\n\n\r\n9.3 Non-Hermitian\r\nSystem with Imaginary Potential\r\nIn standard quantum mechanics, observables are required to be\r\nHermitian. However, in an open system, Hamiltonian may not be Hermitian\r\ndue to the particle or energy exchange with the environment.\r\n\nLet's consider an imaginary potential\n\nThis equation gives solution\n\nwhere  means the wave going right and  means going right.\nThe probability current\n\nThe current decays in the outgoing direction (negative exponenetial).\nWhat's more, \ndecaying of the current always indicates absorption.\nDefine absorption coefficient\n\nHence, \nimaginary potential always means there is absorption.\n\r\n10. Recap of Postulates\r\n\r\nState: Quantum state is represented by a vector in\r\nHilbert space.\r\nMeasurement: A observable corresponds to a\r\nHermitian operator. Measurement gets one of the eigenvalues as the\r\nresult. The state collapses to the eigenstate after measurement.\r\nProbability: The probability to get a eigenvalue\r\ndepends on the square of magnitude of wave function.\r\nTime Evolution: Evolution of state is determined by\r\nSchrödinger’s equation.\r\nHomogeneity: Two same type of particles are\r\nindistingushable. (See this in Thermodynamics\r\nand Statistical Mechanics)\r\n\r\n11. One Dimension Harmonic\r\nOscillator\r\n11.1 Raising and Lowering\r\nOperator\r\n\nLike the classical oscillator of , \nin quantum system,\n\n(For more complicated systems, \napply Taylor expansion near the point)\n\r\n\nSchrödinger equation of harmonic oscillator is\n\nwhere Hamiltonian\n\nDefine two operators:\n\r\n\r\nRaising Operator \n  \n  \r\nLowering Operator \n  \n  \r\n\r\n\nIt's obvious that the two operators are mutually Hermitian.\nWe multiple them together:\n\nThen we have \n\nThe commutator is also clear:\n\n\r\nLet’s focus on the physical meaning of “raising” and “lowering”\r\nnow.\r\n\nWe have known \n\n\nTake inner product \n\nNext, let's determine the normalization constants.\n\n\n\nNow let's consider the raising operator.\n \nWe need to apply the commutator here.\n\nWe can find the normalization constant for the creation operator.\n\nTherefore,\n\n\r\n11.2 Energy Levels\r\n\nSuppose\n\nThen\n\nThe raising/lowering operator operates the eigenstates and raises/lowers them to the next energy level. In a harmonic oscillator, the increment is .\n\nPhysically there's a ground state. Hence, there must be a state , satisfying\nFor ground state.Hence the ground energy iswith increment of .\nThough there is only one praticle in the problem, \nwe can also regard the  as an \"energy quantum\".\nOn this view, \nthe raising and lowering operators are also called \n\"creation\" operator and \"annihilation\" operator, \nfor they add to or remove a particle from the system.\n\r\nSuch view is more ferquent in multi-body systems. You can see it in\r\nThermodynamics\r\n3 - Quantum Statistical Mechanics\r\n11.3 Occupation Operator\r\n\nDefine occupation number\n\n\nindicating the main quantization number.\n\r\n\nWith  all eigenvalues have been found.\nSuppose , .\nthen\n\nThis is a contradiction to the positivity of eigenvalue.\n\r\n11.4 Wave Function\r\n\nLet's start from the ground state first.\n\nThat means\n\nSolve this equation, \n\nNormalize in the entire space\n\nFianlly\n\nFor states with higher energy, \nwe need to use raising operator.\n\n\r\n11.5 Tunnelling\r\nIf we check the wave functions of excited particles (energy higher\r\nthan ground), we can find a critical difference between classical\r\nmechanics and quantum mechanics: The particle can appear at positions\r\nwhose potential is higher than the state energy.\r\n\r\n\r\n\r\n\r\nSketch of Tunnelling\r\n\r\nThis phenomenon is called tunnelling.\r\n11.6 Average of Potential\r\nEnergy\r\n\nFor any  state , \n\nWe can represent  with raising and lowering operators:\n\nBy orthonormality, \n\nSo\n\nHence we can get the kinetic\n\n\r\nExample: \nAn electron confined in a harmonic oscillator ground state.\nThe standard deviation .\nWhat is the energy needed to reach the first excited state?\n\nAt ground state, \nwave function is\n\nIntroduce eigenlength , \nthe wave function is simplified:\n\nThis is a Gaussian distribution with average number 0 and standard deviation .\nHence\n\nThen\n\n\r\n11.7 Coherent States\r\n\nWe first try to find the eigenstate of lowering operator.\n\nNote that lowering operator is not Hermitian,\n must not be real.\nWith a set of orthonormal eigenstates ,\naccording to the completeness of Hilbert space, \n\nTo simplify calculation, \nwe need to reduce  to \n(becuase  is related with Hermitian polynomial, which is hard to calculate).\nTo reach this target, \nuse raising and lowering operator\n\nPlug the coefficient into the decomposition\n\nwhere  is a normalization factor.\nSince  is a constant for specific , \nit can be absorbed into the normalization factor.\n\n\nAt a later time, with time evolution oprtation, \n\nDefine , \nthen \n\n\nThis is the eigenstate of lowering operator, called **coherent state**.\nIn this state, \n\nSince  is time-dependent, \nthe initial value is  \n\nwhere  is the initial global phase.\nPlug this in, \n\nThis result indicates the average of position oscillates with time evolution, \nwhich is similar to the classical oscillator.\n\r\n\nThe standard deviation\n\n\n\nThis is a Gaussian wave packet.\n\r\n12. Free Particles\r\n12.1 Decomnposition to\r\nMomentum Space\r\n\nFree particle is defined as a particle with no potential, .\nIts Schrödinger equation is\n\nThe general solution is\n\nApply time evolution, \n\nIf  is allowed to take negative value, \nthe wave function can be combined into one term\n\nThis is the standard form of planar wave.\nBy comparing both forms, \nwe naturally introduce dispersion relation:\n\nAccording to wave mechanics, \nthe planar wave has phase velocity and group velocity:\n\nThe definition of phase velocity is the velocity of equal-phase plane.\nThe plane satisfies\n\nAnd the group velocity is the velocity of the entire wave packet\nsuperposed by many components.\n\nIf  has peak at \n(This means the most critical frequency component is ), \nexpand it with Taylor series:\n\nPlug it in, \n\nThe wave can be decomposed into two terms:\ncarrier  moves with phase velocity, \nand the integral term is only the function of .\nHence, this term moves with velocity of .\nThis velocity is the speed of envelope, defined as group speed.\n\r\nLet’s go back to the free particle wave function. \nWe try to normalize it:\n\nThe wave function cannot be normalized!\nSo, for a free particle with a definite momentum , \nit cannot be used as a wave function.\nOr in other words, \nfree particle has no definite energy or momentum, \nand it does not evolve as a planar wave.\n\r\nSince the momentum \n is continuous, \nthe general, normalizable solution should be a integral combination of states with \n\nwhere  is a normalization factor.\nThis is a Fourier inverse transformation, \nmeaning the wave function of a free particle is a superposition of components with every  values.\n is the wave function in the  space,\nobtained by Fourier transdormation:\n\nAnd this determines the compoents of different frequencies\n(imagine the frequency domain in signal processing).\n\r\nAlso, note that there is no quantum number in the wave function.\r\nEnergy of free particles is not quantized, since there is no boundary\r\nconditions (standing wave).\r\n12.2 Propagation of Free\r\nParticle\r\n\nSince the wave function is\n\nBy Fourier transformation,\n\nplug in to get the wave function\n\nwhere\n\nThe wave function is in the form of Gaussian wave packet, \nwhich can be normalized now:\n\nIf we define velocity , parameter , \nthen\n\nObviously, \nwith time evolves,\nthe uncertainty of  gradually increases,\nwhich means the position gradually spread to the entire space.\nMeanwhile, \naccording to the Heisenberg's principle of uncertainty, \nthe uncertainty of momentum must gradually decrease.\nWith time going, \nmomentum will gather to one value and you can measure momentum more precisely.\n\r\n13. δ Potential\r\n13.1 Bounded State and\r\nScattering State\r\n\r\nBounded State: a state trapped in a potential.\r\nScattering State: a state that can spread to infinite far.\r\n\r\nNote: due to the tunnelling effect, even if a state with energy\r\n is trapped in a potential well of , there\r\nis still non-zero probability that the particle appears outside the\r\npotential well. Such states are also called scattering state.\r\n13.2 δ Potential Well\r\n\nConsider a  potential well:\n\n\r\n\nThe Schrödinger equation \n\n\r\n\r\nBounded State: \r\n\r\n\nAt , the Schrödinger equation is treated as :\n\n\r\n\nConsidering the boundary condition of  as , the solution is:\n\n\r\n\nThe -function works as a boundary condition at .\n\nContinuity at  .\n\nBy normalization:\n\nGiving .\n\n\n\r\n\nThe integration around  ():\n\n\n\n\nwhere\n\n\nIgnoring first-order infinitesimal:\n\n\nPlug in the wave function\n\n\n\nFor :\n\n\nGiving:\n\n\nthen the wave function is completely determined:\n\n\r\n\r\nScattering state: \r\n\r\n\n\nThe S.E.:\n\n\nSuppose the solution is:\n\n\nAt :\n\n\nAround :\n\n\nSummarize the equations:\n\n\r\nDefine \n\n\r\nSuppose the wave is injected from left, then the 4 terms can be\r\nexplained.\r\n\r\nA: incident wave\r\nB: reflected wave\r\nC: transmitted wave\r\nD: incident wave from the right. .\r\n\r\n\r\n\r\n\r\n\r\nDelta Well and Waves\r\n\r\nSolve the equations above. \n\n\r\nThe probability density occupied is respectively: \n\n\r\nDefine reflection rate \n\n\r\nand transmission rate \n\n\r\nObviously \n\n\r\nIndicating energy conservation.\r\nBy the way, given that ,\r\n\n\n\r\nWhen  increases,  raises and\r\n decays. When , , which\r\nmarks the intuition of classical mechanics. And for reflection, we know\r\na sudden change of the medium will reflect part of the wave. In quantum,\r\nthat’s the same.\r\n13.3 δ-Barrier\r\nCompared with -well, just change  to\r\n. In this situation there’s no bounded states. For\r\nscattering states,\r\n\n\n\r\n is not 0. Meaning a particle with finite energy can pass through a infinite high potential (tunneling).\r\n14. Finite Square Potential\r\n14.1 Finite Square Barrier\r\n\r\n\r\n\r\n\r\nFinite Square Barrier\r\n\r\nThe space can be divided into 3 regions.\r\n\r\n\r\n, \r\n\r\n, \r\n\r\n,  ( term vanishes because there’s no\r\nreflection wave here)\r\n\r\nwhere , .\r\nAt :\r\n\n\n\r\nAt :\r\n\n\n\r\nIf use  as a baseline, all waves can be expressed with\r\n.\r\nReflection: \n\n\r\nOn barrier: \n\n\r\nTransmission: \n\n\r\nThe reflection and transmission rate should be\r\n\n\n\r\nWe can directly obtain\r\n\n\n\r\nwhich indicates probability conservation.\r\nIn most cases in practice, , and approximately,\r\n\n\n\r\n14.2 WKB Approximation.\r\nFor continuous potential, it can be divided into many small square\r\nbarriers with infinitesimal width.\r\n\r\n\r\n\r\n\r\nFinite Square Barrier\r\n\r\nAnd in this case, variation of V is small. The exponential term plays\r\nmuch more important role than the coefficient. Hence the coefficient is\r\nregarded as 1. Then the total transmission rate should be:\r\n\n\n\r\nThis process is called WKB approximation. An application of this is\r\n-Decay. We will calculate the lifetime of radioactive\r\nisotope .\r\n\r\ndecays with the following -decay.\r\n\n\n\r\nThe radius of  is\r\n\n\n\r\nAnd potential of surface\r\n\n\n\r\nIn -decay, Coulomb’s force plays the major role, hence\r\nat ,\r\n\n\n\r\nSince , the potential between  and\r\n is the barrier, where .\r\n\n\n\r\nThe -decay occurs only when -particle\r\ncrosses the barrier.\r\nThe transmission rate\r\n\n\n\r\nwhere\r\n\n\n\r\nSince , , approximate arcs with\r\n\r\n\n\n\r\n\n\n\r\nThis deduces\r\n\n\n\r\nAssume -particle moves with velocity of\r\n like a free particle.\r\nThe frequency to collide the wall is:\r\n\n\n\r\nOnly  of them can pass through the wall.\r\n\n\n\n is the decay constant. Plug into the decay equation\n\n\n\r\nSolving that\r\n\n\n\r\nand average lifetime\r\n\n\n\r\nPlug in data:\r\n\n\n\r\nThis is a semi-classical estimation. Compared to the experimental\r\nmeasurement , this result can be accepted.\r\n14.3 Finite Square Well.\r\n\r\n\r\n\r\n\r\nFinite Square Barrier\r\n\r\nSuppose . The same method to deal with the wave\r\nfunctions.\r\n\n\n\r\nwhere , \r\nTo simplify, we introduce a theorem.\r\nTheorem: If , then \r\ncan always be taken to be either even or odd.\r\nProof. Take spatial inversion for Schrödinger’s\r\nequation:\r\n\n\n\r\nThe operators on both sides  do not change after\r\nspatial inversion. Hence  is also a solution. We can\r\ndefine two new functions:\r\n\n\n\r\nAccording to superposition principle, they are also solutions of the\r\nSchrödinger equation, and are even and odd functions separately.\r\nMoreover, wave function satisfying  is called state with\r\neven parity;  is called odd parity.\r\nBack to the finite square well. We suppose the square well is located\r\nbetween  and . Then for even parity\r\nsolution \n\n with boundary condition at .\r\n\n\n\r\n\n\n\r\nGiving \n\n \n\n\r\nDefine \n\n gives \n\n\r\nSolve the equation with figures\r\n\r\n\r\n\r\n\r\nFinite Square Barrier\r\n\r\nNo matter how shallow the well is, there must be at least one cross\r\npoint which means the bound state. In fact, the position of\r\n determines the number of bound states.\r\n\n\n\r\nSimilarly for odd parity, \n\n\r\n\r\n\r\n\r\n\r\nFinite Square Barrier\r\n\r\nThe number is \n\n\r\nFor  \n\n\r\nwhere , \r\nApply boundary condition \n\n\r\nTransmission rate \n\n\r\nNote when , . The well can be regarded\r\nas transparent.\r\n\n\n\r\nAnd when , , then ,\r\n. The result becomes classical.\r\nReference:\r\nThe Principles of Quantum Mechanics (Paul A. M.\r\nDirac)\r\nIntroduction to Quantum Mechanics (David J. Griffiths,\r\nDarrell F. Schroeter)\r\n","categories":["physics"]},{"title":"Algebra","url":"/categories/math/algebra/","content":"1. Matrices\r\n1.1 Basic Operations\r\n\nLet  and  be positive integers. \nAn  matrix is a collection of  numbers arranged in a rectangular array:\n\nWhere  is called the element of a matrix.\n is called the index.\n\r\n\nIf  or , \na matrix reduced to a row or column vector:\n\n\nAddition of matrices is defined the same way as vector addition.\n\nNote that only matrices with the same length can de added together.\n\r\n\nScalar multiplication is defined as\n\nWhen multiplying a matrix by a vector:\n\nThis defines a linear transformation: a mapping  satisfying:  \n and  for all , .  \nThe matrix  implements , transforming  to a subspace of  (the image).\n\r\n\nMatrix multiplication represents the composition of linear transformations. Consider two matrices  and , and a vector . Then\n\nyields a vector in , demonstrating that  corresponds to applying 's transformation first (from  to ), followed by 's transformation (from  to ). \n\nThe entries of  are computed via vector inner products. For vectors , , the inner product is . Thus:\n\nEquivalently,  where  is the -th column of  and  is the -th row of . Each term  is an outer product (a rank-1 matrix).\n\nAlternatively, the full matrix product expands as:\n\nwhere each term  is an outer product (a rank-1 matrix), and the inner product defines the scalar coefficients in such decompositions.\n\r\n\nMatrix multiplication adheres to fundamental algebraic properties that follow directly from its definition. The associative property states that  for compatible matrices, as verified by the identical -entries:\n\n\nDistributive properties hold bilaterally. Left distributivity:\n\nis confirmed by -entries:\n\nRight distributivity:\n\nfollows similarly from:\n\n\nMatrix multiplication interacts consistently with scalar multiplication:\n\nas each -entry equals:\n\n\nThe identity matrix  serves as the multiplicative neutral element. For :\n\nsince:\n\nwhere  is the Kronecker delta.\n\nUnlike scalar operations, matrix multiplication is generally non-commutative:\n\nThis is evident when dimensions are incompatible (e.g., , ), and persists for square matrices:\n\n\r\n1.2 Row Reduction\r\n\nLeft multiplication  can be computed by operating the rows of . \nLet  and  denote the rows of matrices  and , \nthen in vector notation:\n\nFrom this equation, \nit's clear that if we need to adjust some rows, \nwe just adjust the coefficient.\nThe most common case is that , then , \nall elements is 0 except .\nIf we need swap two rows  and , \nwe can exchange the value of  and . \nScaling some rows is just to scale the factors.\nAnd to make summation, \nsuppose we want  to be , \nby comparing the coefficients, \nwe can get .\n\r\nNatrually, the properties of elementary matrices intrduce a\r\nlemma.\r\nLemma: Elementary matrices are invertible, and their inverses\r\nare also elementary matrices.\r\n\nWe give a story proof here: \nFrom the definitions, \nwe know an elementary matrix  is a linear operation over the rows of a matrix.\nRecall that linear operation is always invertible, \nand their inverses are also linear. \nSo, the inverse of an elementary matrix is the inverse operation of these rows.\naddition becomes subtractive, \nswap changes its direction, \nand scaling factor becomes reciprocal.\nAll of the inverse operations are linear (except 0), \nwhich is also an elementary matrix.\n\r\n\nBy applying row reduction multiple times,\nwe can solve linear equations.\nThis method is called Gaussian elimination. \nwhich is suitable for computers.\n\r\n1.3 Block Matrices\r\n\nBlocking is a trick to simplify matrix multiplication. \nLet , . \nPartition  where  is , \n is . \nPartition  where  is , \n is . The product is:\n\nprovided the inner dimensions match: \ncolumns of  = rows of  (), \ncolumns of  = rows of  ().\n\nFurther decomposition can also be done:\n\nThe multiplication is the same as multiplication of  matrices.\n\n \nBy applying row reduction repeatedly, \nwe solve linear systems via Gaussian elimination. \nThis algorithm transforms the augmented matrix  \ninto row-echelon form (REF), \nallowing systematic solution of . \nThe process comprises forward elimination and back substitution.  \n\r\n\nStarting with the augmented matrix , process columns left to right. For each pivot column : \n\n- Pivot selection: Find the entry with largest absolute value in column  at or below the current pivot row . Swap this row with row  using a row-swap elementary matrix.  \n\n- *Elimination: For each row , add a multiple of row  to row  to zero out entries below the pivot. Use row-replacement elementary matrices .  \n\n- Scaling (optional for REF): Scale row  to make the pivot 1 using .  \n\r\nThe process continues until all pivot columns are processed, yielding\r\nan upper-triangular REF matrix.\r\n\nExample: Solve:  \n  \nInitial augmented matrix:  \n  \nStep 1:  \nSwap rows 1 and 2 (partial pivoting):  \n  \nScale row 1 by :  \n  \nEliminate column 1:  \n  \n  \n  \nStep 2:  \nScale row 2 by 3:  \n  \nEliminate column 2:  \n  \n  \nStep 3:  \nScale row 3 by :  \n  \n\r\nSolve from the bottom row upward:\r\n  \nRow 3:   \r\nRow 2:   \r\nRow 1:   \n \nSolution: .  \n\r\nIf you meet non-square matrix, there must be no solution or infinite\r\nsolutions.\r\n1.4 Trace and Rank\r\nWe call the summation of diagonal elements the trace\r\nof a matrix. The most important thing of trace is related to\r\neigenvalues. We will clarify in the following sections.\r\n\nA set of vectors  is linearly independent \nif  implies . \nThe rank of  is the size of the largest linearly independent column subset. \nFor an  matrix , its columns represent  vectors in . \nThe rank is the maximum number of linearly independent columns—those that cannot be expressed as linear combinations of others. \nThis value is bounded by  due to dimensional constraints: \nat most  vectors can be linearly independent in , and the matrix has only  columns.  \n\r\n\nThis upper bound reflects the behavior of  as a linear map . \nThe rank equals the dimension of the image space (range) of . \nWhen the rank  is strictly less than , \nthe map is rank-deficient: if , \nthe kernel (null space) is nontrivial; \nif , the map fails to span . \nFor full-rank matrices (), \nsquare matrices are invertible, \nwhile rectangular matrices exhibit injectivity (full column rank) or surjectivity (full row rank).  \n\r\nRank is computed via row reduction:\r\n  \n  \nFor example:   \n  \n\r\nThe rank-nullity theorem formalizes the\r\nrelationship:\r\n  \n  \nwhere  is the kernel dimension. In solving , a rank-deficient matrix () implies either no solution (inconsistent) or infinitely many solutions (underdetermined), determined by the augmented matrix's rank.\n\r\n1.5 Determinant\r\n1.6 Eigenvalues and\r\nEigenvectors\r\n1.7 Similar Matrices\r\n1.8 Jordan Canonical Form\r\n1.9 Positive Definite Matrices\r\n2. Groups\r\n2.1 Law of Composition\r\n\nLaw of composition is a binary operation defined on a set . \nTake 2 elements in set , law of composition is a map of\n\nTo simplify, \nwe denote the operation with product.\n\nThis is not real product. \nWe can rewrite all product with addition with all properties unchanged.\n\r\n\nA law of composition is associative if\n\nA law of composition is commutative if\n\nWe have a property:\n\r\nLet an associative law of composition be given on a\r\nset, There is a unique way to define, for\r\nevery integer, a product\r\nofelementsof,\r\ndenoted by, with the following\r\nproperties:\r\n\r\nis the element itself\r\nis given by the law of\r\ncomposition\r\nFor any integerin the large,\r\n\r\n\r\nWe will prove this now. \nFor , \nit is given by the law of composition.\nSuppose we have proven this proposition for all integers .\nWe define the product .\nCheck this product for \n\n\r\n\nAn identity for a law of composition must satisfy\n\nThere can be at most one identity. \nIf there are two identities ,\n\nThat leads to a conflict.\n\r\n\nIf a law of composition is defined on set  and has an identity , \n is called the inverse of  if\n\nThe inverse of  is denoted as .\n\r\nWe have some properties for inverse. Each is useful in the following\r\nchapters.\r\nProposition: If  has a left inverse  (i.e., ) and a right inverse  (i.e., ), then ,  is invertible, and  is its inverse.  \r\nProof:\r\n\nCompute  using associativity:  \n  \nThus, . This common element satisfies  (since  and ), so  is invertible with inverse .  \n\r\nProposition: If  is invertible, its inverse is unique.\r\nProof:\r\n\nSuppose  and  are inverses of , so  and . Then:  \n  \nThus, , proving uniqueness.  \n\r\nProposition: If  and  are invertible, then  is invertible and .\r\nProof:\r\n\nCompute:  \n  \n  \nThus,  is a left and right inverse of , so .  \n\r\n2.2 Groups and Subgroups\r\nA group is a set  with a law of composition that has the following properties:\n\r\n\r\nThe law of composition is associative\r\nSet contains an identity\r\nEvery element in  has a inverse\r\n\r\nIf the law of composition is also commutative, it is called an\r\nAbelian group.\r\n\nThe order of group is the number of elements:\n\nFor example, \n is a group, \nwhich only contains one element 0.\nThen the order of this group is 1.\nIf the order is finite,  is said to be a finite group.\nOtherwise, it is a infinite group.\n\r\nWe have cancellation law:\r\nLet  be elements of group . If\r\n or , then .\r\n\nThe proof is simple:\n\nThe notation  is essential in this proof. \nIf  is not invertible, \nthen the cancellation law no longer holds.\n\r\n\nLet  be a set of maps from set  to itself. \nA function  is invertible if and only if  is bijective.\nIn the set of a function, \nwe define the law of composition to be composition\n\nA map from  to itself is in fact a permutation of the elements in the set.\nOr we say it realign the elements.\nObviously the identity is to remain all the position unchanged.\nFor example\n\nIf identity is included in , \nand all elements in  is invertible,\nthen  and composition operation forms a group. \nNote that the elements in  is permutations in , \nif we number the elements in  with indices  \nand denote the group to be , \nthen the group is called a symmetric group.\nIf a set has  elements, \nthere will be  permutations, \nhence the symmetric group on a  elements set must be  order.\n\r\n\nWe describe  next because  is the smallest group whose\nlaw of composition is not commutative.\nIn this group, \nwe can take three typical elements:\n\nseparately denoted as . \nThe following rules are easy to verify:\n\nUsing the cancellation law, \nit's easy to find\n\nThen any permutations can finally be reduced to one of the elements in .\n\r\n\nA subset  of  is a subgroup if it has the following properties:\n\r\n\r\nClosure: If  and  are in , the  is also in \r\nIdentity: 1 is in \r\nInverses: If  is in , then  is in \r\n\r\nWe don’t need to emphasize associativity because the parent set has\r\nalready gurantee it.\r\nReference:\r\nIntroduction to Linear Algebra, 5th edition, Gilbert\r\nStrang\r\nAlgebra, 2nd edition, Michael Artin\r\n","categories":["math"]},{"title":"Calculus","url":"/categories/math/calculus/","content":"1. Set Theory and Functions\r\n1.1 Generalized\r\nDistributive and De Morgan’s Laws\r\n\nConsider a family of sets , with , where  is an index set.\n\n\n\r\nDistributive Law:\r\n\n\n\n\r\nDe Morgan’s Laws:\r\n\n\n\n\r\nProof of the Distributive Law:\r\nUse (1) as example:\r\n\nLet's prove the first equation.\n\n\n\r\nProof of De Morgan’s Law:\r\nUse (3) as example:\r\n\nLet's prove the third equation.\n\n\n\r\n1.2 Density and Space on the\r\nReal Line\r\n\nConsider the set of rational numbers  on the interval .\r\nLet’s take a value .\nThe points  are in intervals \n\nwith an interval length of .\nThe total length of the union of these intervals is \n\n\r\n\n on  leaves at least a  length of empty space.\nWe can take  to be arbitrarily small.\r\nSo, the real line is filled with irrational numbers\r\neverywhere.\r\n1.3 Upper and Lower Bounds\r\nof ∅\r\n\nSince  is always false,\nthen the proposition\n\nis always true.\nThis means the upper bound of  is any real number.\nBy convention:\n\n\n\r\n1.4 Countability\r\nDefinition: A set is countable if its elements can\r\nbe arranged into a sequence according to a certain rule.\r\nEvery finite set is countable, but not every infinite set is\r\ncountable.\r\n\nConsider , then  is countable. (Diagonalization Principle)\nWe can arrange them in a matrix-like form:\n\n(Remove duplicate elements.)\n\r\n1.5 Cartesian Product\r\nDefinition:  For any element  in set  and any element  in set , a corresponding ordered pair  is formed, and the set of all such pairs is called the Cartesian\r\nproduct of  and , denoted as .\r\n\n\n\r\n1.6 Functions\r\n\nA function is defined by its rule: .\n\r\nThe conditions for a function are:\r\n\r\nDomain \r\nCodomain \r\nUniqueness\r\n\r\nInjective (One-to-one): No two values map to\r\none.\r\nSurjective (Onto): Every element is mapped.\r\nBijective (One-to-one correspondence): Both\r\ninjective and surjective.\r\nProposition for injectivity: .\r\nNote: Each element in the domain must map to a\r\nunique image, but a unique image does not require a unique\r\npre-image.\r\nBoundedness: A function is bounded if there exists  such that for all , .\n\n is an upper bound of .  is a lower bound of .\n\n is an upper bound.\n is a lower bound.\n\r\nMonotonicity: Strictly monotonic\r\n⇒ Monotonic.\r\nPeriodicity, Parity, …\r\n2. Sequence\r\n2.1 Continuity of Real Number\r\nNatural numbers are defined by the following 5 axioms (Peano\r\nAxioms):\r\n\r\n1 is a natural number.\r\nEvery natural number has a successor.\r\n1 is not the successor of any natural number.\r\nThe successor is unique.\r\n(Principle of Mathematical Induction) If  is a proposition about natural numbers such that\n\n\r\n\r\nNatural numbers (closed to plus and multiply) are extended to\r\nintegers () by subtraction, and extended to rational numbers () by division.\n\r\nDefinition: Let  be a non-empty set.\nIf  s.t.  is an upper bound of .\nIf  s.t.  is a lower bound of .\r\nUpper bound: Least upper bound sup S. Lower bound: Greatest lower bound\r\ninf S.\r\nProperties of the least upper bound (sup):\r\n(Let )\r\n\r\n is an upper bound of . .\r\nAny number smaller than  is not an upper bound. .\n\r\n\r\nTheorem: Every non-empty set of real numbers that is\r\nbounded above has a least upper bound (supremum). Every non-empty set of\r\nreal numbers that is bounded below has a greatest lower bound\r\n(infimum).\r\nProof: For all , \r\nHere,  denotes the integer part, and \r\ndenotes the fractional part.\r\nLet \r\nThen any set of real numbers S can be represented by definite\r\ninfinite decimals:\r\n\n\n\nAssume  is bounded above. Let  be the largest integer among the integer parts of the elements  in . Then define:\n\n\nLet  be the largest first decimal digit among the elements in . Then define:\n\n\n\r\n…\r\n\n\n\r\nThis implies:\r\n\n\n\r\nLet . We now prove that  is the supremum\r\nof S.\r\nStep 1: Prove  is an upper bound of .\r\nIf , then exactly one of the following two cases\r\nholds:\r\n\r\n(*) There exists  such that \r\n(**) For all , \r\n\r\nIf case (*) holds, then .\r\nIf case (**) holds, then by comparing digit by digit, we get\r\n.\r\nStep 2: Prove that for every , there exists\r\n such that .\r\nFor any , there exists  such that\r\n.\r\nTake . Then:\r\n\n\n\r\nTherefore:\r\n\n\n\r\nThat is, .\r\nTheorem: The supremum and infimum of a non-empty\r\nbounded set of real numbers are unique.\r\n2.2 Sequence Limits and\r\nInfinitesimals\r\nDefinition: Let  be a given sequence, . If ,  such that , \r\nDenoted as \r\nProperties:\r\n\r\nUniqueness: The limit of a convergent sequence must be unique\r\n\r\nProof: Take \r\n\r\n\r\nWhen , we have  and , which is a contradiction.\n\n\r\n\r\nBoundedness: A convergent sequence must be bounded\r\n\r\nProof: Assume  converges. Take ,  such that , ,\nthen\n\n\r\nTake \n\n\r\nClearly  (discussion for all terms)\r\nNote: A bounded sequence may not necessarily\r\nconverge.\r\n\r\nOrder Preservation: If ,  both converge, , , and , then , \r\n\r\nProof: Take , by \r\n\nThen \n\nSimilarly, by \n\nThen we have\n\n\r\nDef: Sequence with limit equals to 0 is called a\r\ninfinitesimal.\r\n2.3 Arthematic Operations of\r\nLimit\r\nPre-condition: . These introduce two relations:\r\n\n\n\n\nWe can also say  and  are infinitesimals, for their limit is 0.\n\r\n\r\n\r\n\r\nPf. \n\n\r\n\r\n\r\n\r\nPf. \n\n\r\n\r\nPf.\r\nTake  such that , \r\nThat is, \r\n\n\n\r\nThus , \r\nThen \r\n\r\nLet \r\n\n\n\n\n\r\nThat is, \r\n2.4 Squeeze Theorem:\r\nIf  satisfy  for , and , then \r\nProof: \n\n\r\nLet , then for , , .\nTherefore\n\nThat is, , \r\n2.5 Infinite Sequences\r\nDefinition: A sequence  is called an infinite sequence if ,  such that , \r\nIf , it is called a positive infinite sequence:\r\n\n\n\r\nIf , it is called a negative infinite sequence:\r\n\n\n\r\nNote: Infinite sequence ≠ Unbounded sequence.\r\nCounterexample: 1,1,2,1,3,1,4,1,…\r\n\r\nTheorem: If , then  is an infinite sequence if and only if  is an infinitesimal sequence.\r\nProof:\r\n\r\nIf  is infinite  is infinitesimal\r\n\r\n\n\n\r\nThat is, , hence:\r\n\n\n\r\n\r\nIf  is infinitesimal  is infinite\r\n\r\n\n\n\r\nThat is, , hence  is infinite.\r\n\r\nTheorem: If  is infinite and for ,  (uniformly positive lower bound), then  is also infinite.\r\nProof: \n\n\r\nLet \r\nThen for , we have  \r\nCorollary: If  is infinite and , then  is infinite.\r\nArithmetic Operations of Infinite Sequences - Indeterminate\r\nForms:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n\r\n\r\nDefinition: If  satisfies , , we call  monotonically increasing.\r\nIf , it is strictly increasing.\r\n\r\nStolz Theorem: Let  be a strictly increasing positive infinite sequence, and\r\n\n\n\r\nThen \r\nProof: Case 1: \r\n\n\n\r\n\n\n\r\nLet \r\nWhen :\r\n\n\n\r\n\n\n\r\n\n\n\r\n\n\n\r\nSimilarly:\r\n\n\n\r\nThus:\r\n\n\n\r\nSince  is positive infinite:\r\n\n\n\r\nLet , then :\r\n\n\n\r\nThat is, \r\n\r\nCase 2: \r\n\n\n\r\n\n\n\r\nLet , then :\r\n\n\n\r\n\n\n\r\n\n\n\r\n\n\n\r\nSince  is infinite:\r\n\n\n\r\nLet , then :\r\n\n\n\r\nCase 3:  (Proof omitted, similar to\r\nCase 2)\r\n\r\nExample 1: If , then \r\nExample 2: \n\n\r\n2.6\r\nConvergence Criteria (Real Number Continuity Theorem)\r\nMonotone Convergence Theorem: A monotone bounded\r\nsequence must converge.\r\nProof: Assume  is monotonically increasing and bounded above, hence  has a supremum.\r\nLet , we prove .\r\nBy definition of supremum:\r\n\n\n\r\n\n\n\r\nFor ,  and , hence , \r\n\r\nExample: Nested Radical Limit\r\nProve the existence of  (n nested radicals)\r\nLet , then , \r\n(i) Boundedness: Prove , \r\n,  is obvious.\r\nAssume , then , holds.\r\n(ii) Monotonicity: Prove \r\n\r\nAssume \r\n\n\n\r\nTherefore  is monotone bounded and convergent\r\n\n\n\r\n\r\nExaxmple: Fibonacci Sequence Growth Rate\r\nFibonacci sequence: ,  .\nLet , find \r\n\n\n\r\nIf , then ; if , then .\r\n\nSince\n\nThus \n\n\r\n\n\n\n\n\r\nBy Monotone Convergence Theorem,  and  exist\r\n\n\n\r\nSimilarly, \r\nTherefore \r\n\r\nThe Number e\r\nProof: is increasing,  is decreasing, and both have the same limit:\r\n\n\n\n\n\n\n\n\n\r\nTherefore both sequences are monotone bounded\r\n\n\n\r\n\r\np-Series Convergence\r\nLet , , discuss convergence of \r\nCase 1: \r\nLet , \r\n\n\n\n\n\n\n\n\n\n\n\r\nSince ,  converges\r\nCase 2: \r\n\n\n\n\n\n\n\n\n\n\n\r\n\r\nHarmonic Series and Euler’s Constant\r\nLet , prove  converges. Known: , . Taking logarithms:\r\n\n\nThis indicates\n\n\nHence  monotonicly decays.\n\n\n\r\nLet  (Euler's constant)\r\n2.7 Nested Interval Theorem\r\nDefinition: A sequence of closed intervals\r\nsatisfying two conditions:\r\n\n\n\n\n\r\nNested Interval Theorem: If  is a nested sequence of closed intervals, then there exists a unique  belonging to every , and \r\nProof: By the definition of nested intervals, \r\nThat is,  and is bounded above by ;  and is bounded below by \r\nTherefore both  and  have limits\r\n\n\n\r\nLet \r\nBy the Monotone Convergence Theorem and uniqueness of limits, \r\nHence \r\nIf there exists another  satisfying \r\nThen by the Squeeze Theorem, \r\n\r\nExample: Prove  is uncountable.\r\nMethod 1: Nested Interval Theorem\r\nProof by contradiction. Assume  is countable\r\nLet \r\nTake  such that \r\nTrisect  into three intervals:\r\n\n\n\r\n(Trisection ensures  does not belong to at least one interval)\r\nChoose one interval that doesn't contain , call it \r\nRepeat this process to obtain \r\nThis gives us  where:\r\n\r\nThis is a nested sequence of closed intervals\r\n, \r\n\r\nBy the Nested Interval Theorem,  such that , \r\nTherefore , , i.e.,  but , contradiction\r\n\r\nMethod 2: Diagonal Argument\r\nIt suffices to prove  is uncountable\r\nProof by contradiction. Assume it's countable\r\nLet the elements be:\r\n\n\n\r\nConstruct  where , \r\nThen , \r\nTherefore  is uncountable\r\n","categories":["math"]},{"title":"Thermodynamics and Statistical Mechanics","url":"/categories/physics/thermo/","content":"Part I. Macroscopic\r\nThermodynamics\r\n1.1 Equilibrium State\r\nand State Parameters\r\n1.1.1 Equilibrium State\r\n  \nThermodynamic equilibrium represents a fundamental state where a system's macroscopic properties remain constant over time without external influences. This static appearance masks intense microscopic activity - molecules continue moving and colliding, but their collective behavior produces stable macroscopic averages. A critical distinction exists between true equilibrium and steady states: while both show constant properties, steady states require continuous energy/matter exchange with surroundings, like a metal rod maintaining temperature gradient between 0°C and 100°C endpoints. True equilibrium demands three simultaneous conditions: thermal uniformity (no temperature gradients), mechanical uniformity (no pressure gradients), and chemical uniformity (no composition gradients).  \n\nState parameters quantify these equilibrium properties. Volume  represents the available space for molecular motion, defined as the container's internal volume minus the excluded volume of molecules themselves. Pressure  is the perpendicular force per unit area exerted by molecules on container walls, arising from momentum transfer during collisions. For systems without electromagnetic effects or chemical reactions,  and  alone suffice to define the state - such systems are called simple  systems. The equilibrium concept is idealized but essential, as real systems constantly evolve toward this state through particle interactions.  \n\r\n1.1.2 Temperature\r\n  \nThe zeroth law of thermodynamics provides the rigorous foundation for temperature's existence. Consider three systems A, B, and C. When A and C achieve thermal equilibrium, their state parameters satisfy:  \n  \nSimilarly, B-C equilibrium gives:  \n  \nThe zeroth law asserts that A-B equilibrium must then hold. Mathematical analysis reveals these relations factorize into a universal function:  \n  \nEquating expressions proves . This universal function  defines temperature - an intensive property identical for systems in thermal equilibrium.  \n\nTemperature scales implement this concept. Empirical scales like Fahrenheit () and Celsius originated from practical thermometry but suffer from material-dependent nonlinearity. The ideal gas temperature scale resolves this by exploiting gases at low density where intermolecular forces become negligible. For a constant-volume gas thermometer:  \n  \nwhere  is the pressure at water's triple point (273.16 K). This limiting process eliminates gas-specific behaviors, ensuring universal temperature definition. The thermodynamic (Kelvin) scale is defined fundamentally through Carnot efficiency:  \n  \nfor reversible heat engines between reservoirs at  and , matching the ideal gas scale where applicable.  \n\r\n  \nModern practical scales like ITS-90 implement thermodynamic temperatures using specified fixed points and instruments: gas thermometry below 24K, platinum resistance thermometers (13.8K-961°C), and radiation thermometry above 961°C. The Kelvin (K) and Celsius (°C) scales relate through , maintaining identical unit sizes for temperature intervals.  \n\r\n1.1.3 State Equation\r\n  \nThe state equation  establishes fundamental relationships between thermodynamic parameters at equilibrium. Three experimental gas laws provide the foundation:  \n\r\n\r\nBoyle’s law ( at constant ) establishes isothermal compressibility   \n\r\nGay-Lussac’s law ( at constant ) defines volume expansion coefficient \n\r\n\r\nCharles’s law ( at constant ) gives pressure coefficient   \n\nThe ideal gas equation synthesizes these observations with Avogadro's principle. Starting from Boyle's law , consider a constant-pressure process with triple-point reference:  \n  \nCombining with Boyle's law at triple point  yields:  \n  \nAvogadro's law confirms that  is identical for all gases at given moles , defining universal constant :  \n  \nwhere  is molar volume at triple point.  \n\r\n\r\n  \nMicroscopically, Boltzmann's constant  connects macroscopic and molecular descriptions:  \n  \nFor gas mixtures, Dalton's law () with partial pressures  combines to:  \n  \nproving the mixture state equation.  \n\r\n  \nReal gases deviate from ideality at high densities and low temperatures. The van der Waals equation incorporates molecular volume () and attraction ():  \n  \nCritical parameters derive from mathematical conditions at the critical point:  \n  \nSolving these simultaneously yields:  \n  \nThe reduced compressibility factor  provides a universal benchmark. For broader accuracy, the Onnes virial expansion:  \n  \nsystematically corrects deviations through temperature-dependent virial coefficients, measurable via gas compression isotherms.  \n\r\n1.2 Microscopic Model of\r\nMatter\r\n1.2.1 Introduction\r\n  \nThe foundation of molecular kinetic theory lies in understanding matter at its most fundamental level. All macroscopic substances - whether solids, liquids, or gases - consist of vast numbers of microscopic particles called atoms or molecules, separated by empty space. This atomic structure explains why materials can be compressed: the apparent solidity of matter is an illusion created by electromagnetic forces between particles, not actual contact between them. Modern scientific instruments like scanning tunneling microscopes have made this invisible world visible, allowing us to image individual atoms and even manipulate them into structures like letters or patterns.  \n\nThe ceaseless, chaotic motion of these particles forms the heart of thermal phenomena. This motion intensifies with temperature, as dramatically demonstrated by Brownian motion - the random dance of pollen grains or smoke particles suspended in fluid. When observed under a microscope, these particles jitter unpredictably due to unbalanced collisions with surrounding fluid molecules. The smaller the particle, the more violent its motion becomes, providing direct evidence that what we perceive as \"still\" liquid or gas is actually a frenzy of molecular activity. This perpetual motion isn't confined to fluids; even in solids, atoms vibrate around fixed positions like springs connecting a molecular scaffold.  \n\nInteractions between molecules govern material states through competing forces. At extremely close range (&lt;0.1 nm), strong repulsive forces dominate as electron clouds overlap, preventing matter from collapsing into infinite density. At intermediate distances (0.1-1 nm), attractive forces take over through electromagnetic interactions between temporary dipoles in otherwise neutral molecules - the van der Waals forces that give liquids cohesion. These opposing forces create an equilibrium distance where molecules naturally settle, like dancers maintaining personal space in a crowded room. The delicate balance between molecular motion and these interaction forces explains phase transitions: heating provides kinetic energy to overcome attraction, turning solids to liquids to gases.  \n\r\n1.2.2 Pressure of Ideal Gases\r\n  \nTo understand gas pressure at the molecular level, we construct a simplified model that captures essential behaviors while ignoring complex details. Imagine gas molecules as infinitesimal points rather than physical objects - a reasonable approximation since molecular diameters (~10⁻¹⁰ m) are dwarfed by typical intermolecular separations (~10⁻⁹ m at STP). Between collisions, these particles move freely without mutual attraction or repulsion, like commuters ignoring each other in a vast train station. Collisions between molecules or with container walls occur instantaneously and elastically, conserving both momentum and kinetic energy like perfect billiard ball impacts.  \n\nThis idealization emerges naturally from experimental observations: gases expand to fill containers because molecules move independently; low densities make compression easy by reducing intermolecular distances; constant pressure at equilibrium implies steady collision rates. The model's power lies in transforming the chaotic complexity of 10²³ molecules into tractable statistics, where individual paths become irrelevant and collective averages dominate.  \n\r\n1.2.3 Statistical\r\nAssumptions at Equilibrium\r\n  \nWhen gas reaches thermodynamic equilibrium, two powerful statistical principles emerge despite ongoing molecular chaos. First, molecules distribute uniformly in space - any macroscopic volume element contains approximately equal particle numbers regardless of location. This spatial homogeneity allows defining number density  as a constant throughout the container. Second, molecular velocities show no directional preference; all three Cartesian components share identical statistical properties:  \n  \nThe first set indicates no net flow, while the second reveals equal partitioning of kinetic energy across dimensions. These symmetries transform the intractable problem of tracking individual molecules into manageable statistical mechanics.  \n\r\n1.2.4 Ideal Gas Pressure\r\nFormula\r\n  \nPressure emerges from the relentless barrage of molecular impacts on container walls. Consider molecules approaching a wall area  perpendicular to the x-axis. Each collision reverses the x-component of momentum, delivering impulse  to the wall. To find total force, we calculate how many molecules strike  in time . Molecules with velocity  within distance  can reach the wall, forming an imaginary collision cylinder of volume . With number density  for velocity group , the collision count is .  \n\nSumming impulses from all velocity groups:  \n  \nUsing velocity isotropy  and defining translational kinetic energy :  \n  \nThis elegant derivation reveals pressure as a statistical manifestation of molecular kinetic energy.  \n\n\n\n\nDiagram of pressure to the vessel wall\n\r\n1.2.5 Microscopic\r\nInterpretation of Temperature\r\n  \nConnecting microscopic motion to temperature starts with combining the pressure equation  with the macroscopic ideal gas law . Equating these yields the profound relationship:  \n  \nThis simple formula carries deep implications. Temperature directly measures the average kinetic energy of molecular translation - a universal currency where all gas molecules hold equal value regardless of mass. At room temperature (300 K), every molecule - from lightweight hydrogen to heavy xenon - carries approximately  J of translational energy. This energy equipartition explains why lighter molecules move faster to achieve the same energy:  \n  \nFor example, hydrogen molecules ( kg) zip at about 1920 m/s at 300 K, while oxygen molecules ( kg) move at a more sedate 480 m/s.  \n\nThe collective nature of temperature becomes apparent when considering gas mixtures. Dalton's law  emerges naturally since all components share the same  at equilibrium. This energy equality persists even between dissimilar molecules because collisions redistribute kinetic energy until balance is achieved - similar to billiard balls of different masses reaching the same average energy after numerous impacts.  \n\r\n1.2.6 Molecular Forces\r\n  \nBeneath the apparent simplicity of gases lies a complex interplay of electromagnetic forces. Each molecule contains positively charged nuclei surrounded by negatively charged electrons. When molecules approach, their electron clouds distort, creating temporary dipoles that generate attractive forces - the van der Waals interaction. At closer ranges, Pauli repulsion dominates as overlapping electron orbitals resist compression.  \n\nThese competing effects produce the characteristic molecular force curve: strongly repulsive below equilibrium distance , attractive above , and negligible beyond several nanometers. Physicists model this behavior using potential energy functions:  \r\n\r\nLennard-Jones potential: \n   \n  balances short-range repulsion (r⁻¹² term) with longer-range attraction (r⁻⁶ term). The minimum at  represents optimal bonding distance.  \r\nSutherland potential: \n  \n  models impenetrable hard spheres with weak attraction.  \r\nHard-sphere potential: \n   \n  ignores attraction entirely, focusing on excluded volume effects.  \r\n \nThese models serve different purposes: Lennard-Jones accurately describes noble gases, Sutherland simplifies van der Waals theory, while hard-sphere models help understand dense fluids.  \n\r\n\r\n1.2.7 Pressure in van der Waals\r\nGases\r\n  \nReal gases deviate from ideal behavior through two molecular effects: finite size and mutual attraction. Johannes van der Waals ingeniously modified the ideal gas law to account for both. First, molecules occupy physical space, reducing the available volume for motion. For  molecules, the excluded volume isn't simply  times molecular volume because exclusion involves pairwise interactions. Statistical analysis shows:  \n  \nwhere  is the effective molecular diameter. This correction transforms volume in the ideal gas law to .  \n\r\n \nSecond, attractive forces reduce pressure. Surface molecules near container walls experience net inward pulls from bulk molecules, weakening their collisions. This \"internal pressure\"  scales with the product of densities of attracting and attracted molecules:  \n  \nUsing Sutherland's potential, constant  can be derived from potential depth  and range :  \n  \nCombining both corrections yields the van der Waals equation:  \n  \nThis elegantly explains real gas behavior like liquefaction and critical phenomena.  \n\r\n1.3 Distribution of\r\nMolecular Speeds and Energy\r\n1.3.1 Maxwell’s Speed\r\nDistribution Law\r\n\nThe molecular speed distribution in an ideal gas at thermal equilibrium is derived from fundamental statistical principles. Consider the velocity distribution function , defined such that the probability of a molecule having velocity components between  and ,  and , and  and  is . For an isotropic system, this function depends only on the speed , and the velocity components are statistically independent. These assumptions lead to the functional form . Taking the logarithm and applying separation of variables gives , resulting in a Gaussian distribution:  \n  \nThe normalization condition  determines  via Gaussian integrals. The equipartition theorem  fixes , yielding the Maxwell velocity distribution:  \n  \nTo obtain the speed distribution , integrate over all velocity directions (spherical coordinates):  \n  \nThus,  describes the probability density for molecular speeds.\n\r\n1.3.2\r\nCharacteristic Speeds and Distribution Properties\r\n\nThe most probable speed  occurs at the maximum of . Solving  gives:  \n  \nThe average speed  is computed by integrating  weighted by :  \n  \nUsing the substitution  and gamma functions, this evaluates to:  \n  \nThe root-mean-square speed  derives from the mean-square speed:  \n  \nThus . Temperature dependence arises because , causing distribution broadening. Mass dependence  implies heavier molecules exhibit narrower distributions.\n\r\n1.3.3 Boltzmann\r\nDistribution in Force Fields\r\n\nIn conservative force fields, the Maxwell distribution generalizes to the Boltzmann distribution. For a potential energy , the phase-space distribution is:  \n  \nwhere  and  is the density at . Integrating over velocities yields the spatial density:  \n  \nFor gravity , this becomes:  \n  \nThe scale height  characterizes the exponential decay (e.g., ~8 km for Earth's atmosphere). The Maxwell-Boltzmann distribution combines both aspects:  \n  \n\r\n1.3.4\r\nEquipartition Theorem and Energy Distribution\r\n\nThe equipartition theorem states: each quadratic term in a system's Hamiltonian contributes  to the average energy. For a coordinate  with energy , the Boltzmann distribution gives:  \n  \nusing Gaussian integrals. Molecular degrees of freedom include:\n\r\n\r\nTranslational: 3 terms  → \r\nRotational: 2 (diatomic) or 3 (polyatomic) terms →  or \r\nVibrational: Kinetic and potential terms each contribute  per mode\n\nFor a diatomic molecule, total energy is  (rigid) or  (non-rigid). Molar heat capacity at constant volume is:  \n  \nwhere  is the number of quadratic degrees of freedom.\n\r\n\r\n1.4 Mean Free Path of Gas\r\nMolecules\r\n1.4.1 Mean\r\nCollision Frequency and Mean Free Path\r\n  \nGas molecules move at high thermal speeds (e.g., nitrogen at 27°C averages 476 m/s), yet macroscopic diffusion occurs slowly due to frequent collisions that randomize molecular paths. The mean free path  quantifies the average distance a molecule travels between collisions, while the collision frequency  counts collisions per unit time. To model collisions, molecules are treated as rigid spheres with effective diameter , ignoring long-range attraction but accounting for short-range repulsion that prevents overlap.  \n\r\n \nFor identical molecules, the collision cross-section is . When a \"test\" molecule moves at average relative speed  through a gas of number density , it sweeps a collision cylinder of volume  in time , shown in the figure below:\n\n  \n  \n  \nCollision Cylinder of a molecule  \n\nThen the collision frequency is:  \n  \nwhere  is the mean thermal speed. The mean free path follows as:  \n  \nUsing the ideal gas law , this becomes:  \n  \nThus,  inversely scales with pressure (e.g., air at STP:  m; at  Pa:  m).  \n\r\n1.4.2 Distribution of Free\r\nPaths\r\n  \nThe probability that a molecule travels distance  without collision follows exponential decay due to random collisions. \nSuppose at time , \nthere are  molecules at position  that have not undergone any collisions. \nAfter a time interval , \nthe molecular beam moves to position , \nand  molecules collide and are removed.\nThat is, the number of molecules whose free path lies between  and  is .\n\r\n  \nWithin the distance interval  to , the number of molecules undergoing collisions, , is proportional to the number of molecules at , , and proportional to the size of . The proportionality constant is , thus:\n\nThis gives  \n  \nwhere  is the number surviving to distance . The probability density for a free path between  and  is:  \n  \nThe mean  is consistent with . This distribution underpins phenomena like electron beam attenuation in vacuum tubes.  \n\r\n1.4.3 Viscosity\r\n  \nWhen adjacent fluid layers move at different velocities (e.g.,  in a -dependent flow), viscosity arises from momentum transfer perpendicular to the flow direction. \n  \n  \n  \nVelocity gradient in layered flow generating viscous forces  \n\nNewton's law of viscosity states:  \n  \nwhere  is the dynamic viscosity, and  is the velocity gradient. The force  slows faster layers and accelerates slower ones, equalizing momentum. Viscosity units are Pa·s (poise: 1 P = 0.1 Pa·s).  \n\nHere is a derivation:\n\r\n \nExcess momentum\n\n\nThe number of molecules with velocity  arriving at  from region A during time  is\n\nThe momentum flow produced by these molecules is\n\nThe momentum flow produced by all molecules from A to B is\n\n\nExcess momentum\n\n\nThe number of molecules with velocity  arriving at  from region B during time  is\n\nThe momentum flow produced by these molecules is\n\nThe contribution of all molecules from B to A to the momentum flow is\n\n\nThe contribution to the momentum flow through  by all molecules is\n\n\n\n\n\n\n  \n  \n  \nMomentum in Fluid  \n\r\n1.4.4 Heat Conduction\r\n  \nHeat conduction occurs when temperature gradients exist. Fourier's law describes the heat flux  through area  in time :\n\nwhere  is the thermal conductivity (W·m⁻¹·K⁻¹). Heat flows down the temperature gradient, transferring energy from hot to cold regions. In gases, conduction dominates when bulk motion (convection) is negligible.\n\nFor heat conduction in gases to occur without a pressure difference (), the number density  is determined by:\n\nConsider regions A (hotter, at ) and B (cooler, at ) with temperatures  and  respectively:\n\nUsing the mean speed :\n\nFor small temperature differences, the flux is approximately constant:\n\n\nMicroscopically, energy transfer occurs via molecular collisions: molecules from hotter regions carry higher kinetic energy, exchanging it upon collision with cooler-region molecules. The average thermal energy per molecule is:\n\r\n\r\nIn region A:    (for translational energy; for molecules with  degrees of freedom, it's , )\r\n\r\nIn region B:   , \n\nThe number of molecular pairs exchanged across an area  in time  is estimated as:\n\n(The factor  arises from considering the fraction of molecules moving perpendicular to the surface in a specific direction within an isotropic gas).\n\nThe net energy transported along the positive z-axis per exchanged molecular pair is the difference:\n\n\nThe total energy transported through  along the positive z-axis in time  (i.e., the heat ) is:\n\n\nThe temperature difference  is related to the temperature gradient at :\n\nSubstituting this in:\n\n\nComparing this to Fourier's law , the thermal conductivity  is identified as:\n\n\nUsing the molar heat capacity at constant volume  (where  is Avogadro's number), and the specific heat capacity  (per unit mass), and the density , this can be rewritten as:\n \n  \n  \n  \nDemonstration of Heat Conductance  \n\r\n\r\n1.4.5 Diffusion\r\n  \nDiffusion transports mass due to density gradients. Fick's law for the mass  crossing area  in time  is:\n\nwhere  is the diffusion coefficient (m²·s⁻¹), and  is mass density. In gases, self-diffusion (identical molecules) and mutual diffusion (different species) arise from net molecular flux from high-to-low-density regions.\n\nConsider a pressure difference where . Since pressure relates to density via  or equivalently  at constant temperature:\n\n\nThe net number of molecules transported along the positive -axis through  in time  is:\n\n(The factor  accounts for the fraction of molecules moving perpendicular to the surface).\n\nThe corresponding net mass transported is:\n\n\nRelating the density difference to the gradient at :\n\nSubstituting:\n\n\nComparing to Fick's law , the diffusion coefficient  is:\n\n\nFor mixtures of different gases (mutual diffusion), the diffusion coefficient  depends on the interacting species. An example is shown below:\n\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nGas\r\n\r\n (cm²·s⁻¹)\r\n\r\n\r\n\r\n\r\nGas 1: Hydrogen ()\r\n3\r\n0.594\r\n\r\n\r\nGas 2: Carbon Dioxide ()\r\n1\r\n0.605\r\n\r\n\r\n\r\n\n\n  \n  \n  \nDemonstration of Diffusion  \n\n\r\n1.5 The First Law of\r\nThermodynamics\r\n1.5.1 Thermodynamic Processes\r\n\nA thermodynamic process, or simply process, occurs when the state of a system changes over time. For example, when advancing a piston compresses gas in a cylinder, the gas's volume, density, temperature, or pressure will change, and at any moment during the process, the density, pressure, and temperature are not identical throughout the gas. Thermodynamic processes are classified into non-quasistatic processes and quasistatic processes. \n\r\n\nIn non-quasistatic processes, the system transitions from an equilibrium state to a disrupted state before reaching a new equilibrium. The time from equilibrium disruption to new equilibrium establishment is called relaxation time (). Actual processes often proceed rapidly, with the system undergoing further changes before reaching new equilibrium, meaning the system passes through non-equilibrium states that cannot be described by state parameters. The free expansion of ideal gas is a typical non-quasistatic process where \"free\" means the gas expands without resistance. \n\r\n\nIn quasistatic processes, every intermediate state is infinitesimally close to an equilibrium state. This is only possible when the process proceeds \"infinitely slowly.\" For actual processes, quasistatic approximation requires the characteristic time of state change to be much greater than the relaxation time . Equilibrium states in quasistatic processes have definite state parameter values, represented by points on a P-V diagram for simple systems. The quasistatic change process is represented by a curve on the P-V diagram, called the process curve. Though an ideal limit, quasistatic processes are fundamental to thermodynamics and practically significant. Unless otherwise specified, thermodynamic processes refer to quasistatic processes. For instance, gradually heating a system from  to  through small temperature increments is quasistatic, while directly contacting a system at  with a heat source at  is not.\n\r\n1.5.2 Work\r\n\nWork is a method of energy exchange. \nIn thermodynamics, it represents the conversion between ordered mechanical energy from the surroundings and disordered thermal energy of the system, denoted as .\n Work can change the system's state, affecting not only mechanical motion but also thermal motion and electromagnetic states, such as friction heating (mechanical work) or electrical heating (electrical work). \n\nFor gases undergoing quasistatic processes, work done by the system is given by \n\nand \n\nwhere  indicates work done on the system. Graphically, work magnitude equals the area under the process curve on a  diagram. Work is not a state characteristic but a process characteristic—it depends on the path taken between states. For different processes from equilibrium state 1 to 2, work done by the surroundings varies. Work is a process quantity, path-dependent.\n\r\n1.5.3 Heat\r\n\nHeat is another method to change system state, distinct from work. While work involves energy transfer through generalized forces causing generalized displacements, heat transfer occurs due to temperature differences. Joule's experiments demonstrated that heat production or disappearance always accompanies equivalent disappearance or production of other energy forms (mechanical, electrical), proving no separately conserved \"caloric\" exists. Instead, heat, mechanical energy, and electrical energy together conserve energy. Heat () is transferred energy: positive when absorbed by the system, negative when released. Both heat and work quantify energy changes, are process-dependent, but differ in origin: work stems from mechanical interactions, heat from thermal interactions.\n\r\n1.5.4 The First Law of\r\nThermodynamics\r\n\nJoule's experiments revealed a definite equivalence between heat and work (1 cal = 4.186 J), showing interconversion between mechanical/electromagnetic and thermal motion. This led to the energy conservation and transformation law: all matter possesses energy in various forms convertible between each other and transferable between objects, with total quantity conserved. An alternative statement: perpetual motion machines of the first kind are impossible. \n\nFor a system changing from initial state 1 to final state 2 via an adiabatic process (no heat exchange), the work done by surroundings is the adiabatic work. Joule's experiments showed that for fixed initial (state 1, temperature ) and final states (state 2, temperature ), the adiabatic work is path-independent. Thus, internal energy  is defined as a state function satisfying  for any adiabatic process between states 1 and 2. \n\nFor non-adiabatic processes with work  done on the system and heat  absorbed, energy conservation gives:\n\nThis is the mathematical expression of the first law, where  for work done on the system,  for heat absorbed. For infinitesimal processes:\n\nHere,  is exact (state function), while  and  are inexact differentials (process quantities). The first law applies to both quasistatic and non-quasistatic processes, though initial/final states must be equilibrium for calculations. If only volume work exists:\n\n\r\n1.5.5 Heat Capacity and\r\nEnthalpy\r\n\nHeat capacity  is defined as \n \nwhen a system's temperature rises by  with absorbed heat . It depends on process, substance, and mass. \n\nAt constant volume (), , so , giving constant-volume heat capacity:\n\nwhere  and  are functions of  and . \n\nAt constant pressure, work , so absorbed heat is:\n\nDefining enthalpy  (state function), . For infinitesimal processes:\n\nThus, constant-pressure heat capacity is:\n\nwhere  and  are functions of  and . For ideal gases, .\n\r\n1.5.6\r\nInternal Energy of Gases and Joule-Thomson Experiment\r\n\nIn Joule's 1845 free expansion experiment, gas expanded into vacuum with no temperature change observed. Applying the first law (, ) gives , indicating constant internal energy during adiabatic free expansion. Thus, for ideal gases:\n\nHowever, Joule's experiment was imprecise due to water's large heat capacity masking gas temperature changes. The Joule-Thomson experiment (1852) improved accuracy by studying adiabatic throttling. \n  \n  \n  \nJoule Thomson Experiment  \nIn adiabatic throttling, high-pressure gas passes through a porous plug to low pressure with no heat exchange (). \n\nThe process equation gives constant enthalpy. The Joule-Thomson coefficient:\n\ndetermines temperature change:  for cooling,  for heating. Real gases have , proving internal energy depends on both temperature and volume, with molecular forces present. This effect enables practical gas liquefaction (e.g., Linde process).\n\r\n\nFor ideal gases obeying  and :  \nThe internal energy  depends solely on temperature due to negligible intermolecular forces. By the equipartition theorem, each degree of freedom contributes  per mole. For  degrees of freedom:  \n  \nThe constant-volume heat capacity follows by differentiation:  \n  \nEnthalpy is defined as . Substituting the ideal gas law :  \n  \nDifferentiating with respect to  at constant pressure:  \n  \nThus the molar heat capacity at constant pressure is:  \n  \nThe heat capacity ratio  is:  \n  \nThis confirms  and  for all .  \n\r\nProcess Applications:\r\n\r\nIsochoric ( constant): , , , .\r\nIsobaric ( constant): \n\n\n\n\r\nIsothermal ( constant): , , .\r\nAdiabatic (): , leading to , , . Work .\n\nFor a polytropic process , the molar heat capacity  is derived from the first law and process equation. For 1 mole of ideal gas:\n\nThe molar heat capacity is defined as , so:\n\nFrom the polytropic equation  and ideal gas law , solve for :\n\nDifferentiate with respect to :\n\nRearrange to express :\n\nSubstitute:\n\nThus:\n\nUsing  and , express  as:\n\nSubstitute into (2):\n\nSolving for  in terms of heat capacities:\n\n\n\n\n\r\n\r\nThen special cases can be verified:\r\n\r\nIsobaric (): \r\nIsochoric ():  (limit of )\r\nIsothermal ():  (undefined, consistent with )\r\nAdiabatic ():  (since )\n\r\n\r\n1.5.7 Cyclic Processes and\r\nCarnot Cycle\r\n\nCyclic processes involve a working substance returning to its initial state after completing a series of thermodynamic changes. Quasistatic cycles are represented as closed curves on P-V diagrams, with clockwise cycles functioning as heat engines and counter-clockwise cycles as refrigeration systems. In heat engines, the working substance absorbs heat  from a high-temperature reservoir at , performs net work  on the surroundings, and releases heat  to a low-temperature reservoir at . The first law gives:\n\nwith thermal efficiency defined as:\n\n  \n  \n  \nP-V Diagram of Carnot Figure \n\n\r\nThe Carnot cycle—a fundamental model for heat engines—combines two\r\nisothermal and two adiabatic processes. For an ideal gas working\r\nsubstance:\r\n\r\nIsothermal expansion\r\n(A→B):\n\n\r\nAdiabatic expansion\r\n(B→C):\n\n\r\nIsothermal compression\r\n(C→D):\n\n\r\nAdiabatic compression\r\n(D→A):\n\n\nThe adiabatic equations yield the volume ratio relationship:\n\nSubstituting into the efficiency formula:\n\nThis result depends solely on reservoir temperatures and is independent of the working substance.\n\r\n\r\n\nFor refrigeration cycles (counter-clockwise on P-V diagram), the working substance absorbs heat  from a cold reservoir () using work input , releasing heat  to a hot reservoir (). The coefficient of performance is:\n\nFor Carnot refrigerators, the maximum possible performance is:\n\n\r\nPractical engine implementations also include:\r\n\r\nOtto cycle (constant-volume heating) with efficiency:\n\nwhere  is the compression ratio.\r\nDiesel cycle (constant-pressure heating) with\r\nhigher efficiency due to greater compression ratios.\r\n\r\n1.6 The Second Law of\r\nThermodynamics\r\n1.6.1 The Second Law of\r\nThermodynamics\r\n\nThe Second Law of Thermodynamics addresses the directionality of natural processes, complementing the First Law's energy conservation principle. While the First Law prohibits perpetual motion machines of the first kind (violating energy conservation), it does not restrict process directionality. For example, heat spontaneously flows from high to low temperatures but not the reverse. The Second Law resolves this through two equivalent formulations:  \r\n\r\nKelvin Statement (1851): It is impossible to convert heat entirely from a single heat source into work without other effects. This implies heat engines cannot achieve 100% efficiency () and prohibits perpetual motion machines of the second kind.  \r\nClausius Statement (1850): Heat cannot spontaneously flow from a cold to a hot body without external work input. This establishes the directional nature of heat transfer and limits refrigeration efficiency ().\n\r\n\r\n\nThermodynamic processes exhibit irreversibility due to inherent disequilibrium. A process is reversible only if both the system and surroundings return to their initial states without net changes. Kelvin's statement reveals the irreversibility of work-to-heat conversion, while Clausius's statement shows irreversibility in heat conduction. These statements are logically equivalent: violation of one implies violation of the other. Crucially, all irreversible processes are interconnected—demonstrating that irreversibility in one process (e.g., free expansion) implies irreversibility in others (e.g., heat transfer). The core principle is that all macroscopic thermal processes are irreversible; reversible processes are idealizations requiring quasistatic, dissipation-free conditions.\n\r\nWe now prove the equivalence between Kelvin and Clausius\r\nStatements:\r\n(1) Clausius false ⇒ Kelvin false\r\n\nAssume device  violates Clausius: transfers heat  from cold reservoir  to hot reservoir  without work input. Combine with reversible heat engine  operating between  and :\n\n\n\nNet effect: Complete conversion of heat  from  to work , violating Kelvin.\n\r\n(2) Kelvin false ⇒ Clausius false\r\n\nAssume device  violates Kelvin: converts heat  from  entirely to work . Drive refrigerator  with this work:\n\n\n\nNet effect: Heat  spontaneously flows from  to , violating Clausius.\n\r\n\nIrreversibility arises statistically from molecular disorder. Consider gas molecules in a partitioned container: after partition removal, molecules spread uniformly. The uniform distribution has the highest thermodynamic probability (), while ordered states (e.g., all molecules on one side) have negligible  for large . The Boltzmann postulate states that isolated systems in equilibrium have equal microscopic state probabilities. Natural processes evolve toward macroscopic states with higher , increasing disorder. The Second Law's statistical interpretation: isolated systems evolve from low- to high- states, reflecting increased randomness. This applies only to macroscopic systems, not microscopic phenomena like Brownian motion, and is confined to finite isolated systems, not the universe.\n\r\n1.6.2 Carnot Theorem\r\n  \nCarnot Theorem establishes the theoretical limits for heat engine efficiency operating between thermal reservoirs at temperatures  (high) and  (low). The theorem states: all reversible engines operating between these reservoirs achieve identical efficiency  \n  \nwhile irreversible engines satisfy  \n  \nThe proof for reversible engine equality proceeds by contradiction. Consider two reversible engines  and  operating between reservoirs at  and . Assuming , set  to operate reversibly as a refrigerator with adjusted cycles satisfying  and . The inequality  implies  \n  \nwhere  and . The composite system extracts net work  while exchanging zero net heat with both reservoirs. This violates Kelvin's statement by converting heat entirely into work without other effects, forcing  \n  \n\nFor irreversible engines, let  be irreversible and  reversible between  and . Assuming  and running  as a refrigerator with matched , the inequality  implies  \n  \nwhere  and . The composite system extracts net work  with zero net heat exchange at  and net heat loss  at . If , this violates Kelvin's statement by producing positive work from non-positive heat extraction, proving  \n  \nwith equality only for reversible engines.  \n\r\n1.6.3 Thermodynamic Temperature\r\nScale\r\n  \nThe thermodynamic temperature scale, established by Lord Kelvin using Carnot's Theorem, provides a universal temperature definition independent of material properties. For a reversible heat engine operating between reservoirs at empirical temperatures  and , the heat ratio satisfies  \n  \nwhere  is a universal function. Introducing a third reservoir at  with an auxiliary reversible engine yields  \n  \nCombining these ratios gives  \n  \nwhere the universal function  depends only on temperature. Defining thermodynamic temperature  establishes  \n  \nFixing the triple point of water as  aligns this scale with the ideal gas thermometer where applicable, confirming  \n  \nThis derivation provides the theoretical foundation for the absolute temperature scale used in the Carnot efficiency formula  \n  \n\r\n1.6.4 Entropy\r\n  \nThe Second Law of Thermodynamics addresses the directional nature of natural processes, complementing the First Law's energy conservation principle. Kelvin's statement (1851) declares it impossible to convert heat entirely from a single source into work without other effects, implying heat engines cannot achieve 100% efficiency () and prohibiting perpetual motion machines of the second kind. Clausius' statement (1850) establishes that heat cannot spontaneously flow from cold to hot objects without work input, defining the directional nature of heat transfer and limiting refrigeration efficiency (). Thermodynamic processes exhibit inherent irreversibility due to disequilibrium, where reversible processes require both system and surroundings to return exactly to initial states—an idealization demanding quasistatic, dissipation-free conditions. Kelvin's statement reveals work-to-heat conversion irreversibility, while Clausius' statement demonstrates heat conduction irreversibility, with both formulations being logically equivalent: violation of one implies violation of the other. For example, if heat conduction were reversible, one could construct a system transferring heat from cold to hot reservoirs without work, violating Clausius' statement. Similarly, assuming free expansion reversibility leads to violation of Kelvin's statement, confirming all macroscopic thermal processes are fundamentally irreversible.  \n\r\n  \nIrreversibility emerges statistically from molecular disorder. Consider  gas molecules in a partitioned container: after partition removal, molecules spread uniformly with maximum thermodynamic probability , while ordered states (e.g., all molecules on one side) become negligible for . Boltzmann's postulate states that isolated equilibrium systems have equal microscopic state probabilities, explaining why natural processes evolve toward higher  states, increasing disorder. This statistical interpretation—isolated systems evolve from low  to high  states—applies exclusively to macroscopic systems and finite isolated environments. Carnot Theorem establishes efficiency limits between reservoirs at : all reversible engines achieve , while irreversible engines satisfy . The proof for reversible equality involves showing that two engines with different efficiencies would produce net work from a single reservoir when coupled, violating Kelvin's statement.  \n\r\n  \nKelvin established a universal temperature scale using Carnot's Theorem. For a reversible engine between reservoirs:  \n  \nafter defining thermodynamic temperature  through heat ratios and fixing water's triple point at . Entropy  is defined via reversible cycles:  \n  \nCombining with the First Law yields:  \n  \nFor ideal gases:  \n  \nThe Clausius inequality  implies , leading to the entropy increase principle for isolated systems:  \n  \nwhere free expansion gives  and heat conduction yields . Boltzmann's microscopic interpretation:  \n  \nquantifies disorder through microstate count . For open systems:  \n  \nwith Maxwell's demon paradox resolved by Landauer's principle: erasing 1 bit dissipates at least  heat.  \n\r\nPart II. Classical\r\nStatistical Mechanics\r\nNote:\r\n\r\nBefore you read this part, please make sure you have learned\r\nClassical Mechanics, Quantum Mechanics and\r\nProbability.\r\nIn Part II and Part III, Boltzmann constant are all normalized to\r\n  . So temperature    and energy   \r\nhave the same unit.\r\n\r\n2.1 Description of States\r\nStatistical physics connects the behavior of individual particles to\r\nthe measurable properties of materials we observe in everyday life. To\r\nunderstand how this connection works, we first need a way to describe\r\nthe detailed state of a many-particle system.\r\nConsider an isolated system composed of  particles,\r\nlike molecules in a sealed container. Each particle requires six numbers\r\nto fully specify its mechanical state: three position coordinates\r\n and three momentum components . For the\r\nentire system, we must account for all particles simultaneously.\r\nThe complete microscopic state - called a microstate - is therefore\r\ndescribed by listing all positions and momenta together: \n\n\r\nThis collection of  numbers forms a mathematical point in\r\na -dimensional abstract space known as phase\r\nspace.\r\nClassical mechanics demonstrates that if we know this phase space\r\npoint at any instant, and the system’s Hamiltonian\r\n(generally considered as energy), we can in principle\r\ndetermine its future evolution through Hamilton’s equations:\r\n. Thus, a single point in phase space completely defines\r\nthe instantaneous mechanical state of the entire system.\r\nHowever, for macroscopic systems where , tracking\r\nindividual phase space points becomes fundamentally impossible.\r\nMoreover, laboratory measurements of quantities like pressure or\r\ntemperature inherently average over enormous numbers of microstates -\r\napproximately  microstates contribute to a single second\r\nof macroscopic observation.\r\nThis practical limitation necessitates a shift from deterministic\r\nmechanics to probabilistic description. Rather than following exact\r\ntrajectories, we must consider how microstates are distributed\r\nthroughout phase space, leading us to the core methodology of\r\nstatistical physics.\r\nBased on this description, we must specify the characterization of\r\nthe macro-system. For an isolated system (no exchange of particles and\r\nenergy with the environment), its macro-state can be completely\r\ndetermined by 3 measureable conservation quantities:\r\n\r\nNumber of particles \r\nVolume of the system \r\nEnergy of the system \r\n\r\nWhy these three? Macroscopic measurements cannot distinguish\r\nmicrostates sharing same . Hence, parameters\r\n determines a macro-state of a system.\r\nSuch a macro-state contains a massive number of micro-states. It\r\ncorresponds to a set in the phase space:\r\n\n\n\r\nThe condition  defines a -dimensional\r\nhypersurface (zero volume), which causes difficulty for calculation of\r\nprobability. To solve this problem, we need to introduce energy\r\nshell.\r\nBased on the hypersurface, we extend another dimension to give it a\r\nsmall “thickness” . Then, the hypersurface turns into a\r\nthin space, represented by:\r\n\n\n\r\nThis extended space is called a energy shell. Note\r\nthat  must be further smaller than , but\r\nit’s necessary to include sufficient microstates ().\r\nWith a dimension of , we can calculate the probability\r\nin the phase space.\r\nWe can figure out the “volume” of the energy shell:\r\n\n\n\r\nwhere \n\n\r\nThe volume describes the number of micro-states in\r\n\r\n2.2 Boltzmann Postulate\r\nInitially, researchers tried to establish connections between macro\r\nand micro states by ergocity assumption. They think\r\nisolated systems traverse all reachable states on an energy hypersurface\r\nfor a sufficiently long time. However, time scale required for strict\r\ntraversal is far more larger than the age of the universe, so it’s\r\nimpossible in measuring.\r\nTo solve this problem, Boltzmann raised the most postulate in\r\nstatistical mechanics:\r\nBoltzmann Postulate: For an isolated system under\r\nequilibrium, all micro-states have equal probability.\r\nEquilibrium means all observable properties (e.g., temperature,\r\npressure, density) are time-invariant and uniform in space. Based on\r\nthis postulate, we can figure out the probability of a microstate in an\r\nenergy shell: \n\n\r\nwhere  is the microstate.\r\nAlmost everything in statistical mechanics is established on the\r\nBoltzmann’s postulate.\r\n2.3 Entropy\r\n2.3.1 Gibbs-Shannon Entropy\r\nSince macroscopic measurement cannot distinguish microstates, while\r\none macrostates include massive microstates, we need a quantity to\r\nquantify the scale of indistinguishability.\r\nImagine you are predicting the weather tomorrow: If it is known that\r\ntomorrow will definitely be sunny (), there is no doubt\r\nabout the outcome of the prediction and the uncertainty is zero. When\r\nyou learn that it will be sunny, the amount of information you get is\r\nalso almost zero (you already knew). If the probability of it being\r\nsunny or rainy tomorrow is 50/50 (), the prediction is\r\nmost uncertain. You get the most information (eliminating the most\r\nuncertainty) when you learn that it will be sunny (or rainy). If the\r\nprobability of tomorrow being sunny is 0.9 and rainy is 0.1, the\r\nuncertainty is somewhere in between. Learning that it will be sunny\r\nbrings less information (expected) and learning that it will be rainy\r\nbrings more information (unexpected).\r\nFrom the example above, it’s clear that information carried by the\r\nevents is strongly related to the probability distribution. Generally,\r\nwe can define a function  to describe the amount of\r\ninformation. This function should satisfy the conditions below:\r\n\r\n\r\n\r\nis continuous\r\nIf an event occurs with probability ,\r\n\r\nFor independent events , ,\r\n\r\n\r\nWe now derive the explicit form of :\r\nIn terms of probabilities, letting  and\r\n, we have the functional equation: \r\nDefine  for . From the conditions,\r\n is continuous, , , and it\r\nsatisfies: \n\n\r\nWe assume that  is differentiable on .\r\nThis assumption is justified because the continuity of \r\nand the functional equation imply that  is differentiable\r\n(as is standard in solving Cauchy functional equations).\r\nFix an arbitrary . Consider the functional equation as\r\na function of : \n\n Differentiate both sides\r\nwith respect to  (treating  as\r\nconstant):\r\n\r\nLeft side:  by the chain rule.\r\nRight side: , since  is constant\r\nwith respect to .\r\n\r\nThus: \n\n Solving for :\r\n\n\n\r\nEquate the expressions for : \n\n\r\nRearranging: \n\n\r\nEquation (*) holds for all . Since the left side\r\ndepends only on  and the right side only on\r\n, both sides must equal a constant, say .\r\nTherefore: \n\n where  is a constant. Solving\r\nfor : \n\n\r\nIntegrate both sides with respect to : \n\n\r\nwhere  is the constant of integration.\r\nApply the condition : \n\n Since\r\n, we have . Thus: \n\n\r\nNow apply the condition  for all . For\r\n, , so to ensure , we must\r\nhave . Let  where . Then:\r\n\n\n This is equivalent to: \n\n since any\r\nlogarithm base can be absorbed into the constant  (as\r\n, so  scales accordingly).\r\nNow we have proved . Returning to the context of\r\nmicrostates, we can define Gibbs-Shannon entropy:\r\n\n\n\r\nwhere  phase space probability density.\r\nThe Gibbs-Shannon entropy reflects the uncertainty and disorder of a\r\nthermodynamic system.\r\n2.3.2 Boltzmann Entropy\r\nAfter Gibbs-Shannon entropy, we can define another type of entropy\r\ncalled Boltzmann entropy.\r\n\n\n\r\nwhere  is the volume of the phase space. We can find\r\nthat when , . So Boltzmann entropy\r\nis a special case of Gibbs-Shannon entropy at equilibrium. By\r\nthe way, Boltzmann entropy is only defined at equlibrium, while\r\nGibbs-Shannon entropy can also be defined at non-equilibrium.\r\n2.3.3 Maximal Entropy Principle\r\nBoltzmann’s postulate of equal probability leads to the principle of\r\nmaximum entropy: The Gibbs-Shannon entropy of an isolated\r\nthermodynamic system is maximized at equilibrium. The\r\nprocess of converging to equilibrium is a process of increasing\r\nentropy.\r\nThere is a proof: \n\n\r\nThe equals sign is taken when . This principle means\r\nequilibrium states gives maximum entropy. By the thermodynamics 2nd law,\r\nany isolated systems tend to evolve towards equilibrium state.\r\n2.4 Ensembles\r\nAn ensemble in statistical mechanics is a probability distribution\r\nover the system’s phase space, representing our knowledge (or ignorance)\r\nof the system’s exact microstate.\r\n2.4.1 Micro-canonical Ensemble\r\nThe microcanonical ensemble is the starting point of equilibrium\r\nstatistical mechanics. It describes an isolated system —one that can\r\nexchange neither energy, volume, nor particles with its surroundings\r\n—and assigns probabilities to its microscopic states.\r\nIn classical mechanics, the state of a system of \r\nparticles, confined in a box with volume  , is specified\r\nby a point , the -dimensional phase space\r\nof positions and momenta. But thermodynamics makes no reference to\r\nmicrostates — it deals only with macroscopic variables such as energy\r\n, volume  , and particle number\r\n. The goal of statistical mechanics is to explain\r\nthermodynamics as a consequence of statistical properties of microscopic\r\ndegrees of freedom.\r\nRecall the energy shell , instead of the\r\ncharacteristic function , we can also represent it with\r\ndelta function:\r\n\n\n\r\nwhere  is referred to as the “surface area” of the\r\nconstant energy surface.\r\nIn the microcanonical ensemble, the fundamental thermodynamic\r\nquantity is the Boltzmann entropy (at equilibrium), which is defined\r\nas\r\n\n\n\r\nThe factor on the denominator is to elliminate quantum effects. We\r\ncan rewrite the entropy:\r\n\n\n\r\nIt is obvious that  and  are both\r\nextensive, whereas  is subextensive. Hence in the\r\nthermodynamic limit, we may ignore  altogether, and\r\nhave\r\n\n\n\r\nIn thermodynamics, all state variables can be derived from the\r\nfundamental relation . It is useful to think of this\r\nfunction as a surface in a four dimensional space spanned by\r\n. This surface was called the fundamental surface by\r\nCallen. The microcanonical ensemble realizes this idea concretely by\r\nconnecting entropy to the volume of phase space. Once we have the\r\nentropy as a function of energy, volume, and particle number, we can\r\ndefine all other thermodynamic quantities as its partial\r\nderivatives.\r\nTemperature  is defined as: \n\n\r\nTemperature measures how rapidly the number of accessible microstates\r\nincreases with energy. For the temperature to be well-defined and\r\npositive, the function  must grow monotonically with\r\n. Physically, this is almost always true for large\r\nsystems — higher energy means more ways to distribute that energy among\r\nmicroscopic degrees of freedom.\r\nPressure  is defined as: \n\n Pressure\r\narises from the fact that increasing the volume allows more microstates\r\nto be accessible —especially if particles are free to move. Thus, the\r\nentropy increases with volume, and this increase (scaled by temperature)\r\ndefines the pressure exerted by the system. However, negative pressure\r\nis not a signature of thermodynamic instability.\r\nIf the number of particles  is also treated as a\r\nthermodynamic variable, chemical potential  is defined\r\nas: \n\n This expresses how the entropy changes when a\r\nparticle is added to the system, at fixed energy and volume. It is\r\nespecially important when studying systems that can exchange particles\r\n(grand canonical ensemble), but the definition remains valid even in the\r\nmicrocanonical ensemble as a formal identity.\r\nTo recap, we can reassign these definitions: \n\n reassign\r\nagain: \n\n\r\nThis is in fact the 1st thermodynamics law.\r\nAnd it’s easy to extend all the things in this section above to\r\nmulti-component systems. Just extend  to\r\n\n\n\r\nLet’s take the ideal gas in micro-canonical ensemble for example:\r\nConsider a classical ideal gas of  indistinguishable\r\nparticles of mass , confined to volume .\r\nThe Hamiltonian is purely kinetic:\r\n  \n  \n\r\nThe surface area  is defined as:\r\n  \n  \n\r\nIntegrate over coordinates (yields ):\r\n  \n  \n\r\nSet . Using the surface area of a\r\n-dimensional sphere:\r\n  \n  \n\r\nFor :\r\n  \n  \n\r\n  \n  \n\r\nApply Stirling’s approximation for large :\r\n  \n  \n\r\nRetain extensive terms:\r\n  \n  \n\r\nThis is the Sackur-Tetrode entropy.\r\nScale variables by :\r\n  \n  \n\r\n\r\nTemperature:\r\n  \n    \n  \r\n\r\nPressure:\r\n  \n    \n  \r\n\r\nChemical potential:\r\n  \n    \n  \r\n\r\n2.4.2 Canonical Ensemble\r\nDifferent from micro-canonical ensemble, the canonical ensemble\r\ndescribes systems in thermal equilibrium with a heat bath at fixed\r\ntemperature. It allows variation of , which in\r\nmicro-canonical fixed.\r\nConsider a large, isolated system with total energy \r\n(microcanonical ensemble). If we divide the entire system into 2 parts:\r\nsubsystem and reservoir, the energy will be divided into 3 parts:\r\n\r\nSystem: Small subsystem with Hamiltonian\r\n\r\nHeat bath: Large reservoir with Hamiltonian\r\n\r\nInteraction: Interaction between the system and\r\nthe reservior \r\n\r\nHowever, in large systems, interaction is extremely weak compared to\r\ninteral actions. So interaction Hamiltonian is neglected.\r\nThe system and reservoir exchange energy through a diathermal wall,\r\nwith fixed  and  for both.\r\n\n\n\n\nSystem of Canonical Ensemble\n\r\nBy Boltzmann’s postulate, every microstate of the combined system has\r\nequal probability. The total number of microstates is:\r\n  \n  \n\r\nWe split this integral by inserting :\r\n  \n  \n\r\nNow consider microstate  in the subsystem. Given\r\n,  is fixed, but  is still\r\nvarying. There’re many ’s which has energy of\r\n. Hence, the probability density for the system to be in\r\nmicrostate  is proportional to the number of bath states\r\navailable:\r\n  \n  \n\r\nwhere  is the Boltzmann entropy of the bath.\r\nSince the bath is much larger than the system (), we\r\nexpand:\r\n  \n  \n\r\nDefine the inverse temperature:\r\n  \n  \n\r\nThus:\r\n  \n  \n\r\nThe proportionality constant defines the partition\r\nfunction:\r\n  \n  \n\r\ngiving the canonical distribution:\r\n  \n  \n\r\nThe probability density for the system to have energy \r\nis:\r\n  \n  \n\r\nwhere  is the system’s density of states.\r\nThe partition function, , is an extremely important\r\nfunction in statistical mechanics. (Partition function of\r\nmicro-canonical ensemble is ) It serves as the generating\r\nfunction for all thermodynamic properties of the system.\r\nLet’s define free energy first:   \n  \n\r\nis called the Helmholtz Free Energy. Through\r\nHelmholtz free energy and partition function, we can calculate almost\r\nall thermodynamics quantities.\r\n\r\nAverage Energy： \n\n\r\nGibbs-Shannon Entropy: \n\n\r\n\r\nThis result can derive the relationship between Helmoholtz free\r\nenergy with energy:   \n  \n\r\nTake derivative and plug result of , we have\r\n  \n  \n\r\n\r\nEnergy Variance: \n\n\r\n\r\nWe also have\r\n\n\n\r\nHence,\r\n\n\n\r\nThis shows equivalence with the microcanonical ensemble in the\r\nthermodynamic limit.\r\n\r\nThe bath’s constant temperature  emerges from its\r\nlarge size\r\n\r\nRare high-energy states are exponentially suppressed by\r\n\r\n\r\nFree energy  encodes the competition between energy\r\nand entropy\r\n\r\n2.4.3 Grand Canonical Ensemble\r\nThe grand canonical ensemble is almost the same as canonical ensemble\r\nexcept that  is also exchangeable between the subsystem\r\nand reservoir.\r\n\n\n\n\nSystem of Grand Canonical Ensemble\n\r\nImagine a large, isolated system characterized by its total energy\r\n, total particle number , and total volume\r\n. This overarching system can be described by its\r\nmicrocanonical partition function, , where\r\n represents the total Hamiltonian and \r\nsignifies integration over all possible microstates.\r\nNow, consider dividing this isolated system into a small subsystem\r\n(denoted by the subscript ‘s’) and a vast reservoir (subscript ‘r’). The\r\ntotal particle number is conserved, meaning . The\r\nmicrocanonical partition function can then be expressed as a sum over\r\npossible particle numbers in the subsystem () and an\r\nintegral over possible energies in the subsystem ():\r\n\n\n\r\nThe probability density of finding the subsystem with energy\r\n and particle number  is given by:\r\n\n\n\r\nHere,  is the entropy of the reservoir. For a\r\nsufficiently large reservoir, we can perform a Taylor expansion of its\r\nentropy around the total energy and particle number of the isolated\r\nsystem:\r\n\n\n\r\nIn this expansion,  is the inverse temperature (where\r\n is temperature), and  is the chemical\r\npotential. Substituting this back into the probability density equation,\r\nwe arrive at a crucial expression for the probability density in the\r\ngrand canonical ensemble:\r\n\n\n\r\nThe denominator, , is known as the grand\r\ncanonical partition function:\r\n\n\n\r\nThis partition function effectively normalizes the probability. The\r\nprobability of a specific microstate () of the subsystem\r\nwith  particles is then:\r\n\n\n\r\nThe normalization condition, , allows us to express\r\nthe grand canonical partition function in terms of the Hamiltonian\r\n for a system with  particles:\r\n\n\n\r\nA remarkable aspect of the grand canonical ensemble is its direct\r\nrelationship to the canonical ensemble. The canonical partition\r\nfunction, , describes a system with a fixed number of\r\nparticles , fixed volume , and fixed\r\ntemperature . The grand canonical partition function can\r\nbe seen as a sum over all possible canonical ensembles, weighted by the\r\nfactor :\r\n\n\n\r\nThis equation elegantly demonstrates that the grand canonical\r\nensemble is, in essence, a statistical amalgamation of many canonical\r\nensembles, each corresponding to a different number of particles. This\r\nformulation is particularly powerful for systems where particle number\r\nis not a conserved quantity.\r\nJust as the Helmholtz free energy is derived from the canonical\r\npartition function, the grand potential,\r\n, is defined from the grand canonical partition\r\nfunction:\r\n\n\n\r\nThe grand potential provides a direct link to the system’s\r\nthermodynamic properties. From the Gibbs-Shannon entropy,\r\n, we can derive a fundamental relationship:\r\n\n\n\r\nRearranging this equation, we find an important expression for the\r\ngrand potential in terms of other thermodynamic quantities:\r\n\n\n\r\nHere,  is the Helmholtz free energy ().\r\nDifferentiating the grand potential yields several crucial thermodynamic\r\nrelations:\r\n\n\n\r\nFrom this differential, we can derive the following thermodynamic\r\nrelations:\r\n\r\nAverage Particle Number: \r\nPressure: \r\nAverage Energy: \r\n\r\nFrom the first law of thermodynamics, , and the\r\nextensivity of energy (), we know that .\r\nCombining this with the definition of the grand potential,\r\n, leads to a remarkable identity:\r\n\n\n\r\nThis equation establishes a direct connection between the grand\r\npotential and the pressure-volume product of the system. Furthermore,\r\nthe Gibbs free energy, , can be shown to be equal to\r\n:\r\n\n\n\r\nThis implies that the chemical potential  represents\r\nthe Gibbs free energy per particle, . Differentiating the\r\nGibbs free energy leads to the Gibbs-Duhem\r\nrelation:\r\n\n\n\r\nThis relation highlights the interdependence of intensive variables\r\n(temperature, pressure, and chemical potential). On a per-particle\r\nbasis, it can be written as:\r\n\n\n\r\nwhere  is the entropy per particle and \r\nis the volume per particle.\r\nA key aspect of the grand canonical ensemble is its ability to\r\ndescribe fluctuations in energy and particle number, which are\r\ninherently allowed due to the system’s coupling with the reservoir. The\r\nmean square fluctuations are defined as:\r\n\n\n\r\n\n\n\r\nThese fluctuations are directly related to derivatives of the grand\r\ncanonical partition function:\r\n\r\nParticle Number Fluctuations: \r\nEnergy and Particle Number Cross-Fluctuations: \r\n\r\nFor macroscopic systems, these fluctuations scale as \r\nand are therefore negligible compared to the average values, which scale\r\nas . This means that for large systems, the grand\r\ncanonical ensemble’s predictions for average quantities will converge\r\nwith those from other ensembles. However, the grand canonical ensemble\r\nremains invaluable for understanding and calculating properties of open\r\nsystems where particle exchange is crucial.\r\n2.4.4 Recap\r\nIn the thermodynamic limit, where the particle number \r\nand volume  approach infinity while the density\r\n remains constant, the microcanonical (NVE), canonical\r\n(NVT), and grand canonical (µVT) ensembles yield identical thermodynamic\r\npredictions. This equivalence stems from the suppression of relative\r\nfluctuations in macroscopic variables. Within the canonical ensemble,\r\nenergy fluctuations diminish as the system size increases, with relative\r\nenergy fluctuations scaling as . As ,\r\nthese fluctuations vanish, rendering the sharply peaked energy\r\ndistribution indistinguishable from a fixed-energy microcanonical\r\ndescription.\r\nSimilarly, in the grand canonical ensemble, both particle number and\r\nenergy fluctuations become negligible. Particle number fluctuations obey\r\n, which vanishes in the thermodynamic limit, effectively\r\nfixing the particle count. Concurrently, energy fluctuations relative to\r\nthe mean energy disappear, aligning the ensemble with the canonical and\r\nmicrocanonical frameworks. The thermodynamic potentials of each\r\nensemble—entropy , Helmholtz free energy ,\r\nand grand potential —are rigorously linked through\r\nLegendre transforms. These transforms become exact in the thermodynamic\r\nlimit, ensuring identical predictions for intensive properties like\r\npressure or energy density across all ensembles.\r\nThus, the vanishing relative fluctuations and mathematical\r\nconsistency of thermodynamic potentials establish the equivalence of the\r\nthree ensembles for macroscopic systems.\r\n2.5 Extremum Principles\r\n2.5.1 Thermodynamic Potentials\r\nWe have known the energy , Helmholtz Free Energy\r\n and grand potential .\r\n\n\n\r\nOperate a Legendre transformation,\r\n\n\n\r\nThis is defined as the Gibbs Free Energy.\r\nTake a diffrential, we can get \n\n\r\nWe can also define Enthalpy: \n\n\r\nWhat we must clarify is that differential variables in the\r\ndifferential equation is the natural variable of that\r\npotential.\r\nTake energy  for example: we have \n\n\r\nand we can expand this example to . For example we can\r\nsay \n\n\r\n2.5.2 Maxwell Relation\r\nThe thermodynamic potentials are well-defined single-valued functions\r\nof their natural variables. In mixed second order derivatives such as\r\n, the order of partial derivative can be exchanged:\r\n\n\n\r\nsince , , so we obtain:\r\n\n\n\r\nThis leads to a very large number of identities between partial\r\nderivatives of various thermodynamic variables, all called Maxwell\r\nrelations: Since these differentials are exact, mixed partials yield\r\nMaxwell relations. We list all 15 Maxwell relations below:\r\n\r\n\r\n\r\nPotential\r\nRelations\r\n\r\n\r\n\r\n\r\n\r\n, ,\r\n\r\n\r\n\r\n\r\n, ,\r\n\r\n\r\n\r\n\r\n, ,\r\n\r\n\r\n\r\n\r\n, ,\r\n\r\n\r\n\r\n\r\n, ,\r\n\r\n\r\n\r\n\r\n2.5.3 Extremum Principles\r\nAs we have already shown in Lect. 2, the equilibrium state of an\r\nisolated system maximizes the Gibbs-Shannon entropy, subject to the\r\nconstraints of probability normalization and fixed energy (which means\r\nthat the prob- ability density function vanishes outside the energy\r\nsurface). The entropy maximizing probability distribution is an equal\r\nprobability distribution on the energy surface. Hence the\r\nmaximal entropy principle is ultimately equivalent to\r\nBoltzmann’s postulate of equal probability. Either can be used as a\r\nstarting point of equilibrium statistical mechanics.\r\nConsider a system with continuous microstates  in\r\nthermal contact with a heat bath at temperature , with\r\nfixed volume  and particle number . Let\r\n be the energy of microstate  and\r\n the probability density of an arbitrary\r\nnon-equilibrium state. The average energy is: \n\n\r\nand the Gibbs-Shannon entropy is: \n\n Define the\r\nnon-equilibrium Helmholtz free energy as: \n\n where\r\n. For fixed , we seek the \r\nextremizing , subject to the constraint of probability\r\nnormalization: \n\n Introducing a Lagrange multiplier\r\n, the relevant functional that needs to be extremized is:\r\n\n\n The first variation, , gives:\r\n\n\n which yields a canonical distribution: \n\n\r\nThe second variation, , confirms that this is a minimum.\r\nThis  matches the canonical distribution, derived via\r\nBoltzmann’s postulate. Hence we obtain the following extremum\r\nprinciple for the equilibrium state of a thermally open system that can\r\nexchange energy with an equilibrium heat bath:\r\nAt equilibrium, the non-equilibrium Helmholtz free energy\r\n of a thermally open system is minimized.\r\nNote that this principle is valid for both small systems and large\r\nsystems. It is straightforward to verify that this principle can also be\r\nreformulated in the following equivalent form:\r\nAt equilibrium, the Gibbs-Shannon entropy of a thermally open system\r\nis maximized subject to normalization and fixed average energy.\r\nNow consider a system exchanging energy and particles with a bath at\r\ntemperature  and chemical potential , with\r\nfixed volume . Let  be the discrete\r\nparticle number, and  a continuous microstate for given\r\n, with energy . For a non-equilibrium\r\nstate, the probability density  yields averages:\r\n\n\n and Gibbs-Shannon entropy: \n\n The\r\nnon-equilibrium grand potential is defined as: \n\n\r\nWe seek  extremizing , subject to\r\nnormalization: \n\n Using a Lagrange multiplier\r\n, the functional is: \n\n Setting the\r\nfunctional derivative to zero: \n\n we find the grand\r\ncanonical distribution: \n\n where  is the\r\nchemical potential, and  is the grand-canonical partition\r\nfunction. We can also show that the second variation is positive:\r\n, confirming that this is a minimum of the grand\r\npotential. This establishes the following extremum principle for an open\r\nequilibrium system that exchanges energy and particles with its\r\nenvironment:\r\nAt equilibrium, the non-equilibrium grand potential \r\nof an open system is minimized.\r\nAn equivalent representation of this extremum principle is the\r\nfollowing:\r\nAt the thermodynamic equilibrium, the Gibbs-Shannon entropy of an\r\nopen system is maximized subject to the constraints of probability\r\nnormalization and fixed average energy, and fixed average particle\r\nnumber.\r\n2.6 Equilibrium Conditions\r\n2.6.1 Thermal Equilibrium\r\nConsider an isolated system composed of two subsystems that can\r\nexchange energy (This is in fact the canonical ensemble). The number of\r\naccessible microstates for a given division of energy is:\r\n\n\n\r\nand the corresponding probability is \n\n\r\nAccording to the maximal entropy principle, we need to maximize\r\n. To find the value of  maximizing\r\n (denoted as ), we should take the\r\nderivative:\r\n\n\n\r\nThis gives \n\n\r\nExpanding  about , with\r\n\r\n\n\n\r\nyields a Gaussian distribution \n\n and the fluctuation\r\n\n\n\r\nIf , then the entropy change associated with a small\r\nenergy transfer  from system 1 to system 2 is:\r\n\n\n\r\nThis indicates that entropy increases when energy flows from the\r\nhotter to the colder subsystem.\r\n2.6.2 Mechanical Equilibrium\r\nNow consider two subsystems that can exchange volume through a\r\nmovable, frictionless piston, while the total energy and total volume\r\nremain fixed. Let subsystem 1 occupy volume , and\r\nsubsystem 2 occupy . The total entropy is:\r\n\n\n\r\nThe most probable configuration maximizes the total entropy with\r\nrespect to variations in  \n\n\r\nsince , we have \n\n\r\nAssuming thermal equilibrium has already been established, this\r\nsimplifies to \n\n\r\nThis is the mechanical equilibrium.\r\nThe same as temperature, we can derive such a relationship:\r\n\n\n\r\nThis aligns with the everyday notion of pressure as a force that\r\ndrives expansion. In equilibrium, the driving force vanishes; outside\r\nequilibrium, it gives rise to directional motion that increases entropy.\r\nHence, the common physical interpretation of pressure as an expansive\r\nforce is fully consistent with its thermodynamic and statistical\r\ndefinitions\r\n2.6.3 Chemical Equilibrium\r\nThe same, we can also derive chemical equilibrium. Consider a system\r\nwith multiple components, we have \n\n\r\nUsing the identity: \n\n\r\nWe have \n\n\r\nHence, particles flow spontaneously from higher to lower chemical\r\npotential, just as heat flows from hot to cold and volume expands from\r\nhigh to low pressure. The chemical potential can thus be interpreted as\r\na generalized ”force” driving particle exchange toward equilibrium.\r\n2.7 Stability Condition\r\n2.7.1 Entropy Stability\r\nConsider two systems with same macrostates (i.e., same\r\n). According to the scaling property, we have\r\n\n\n\r\nNow apply small perturbations to the system: \n\n\r\nOur target is to find the stability condition. Recall that stability\r\ncorresponds to second order derivative. So we expand \n\n\r\nThe differential of the entropy for a single subsystem is:\r\n\n\n\r\nWe can take differential of these equations. Finallt get\r\n\n\n\r\nSince entropy is maximized, if we want the system to be stable,\r\n. Hence \n\n\r\nThen \n\n\r\n2.7.2 Diagonalization of\r\n\r\nWe can express the second variation of entropy as a quadratic\r\nform:\r\n  \n  \n\r\nwhere , , etc.\r\nFrom the thermodynamic identity:\r\n  \n  \n\r\nwe can solve for :\r\n  \n  \n\r\nSubstituting into  eliminates cross-terms with\r\n:\r\n  \n  \n\r\nTo simplify, define new coefficients:\r\n  \n  \n\r\nand introduce a new variable :\r\n  \n  \n\r\nSuppose , then \ndS = \n\r\nWe can derive a Jacobian determinant \n\n Change variables\r\nto \n, , \n\r\n\n\n\r\nSince \n\n\r\nthen we can find that \n\n\r\nSubstituding the results into ,   \n  \n\r\nThis requires two independent stability conditions:\r\n  \n  \n\r\n2.7.3 Chemical Stability\r\nWe consider . Then second derivative reduces to\r\n  \n  \n\r\nThis leads to another stability condition:   \n  \n\r\n  \nFor an extensive system, the Helmholtz free energy is:  \n\r\n  \n  \n\r\n  \nChemical potential:  \n\r\n  \n  \n\r\n  \nDifferentiate  w.r.t :  \n\r\n  \n  \n\r\n  \nPressure relation:  \n\r\n  \n  \n\r\nSubstitute:\r\n  \n  \n\r\nSo chemical stability leads to  .\r\n2.8 First Order Phase\r\nTransition\r\n2.8.1 Conflict at Transition\r\nPoint\r\nFirst-order phase transitions, such as boiling or melting, are marked\r\nby discontinuities in thermodynamic observables—like energy or volume—as\r\nexternal parameters like temperature or pressure are varied. These\r\ndiscontinuities signal the coexistence of macroscopically distinct\r\nphases and arise from a subtle breakdown of stability in thermodynamic\r\npotentials. Understanding them requires a precise analysis of entropy\r\ncurvature (the second order derivative), ensemble equivalence, and the\r\ngeometry of thermodynamic functions.\r\nFirst consider micro-canonical ensemble. We assume that the volume is\r\nfixed, so we do not have to worry about  and\r\n. Temperature comes:   \n \n\r\nWe can find the heat capacity:   \n \n\r\nWe use superscript MC to indicate that it is calculated using\r\nmicro-canonical ensemble.\r\nNormally  is concave, so , leading to a\r\npositive capacity. But for some systems,  can be convex,\r\nmaking . You can see it clearly in the following\r\nfigure:\r\n\n\n\n\nEntropy Curve at Phase Transition Point\n\r\nNow we study the same system using the canonical ensemble. Because of\r\nthe equivalence of different ensembles under thermodynamics limit,\r\ncanonical ensemble should give the same result as that of\r\nmicro-canonical ensemble.\r\nWe have partition function:   \n \n\r\nand Helmholtz free energy:   \n \n\r\nThe average energy is   \n \n\r\nWe denote the capacity in canonical ensemble as . It\r\ncan be calculated by   \n \n\r\nFluctuation is always positive, so  must also be\r\npositive at any time. However, this will lead to a conflict when entropy\r\nis convex. Such a weird conflict must contain some unusual physical\r\nphenomenon, this is the first order phase transition.\r\n2.8.2 Phase\r\nTransition under Micro-canonical Ensemble\r\nTo resolve the conflict between ensembles, we employ a geometric\r\napproach that reveals the physical mechanism of phase separation.\r\nLet’s go back to the curve of entropy. \n\n\n\nEntropy Curve at Phase Transition Point\n\r\nFor any energy  between  and\r\n, we can achieve higher entropy by forming a mixture of\r\ntwo phases:\r\n\r\nFraction  in phase  (energy\r\n, entropy )\r\nFraction  in phase  (energy\r\n, entropy )\r\n\r\nThe mixture entropy exceeds the homogeneous entropy:\r\n\n\n\r\nThe optimal phases (, ) are determined by the\r\ndouble tangent condition:\r\n\r\nThe straight line connecting  and \r\nmust be tangent to  at both points\r\nThis requires equal slopes at  and\r\n:\r\n\r\n\n\n\r\nThe slope equality implies: \n\n This guarantees\r\nthermal equilibrium between the two phases. Temperature\r\nat this point is denoted as , called critical\r\ntemperature. Furthermore, we can show chemical\r\nequilibrium: \n\n\r\nThe equilibrium entropy follows the concave hull: \n\n\r\nwhere the mixture entropy is the linear interpolation:\r\n\n\n\r\nBetween  and , . So we\r\ncan find the capacity divergent: \n\n\r\nThe apparent contradiction between the microcanonical heat capacity\r\n and canonical heat capacity  is resolved\r\nthrough a fundamental singularity. This divergence eliminates the\r\ndistinction between positive and negative values - the pole singularity\r\nrenders the sign ambiguity physically meaningless. The “negative”\r\n in the microcanonical ensemble and the strictly positive\r\n requirement in the canonical ensemble converge to\r\nidentical divergent behavior at the phase transition.\r\nIt agrees with our life experiment. We know that melting and boiling\r\nwill absorb extra heat while temperature remains unchanged. According to\r\nthe definition of capacity, it is bound to be infinite. The heat change\r\nduring the process is called latent heat. We will\r\nclarify more details in the following sections.\r\nWe must pay attention to the fact that the convex curve is a result\r\nof state equation, and the double tangent line is how entropy evolves in\r\nreality. The entropy curve’s convex region arises directly from the\r\nsystem’s fundamental equation of state—this curved path represents all\r\npossible homogeneous configurations governed by microscopic\r\ninteractions. However, this convex segment remains thermodynamically\r\nforbidden in physical systems due to its instability. The double tangent\r\nline reveals what actually occurs: systems spontaneously undergo phase\r\nseparation to follow this straight-line path in the entropy-energy\r\nplane.\r\nThis straight trajectory connects coexisting phases at energies\r\n and , bypassing the unstable curved\r\nsegment entirely. The system “short-circuits” the equation of state’s\r\nprediction through phase separation, with the slope \r\ndefining the constant transition temperature.\r\nThis geometric duality—curved state equation versus straight\r\nphase-transition path—explains why boiling water at 100°C absorbs heat\r\nwithout temperature change: the system evolves along the double tangent\r\nline, not the unstable curved entropy path predicted for homogeneous\r\nstates.\r\n2.8.3 Phase\r\nTransition under Canonical Ensemble\r\nSince the conflict has been solved, we will derive the whole system\r\nwith canonical ensemble.\r\nConsider a simple fluid ( system) with particle number\r\n and volume  in contact with a heat bath\r\nat temperature . According to the minimum free energy\r\nprinciple, the Helmholtz free energy is minimized at equilibrium.\r\nSuppose we compute the Helmholtz free energy per particle\r\n assuming homogeneity. The differential form is:\r\n  \n  \n\r\nPressure is obtained via derivative:   \n  \n\r\nThe mechanical stability condition requires:   \n  \n\r\nWhen this condition is violated in interval , phase\r\nseparation occurs.\r\n\n\n\n\nHelmholtz Free Energy at Phase Transition Point\n\r\nUsing the same method, we find the two points of phase transition via\r\n. Transition points satisfy:   \n  \n\r\nwhere  is coexistence pressure. The double tangent is:\r\n  \n  \n\r\nFor , the system minimizes free energy by separating into\r\nphases:   \n  \n\r\nEquilibrium free energy is the convex hull:   \n  \n\r\nWe can still derive equlibrium condirtions:\r\n\r\nThermal equilibrium: Same\r\n\r\nMechanical equilibrium: \r\nChemical equilibrium: \r\n\r\n2.8.4 Maxwell Construction\r\nThe Helmholtz free energy curve exhibits a concave\r\nregion within the phase transition interval. This concavity\r\nviolates the mechanical stability condition requiring ,\r\nindicating that homogeneous states in this region are thermodynamically\r\nunstable.\r\nThe chemical equilibrium condition for coexisting phases is given\r\nby:\r\n  \n  \n\r\nUsing the pressure definition:\r\n  \n  \n\r\nwe evaluate the integral:\r\n  \n  \n\r\nThis result is the Maxwell construction, whose\r\nessence is the enforcement of chemical potential equilibrium.\r\nGeometrically, it requires the area of Region I to equal that of Region\r\nII in the figure below:\r\n  \n \n \n \nMaxwell Construction in  Curve \n\r\n2.8.5 Latent Heat\r\nDuring first-order phase transitions like boiling or melting, systems\r\nabsorb or release significant energy while maintaining constant\r\ntemperature— a phenomenon most familiar when water boils at\r\n and remains at that temperature until fully vaporized.\r\nThis hidden energy exchange, termed latent heat\r\n(denoted ), breaks or forms molecular bonds rather than\r\nincreasing thermal motion.\r\nFor a system of  particles, the total latent heat is\r\nquantified as \n\n\r\nwhere  represents the entropy difference per particle\r\nbetween phases—for example, vaporizing water requires approximately\r\n to overcome intermolecular forces. Crucially, latent\r\nheat equivalently equals the enthalpy difference\r\n between coexisting phases, expressed as ,\r\na relationship arising directly from the equality of chemical potentials\r\n() at phase equilibrium.\r\nThe coexistence of phases in the temperature-pressure\r\n() plane is defined by the chemical potential equality\r\n, describing an intersection curve of two surfaces in\r\n space where phases are equally stable; elsewhere, the\r\nphase with lower  dominates as it minimizes the Gibbs\r\nfree energy. When tracing small displacements along this coexistence\r\ncurve (), the preservation of chemical equilibrium\r\n leads to the Clapeyron equation:\r\n  \n  \n\r\nwhere  is the specific volume change per particle. This\r\nequation reveals how coexistence pressures shift with temperature: for\r\nmost substances, volume expands during melting or boiling\r\n(), resulting in a positive slope ();\r\nwater, however, exhibits anomalous behavior where ice melting\r\ndecreases volume ( since liquid water is\r\ndenser), producing a negative slope () that explains why\r\nincreased pressure melts ice at constant temperature—a principle\r\nenabling ice skating. For water boiling at , the values\r\n and  yield , indicating\r\nsubstantial pressure sensitivity near phase boundaries.\r\n2.8.6 Stable and Metastable\r\nThe critical point is the thermodynamic state where\r\ndistinct liquid and gas phases become indistinguishable, characterized\r\nby:\r\n\r\nCritical temperature ()\r\nCritical pressure ()\r\nCritical volume ()\r\n\r\nAt this point, liquid-gas coexistence terminates, surface tension\r\nvanishes, and thermodynamic response functions (e.g., compressibility\r\n) diverge. You can see it clearly in the following\r\nfigure:   \n \n \n \nCritical Point \n\r\nTaking Van der Waals gas for example. It has state equation of\r\n  \n  \n\r\nwhere  is the volume per particle. For temperatures\r\nbelow ,  curve is not monotonic, which\r\nmeans there’re concave regions. This region indicates \r\n(instability). The boundary of phase transition is constructed by\r\nMaxwell construction. The critical point is determined by\r\n  \n  \n\r\nSolving this for Van der Waals gas, the result is   \n  \n\r\nWe need to illustrate the elements in  figure.\r\n  \n \n \n \n Diagram after Maxwell Construction \n\r\nWe must relate this figure to the Helmholtz free energy double\r\ntangent line. Point O and D are the phase transitions boundary,\r\ncorresponding to the tangent points of . Line OD is the\r\nprojection of constructed double tangent line. Curve OMFD is the\r\nprojection of  before construction. Obviously, with\r\ntemperature growing, skrewed point M and F gradually converges to their\r\nmiddle point K and finally merged to one point at critical temperature\r\n.\r\nWhat still puzzles us is the meaning of point M and F. Notice that\r\nbetween M and F is the unstable region we mentioned above. so M and F is\r\nthe boundary of instability regions. The segments OM and FD correspond\r\nto metastable states, where the system resides in a\r\nlocal minimum of the Helmholtz free energy. These states satisfy local\r\nstability conditions () but are thermodynamically\r\ninferior to the phase-separated state on the Maxwell line. Classic\r\nexamples include:\r\n\r\nSupercooled water: Liquid persists below  (e.g.,\r\ndown to ) due to kinetic barriers inhibiting ice\r\nnucleation.\r\nSuperheated water: Liquid exists above  (e.g., up\r\nto  at high pressure) without boiling.\r\n\r\nThese metastable states are sensitive to perturbations:\r\n\r\nNucleation triggers collapse: Introduction of impurities,\r\nvibrations, or density fluctuations drives the system toward the global\r\nminimum—spontaneously phase-separating along the Maxwell line\r\nOD.\r\nPhysical mechanism: Local free energy minima (OM/FD) are\r\nseparated from the coexistence line (OD) by energy barriers; once\r\novercome, the system releases latent heat and achieves equilibrium via\r\nphase separation.\r\n\r\nNow we will have to calculate the exact boundary line on\r\n plane. We will use a method called asymptotic\r\nexpansion. Still take Van der Waals gas,\r\nTo analyze behavior near the critical point, we define reduced\r\nvariables:\r\n  \n  \n\r\nExpanding the Van der Waals equation around the critical point\r\ngives:\r\n  \n  \n\r\nThis governs liquid-gas coexistence for .\r\nThe coexistence volumes  (liquid) and \r\n(gas) satisfy:\r\n\r\nEqual pressure: \r\nEqual chemical potential:\r\n\r\nMaxwell area rule:\r\n  \n  \n\r\n\r\nAssume asymptotic expansions (square root to avoid pole point):\r\n  \n  \n\r\nCombining the equations above and matching orders of \r\nyields:\r\n  \n  \n\r\nApplying the area condition determines :\r\n  \n  \n\r\nThus, the coexistence volumes are:\r\n  \n  \n\r\nand the coexistence pressure is:\r\n  \n  \n\r\nThe derivative defines stability:\r\n  \n  \n\r\nThe spinodal points  (where\r\n) mark stability limits:\r\n  \n  \n\r\nHere is a summary:\r\n\r\nStable liquid: \r\nStable gas: \r\nMetastable supercooled liquid:\r\n\r\nMetastable superheated vapor:\r\n\r\nUnstable (spinodal) region:\r\n\r\n\r\n\r\n\r\n\r\nStability diagram for  (horizontal line: Maxwell\r\nconstruction)\r\n\r\n\r\n2.8.7 Gibbs Phase Rule\r\nIn this section we will research multi-component systems. For\r\nmulti-component systems, differential equation of energy is expanded to\r\n  \n  \n\r\nGibbs free energy  is especially useful here. Its\r\ndifferential form is   \n  \n\r\nThis equation gives   \n  \n\r\nCommutativity of partial derivatives then leads to\r\n  \n  \n\r\nInspecting formulas above, we may find a constraint of chemical\r\npotential:   \n  \n\r\nSo   \n  \n\r\nNow suppose we have a system with  chemical components\r\nand  coexisting phases. Coexisting phases means the two\r\nphases have macroscopic, observable boundary. For example, solid ice,\r\nliquid water, soild alcohol are three coexisting phases.\r\n\nWe assume the temperature  and pressure  are uniform and fixed throughout the system,\nas required by equilibrium.\n\r\n\nTherefore, the only remaining intensive\nvariables needed to characterize each phase are the relative compositions. \nWe denote  as the mole fraction of component  in phase . \nHence we have\n\n\nBecause of this constraint, \neach phase has only  independent variables. \nSo across all  phases, \nthere're  indepndent variables. \nMeanwhile, \nbesides the fractions, \nthe chemical potential is also related to  and . \nSo  has degree of freedom (DOF) of . \n\nNext, equilibrium requires that all chemical potentials must be equal: \n\n\nThis gives  equations, reducing DOF of . \nThen the total DOF turns out to be\n\n\nThis is called **Gibbs Phase Rule**, \ntelling us the relationship between components number and coexisting phase number. \n\nThe DOF indicates how many parameters can be adjusted while\nremaining the current state (both  and ) unchanged. \nTo determine the maxima of coexisting phase number, \nwe should impose  to be 0, and get\n\n\nFor example, we have a mixture composed by water and alcohol. \nBy Gibbs phase rule, \nthere're at most 4 phases coexisting. \nThey may be liquid water, gas water, liquid alcohol, gas alcohol. \nIf we reduce the temperature to freeze water, \ngas alcohol will disappear. \n\n\r\n2.8.8 Chemical Reactions\r\n\n\nConsider a chemical reaction involving  components:\n\n\nwhere  denotes the -th chemical species and  is its stoichiometric coefficient.\n indicates products and  for reactants. \nLet  denote the number of moles of species . \n\nWe define the reaction coordinate . \nIt is given by \n\nThe mole number of each species can then be written as\n\nwhere  is the initial amount of species i before the reaction proceeds.\n\nUnder fixed , \nGibbs energy is written as \n\n\nDifferentiate both sides, we get\n\n\nThe second term vanishes because\n\n\nSo we can represent the Gibbs free energy with total particle number change:\n\n\nwhere  is called the Gibbs free energy of reaction. \n\nThis is a quantity that indicates the extent of the reaction \n(because  increases during the reaction). \nYou can consider it as the derivative of  to the extent of the reaction. \nIn our direct sense, \nwhen the reaction reaches equilibrium, \n.\nWe will prove it next, using the second thermodynamics law.\n\nConsider a typical canonical ensemble system, \ncomposed by a subsytem and a reservoir. \nThe total entropy change must not be negative:\n\n\nSuppose the system is perturbed and the macroparameters changed a small value. \nwhere its energy and volume change by  and . \nThe reservior, correspondingly, changed  and . \nThen their change of entropy can be written as \n\n\nSince  are fixed, the total entropy change can then be written as\n\n\nThis gives .\n\nTherefore, \n\n\n\nThis implies that the reaction proceeds in the direction that \nreduces . \nAt equlibrium, \n\n\nThus, the system minimizes its Gibbs free energy at fixed , \nand total atom numbers.\n\nIf , which means the reaction goes forward, \nwhereas if  , which means the reaction goes backward. \n\nIn practice, the stoichiometric equation suggests that the reaction\ncould proceed until one of the components is fully consumed. \nHowever, this is not thermodynamically possible.\n\nAs the mole number of any species , \nthe chemical potential typically diverges:\n\nfor ideal gases and ideal mixtures. This means that as the number density of\na reactant decreases, the chemical potential becomes increasingly negative,\ncreating a divergent chemical force that opposes further reaction.\n\nTherefore, the system can never actually reach a state where \nfor any component. Instead, the reaction slows and stalls asymptotically\nas depletion is approached.\n\n\n\nSimilarly, we can also define  and . \nThe partial molar enthalpy and partial molar entropy are defined as\n\n\nThese two variables play the same role as chemical potential  in the \nconstruction of Gibbs free energy.\n\n and  are all homogeneous functions about , \n\n\nTaking derivatives about  for both sides, we get\n\n\nRecall , we further find\n\n\nThen like what we did for chemical potential and Gibbs free energy, \nwe can define similar quantities for  and :\n\n\nand naturally get\n\n\nLet us assume that the reaction is thermodynamically reversible. \nConsider an infinitesimal step of chemical reaction with .\nThe heat of reaction, i.e., the heat absorbed by the system during the\nreaction step, is given by\n\n\nIf we further assume that the system is very close to equilibrium, \n vanishes. \nThen heat reduced to\n\n\nThe law of mass action provides a quantitative relationship between\nthe chemical potentials of the reacting species and their concentrations (or\nactivities) at chemical equilibrium. It is an expression of the condition\nthat the Gibbs free energy is minimized, or equivalently, that the chemical\npotentials are balanced across the reaction.\n\nThe chemical potential of ideal gas is:\n\nwhere  is the de Brogile thermal wavelength. \nLet us choose  as an arbitrary reference pressure, \nand defined  as\n\n\nThen \n\n\nwhere  is the concentration (number density).\n\nWe define activity :\n\n\nThen the chemical potential takes the unified form\n\n\nNear equlibrium, \n\n\nWe reassign this equation:\n\n\nFurther define equilibrium constant:\n\n\nNote that the equilibrium constant is independent of the duration of reaction. \nThis constant is defined on reference state. \n\nThere is another similar quantity, \nbut defined everywhere during an reaction, \ncalled reaction quotient:\n\n\nThey look the same, but is in fac quite different. \nTo clarify, we use :\n\n\n\nSince  at equilibrium, \nwe can see that  matches this condition.\n\n\r\n2.9 Second Order Phase\r\nTransition\r\n2.9.1 Ising Model\r\nA phase transition occurs when a material’s macroscopic properties\r\nchange abruptly as an external parameter, like temperature, is varied.\r\nIn a first-order phase transition, exemplified by phenomena like boiling\r\nwhere density changes suddenly, specific thermodynamic quantities\r\nexhibit a discontinuous jump at the transition point. Second-order phase\r\ntransitions, also termed continuous phase transitions, present a\r\ndifferent picture. Here, a fundamental physical quantity known as the\r\norder parameter is central. The order parameter\r\nquantitatively measures the degree of order or broken symmetry within\r\nthe material’s structure.\r\nSpecifically, the order parameter takes on a non-zero value in the\r\nphase exhibiting long-range order, typically at lower temperatures,\r\nsignifying the established order, such as spontaneous magnetization in a\r\nferromagnet. As the temperature approaches the critical temperature in a\r\nsecond-order transition, this order parameter diminishes continuously\r\nand smoothly, finally reaching zero in the disordered, high-temperature\r\nphase. Crucially, while the order parameter itself evolves continuously\r\nto zero in a second-order transition, its derivatives with respect to\r\nexternal parameters like temperature – quantities such as susceptibility\r\nor specific heat – typically exhibit discontinuities or even diverge at\r\nthe critical point. Therefore, the defining characteristic\r\ndistinguishing first-order from second-order phase transitions lies in\r\nthe behavior of the order parameter: a discontinuous jump signals a\r\nfirst-order transition, whereas a continuous decrease to zero signifies\r\na second-order transition.\r\n\nIn this chapter, \nwe use iron as a example of second order phase transitions. \nIron exhibits ferromagnetism below its critical Curie temperature, \nwhere spontaneous magnetization emerges, \nwhile becoming paramagnetic above this temperature. \nThe fundamental order parameter characterizing this transition is the magnetization density, \ndenoted as .\n\nConsider a macroscopic iron sample in thermal equilibrium with a heat bath at temperature . \nTo construct a tractable statistical model, \nwe partition the system into  cubic lattice cells. \nEach cell contains a thermodynamically significant ensemble of atoms - \nsufficiently large that statistical averaging becomes valid, \nyet small enough to preserve spatial resolution. \nWithin each cell , \nwe define the coarse-grained magnetization  as the volume-average of atomic moments:\n\n\n\r\nCoarse-graining \nprocedure effectively reduces the complex many-atom system to a lattice representation. \nCrucially, in the ferromagnetic phase (), two key physical effects emerge: \n\r\n\r\nLocal alignment: Exchange interactions drive\r\nnear-perfect parallel alignment of atomic moments within each\r\ncell\r\nSpontaneous symmetry breaking: The entire cell\r\nspontaneously selects a preferred magnetization direction\r\n\r\n\nThese collective phenomena justify representing each coarse-grained unit by a binary spin variable , \nwhere:  \n\n-  corresponds to macroscopic \"spin up\" orientation  \n  \n-  represents macroscopic \"spin down\" orientation  \n\nThe discrete  description captures the essential physics of broken symmetry while neglecting subleading fluctuations. \nThis simplification preserves universal critical behavior near  and enables powerful analytical and computational approaches to phase transition modeling.\n\nAfter coarse graining, we will introduce **Ising model**. \nIts diagram is shown below:\n\n\n\n\nDiagram of Ising Model\n\nThe Ising model assigns binary spins  to each sube of lattice. \nSpins interact with their nearest neighbor spins and the external field. \nThis give Hamiltonian:\n\nHere  indicates ferromagnetic coupling and  indicates anti-ferromagnetic.\n is the external magnetic field.\nBecause  can only take , \nif more than 3 spins are considered, \nthe equation will finally collapsed to a two-body term, \nwhich is equivalent to two-body interaction.\nMeanwhile it has a trnaslational symmetry, \nwhich means the Hamiltonian and physical properties remain invariant \nunder any discrete shift of all spins by one lattice site due to the \nuniform lattice structure and identical interactions.\n\nUsing canonical ensemble, \nwe can write the partition function:\n\nwhere\n\n\nDefine Helmholze energy:\n\nThermodynamic observables are obtained as derivatives of . For example,\nthe magnetization per site is:\n\n\nMeanwhile, check the Helmholtz free energy, \ntake derivative about external field :\n\n\n is in fact the probability distribution, \nsince we are in canonical ensemble. \nHence, the derivative above turns out to be\n\n\nThen magnetization  can be represented with derivative of :\n\n\nThe magnetic susceptibility, \nmeasuring the response to external field, is:\n\n\nThis equation indicates that the magnetic susceptibility \ndepends on the fluctuation of total magnetization.\n\nThe two-point correlation function measures the extent to which spins\nat different locations are correlated:\n\n\nIn a trnaslationally invarant system, \n, \ncalled correlation function. \nThen the magnetic suspectivity is given by\n\n\nLet  be a trial probability distribution over spin configurations \n. \nRecall that the non-equilibrium Helmholtz free energy functional is defined as\n\n\nTo minimize , take variation, \n\n\nAt equilibrium, we have\n\n\nWe can use this\ninequality to find a variational approximation to the equilibrium state and\nthe equilibrium free energy.\n\nNext is the key step:\n\r\nWe replace the actual fluctuating interactions on a spin with\r\nan effective field generated by the average magnetization of its\r\nneighbors.\r\n\nBased on this assumption, \nprobabilities of each spin are decoupled and become independent. \nThen the joint distribution becomes simple product of each probabilities:\n\nWe suppose , \nfor it has translational symmetry. \nFor the latter, spins in neighboring sites are anti-parallel to each other, and\nhence we would have to introduce two different probability distributions for\ntwo sublattices.\nFor a single spin, \nwe have two constraints:\n\n\nSolving this, we get\n\n\nThen, with some derivation, \nwe can find the condition to minimize :\n\nwhere z is the coordination number, \nmeaning how many spins there are next to one specific spin.\n\nBecause \n\nWe have\n\n\n\nAnd\n\n\nThen\n\n\nThis is the condition of minimizing . \nUnfortunately, we can't find an analytic solution for this equation. \nLet's check a special situation with . \nThen equation becomes\n\n\nWe can research equation  instead. \nWhen ,  has 3 solutions; \nand when ,  has only 1 solution. \nObviously  here. \nWe can see the critical situation is when . \nSo we can easily find the critical temperature.\n\n\nWhen the equation yields three solutions, \ntwo symmetric non-zero solutions emerge. \nAs temperature decreases, \ninitial fluctuations determine which branch the system selects, \nleading to spontaneous symmetry breaking.\n\r\n2.9.2 Scaling Hypothesis\r\n\nIn second-order (continuous) phase transitions, \nthe system's symmetry changes, \nand critical phenomena emerge near the critical point. \nThese phenomena—including power-law divergences, \nscaling laws, universality, \nand fractal behavior—arise from the divergence of the correlation length  and correlation time. \nCritical phenomena are characterized by a set of critical exponents, \nwhich describe how physical quantities behave as power laws in the reduced temperature \n (where  is the critical temperature) or the external field .\n\nTo analyze these behaviors, \nphysical quantities are decomposed into:\n\r\n\r\nA singular (divergent) part: This part diverges or vanishes as a\r\npower law at the critical point.\r\nA regular (smooth) part: This part remains finite and\r\nnon-singular.\r\n\r\nThe critical exponents are defined by the power-law scaling of the\r\nsingular parts. Below are the key power-law relations, illustrated using\r\nthe Ising model as an example.\r\n\r\nSpecific Heat ():\n   \n   The singular part of the specific heat diverges as  when  (i.e., as ). For ,  diverges at ; for , it may exhibit a logarithmic divergence.\n  \r\nOrder Parameter ():\n   \n   Below  (), the spontaneous magnetization (order parameter) vanishes as . This describes how the ordered phase disappears as  approaches  from below.\n  \r\nSusceptibility ():\n   \n   The magnetic susceptibility  (response to an external field) diverges as  near . This indicates enhanced response to perturbations at criticality.\n  \r\nCritical Isotherm ( at ):\n   \n   At  (), the magnetization depends on the external field  as a power law. A large  implies weak response to  near criticality.\n  \r\nCorrelation Function ():\n   \n   The correlation function  describes spatial correlations. \n   The correlation length indicates how far one spin can affect another spin. \n   At criticality (), it decays algebraically as , where  is the spatial dimension. The exponent  quantifies deviations from mean-field decay.\n  \r\nCorrelation Length ():\n   \n   The correlation length diverges as  near , defining the spatial scale over which fluctuations are correlated. This divergence underpins all critical phenomena.\n   \r\n\r\n\nThe exponents  are universal — \nthey depend only on spatial dimension  and symmetry (e.g., Ising class), \nnot microscopic details.\nPhysists have measured them in experiment, \ncurrently you can consider them as a experimental conclusion. \n\nThese power laws provide a foundational framework for understanding critical phenomena in diverse systems, \nfrom magnets to fluids.\n\nTo explain why the power laws hold, \nLandau raises scaling hypothesis. \nScaling hypothesis is the central idea underlying modern theories of\ncritical phenomena. It consists of two main postulates:\n\r\n\r\nSingular part of free energy. Near the critical point, the free\nenergy  can be decomposed into a smooth part and a singular part. \nThe singular part  is assumed to be a generalized\nhomogeneous function of  and :\n\n\n is called the scaling function, \nwhich is smooth except at \n\n\r\nSingle length scale: Close to the critical point, the correlation\r\nlength, becomes the only relevant length scale. This means that second\r\norder transition is because that near the critical point, correlation\r\nlength becomes too large (divergent) that all microscopic scales are too\r\nsmall compared to the correlation length.\r\n\r\n\nSo any scaling for the entire system will not change the properties of the system. \nHence, the singular part of free energy must only depend on temperature  \nand correlation length . \nBy dimension analysis, \nin -dimension space,  must have unit of . \nThen it's natural to find\n\n\nIt turns out this assumption holds only for . \n\nLet us first inspect the implication of the scaling form of the singular free energy:  \n  \nSetting , we require  to ensure singular behavior, leading to:  \n  \nThe heat capacity is derived from the second derivative of the free energy:  \n  \nconfirming  as the heat capacity exponent.  \n\nDifferentiating  with respect to  gives the magnetization:  \n  \nFor  and , spontaneous magnetization requires , yielding:  \n  \n\nAnother derivative gives the susceptibility:  \n  \n\nAt , , leading to:  \n  \n\nFrom the single-length-scale hypothesis  and :  \n  \n\nThe susceptibility-correlation relation  combined with  for  gives:  \n  \n\nThese yield five scaling relations for exponents \n. \nEliminating  gives four experimentally testable relations:  \n\r\n\r\nFisher law:   \r\nRushbrooke law:   \r\nWidom law:   \r\nJosephson law:   \r\n\r\nTherefore only two of these exponents are independent.\r\n2.9.3 Landau Free Energy\r\n\nLet us go back to the Ising Hamiltonian\n\n\n\nand the canonical partition function as well as the free energy:\n\n\nSince , \nwe now carry out the sum over spin configurations in Eq. (36) in two steps:\n\r\n\r\nFirst, sum over all spin configurations with fixed\r\n.\r\nThen sum over all allowed values of .\r\n\r\n\nThe partition function  can then be written as:\n\n\nwhere  is the partition function of a sub-ensemnle which  is fixed.\n\nThis ensemble keeps the total magnetization  fixed and sums over microstates compatible with that constraint.\n\nThe Landau free energy can then be defined:\n\n\nAnd the free energy in the total  ensemble is given by\n\n\nIn the thermodynamic limit, both  and  are extensive quantities.\nHence the sum over M is dominated by only one term: the value of\n that maximizes the exponent. \nBecause this term exceeds by far than other terms, even their summation. \nThis operation is called \r\nsaddle point approximation .\n\nThen \n\n\nTo find  value in this situation, denoeted as \n\nThis equation in fact gives the state equation. \n\r\n2.9.4 Landau Theory of Phase\r\nTransitions\r\n\nWe define free energy per spin and Landau free energy per spin:\n\n\nAccording to section 2.9.3, \n\n\nIt is Landau's idea to expand  as a Taylor series of :\n\n\nOdd terms vanish because we require energy must remain unchanged \nwhen  changes its direction (symmetry).\nIn the equation above, \n is caused by the background, \n changes its sign near . \n\nNear critical point, \nwe can introduce reduced temperature  \nand  to simplify.\n\r\n\r\nWhen , , \nand we can ignore the quartic term in the Landau free energy because .\n\n\nThe minimum occurs at , \nand substituting back gives:\n\n\nEvidently,  and  are respectively the regular part and \nthe singular part of the free energy. \nWe need to cast the singular free energy into the scaling form.\n\n\nThe equation above gives . \nTo balance coefficient, \n\n\nThis gives \n\nHowever,  defines the behavior of . \nFrom the exact form of , \nwhen , sigular part vanishes. \nThen we  can only have one solution:\n\nNote that  converges under such parameters configuration. \nThis is in fact the limitations of Landau's theory. \n\r\nWhen , \nfrom the analysis above,  is always the regular part of the free energy. We may therefore ignore it in the Legendre transform and focus directly on the singular free energy. For , the singular free energy is derived by minimizing:  \n\n  \n\nTo extract the scaling form, rescale the variables. Define the dimensionless magnetization and field:  \n\n  \n\nSubstituting into the free energy expression gives:  \n\n  \n\nwhere  is a dimensionless function determined by the dimensionless minimization problem. \nThis recovers the scaling form:  \n\n  \n\nconfirming the critical exponents:  \n\n  \n\nOther exponents can also be calculated by Landau's function.\n\r\nSpontaneous magnetization exponent :\nAt  and , minimization of the Landau free energy gives:\n\nThus, .\n\r\nCritical isotherm exponent :\nAt , minimization yields:\n\nHence, .\n\r\nSusceptibility exponent :\nFor  (), the linear response gives:\n\nThus, .\n\r\nCorrelation length exponent  and correlation function exponent : \nUsing the Gaussian approximation of the Landau-Ginzburg functional:\n\nThe correlation function satisfies:\n\nwith solution:\n\nThis implies . The short-distance behavior  corresponds to  (since  implies ).\n\r\nVerification via Fisher relation: The consistency is confirmed\r\nby:\n\n\r\nSummary of mean-field exponents:\nAll critical exponents in Landau theory are:\n\nThese satisfy the scaling relations and are exact for spatial dimensions .\n\r\n\r\nPart III. Quantum\r\nStatistical Mechanics\r\n3.1 Fermions and Bosons\r\n3.1.1 Fundamental Properties\r\nFermions—encompassing electrons, protons, neutrons, and quarks—are\r\nquantum particles with half-integer spin\r\n whose wavefunctions exhibit\r\nantisymmetry under particle exchange. This defining\r\nproperty arises from the spin-statistics theorem and fundamentally\r\nshapes their behavior:\r\n- Fermionic antisymmetry: The wavefunction reverses\r\nsign when any two particles are exchanged:\r\n\n    \n   - Pauli exclusion principle: Fermions\r\nresist occupying identical quantum states, leading to spatial exclusion\r\nand degeneracy pressure in neutron stars and white dwarfs.\r\nBosons—including photons, gluons, and Higgs\r\nbosons—possess integer spin \n and exhibit\r\nsymmetric wavefunctions under particle exchange:\r\n- Bosonic symmetry: The wavefunction remains invariant\r\nunder particle exchange:\r\n\n    \n   - Bose enhancement: Multiple bosons\r\ncongregate in low-energy states, enabling phenomena like\r\nsuperconductivity and Bose-Einstein condensation.\r\nQuantum particles also obey principle of\r\nindistinguishability fundamentally distinguishes\r\nquantum statistics from classical mechanics. While classical particles\r\nmaintain identity through trajectories, quantum particles lose\r\nindividuality—their wavefunctions coalesce into collective states. The\r\npermutation operator  exchanging particles\r\n and  embodies this principle:\r\n\n  \n Crucially in 3+1 dimensions, \r\nsatisfies:\r\n\n  \n requiring physical states to transform as:\r\n\n  \n This phase constraint crystallizes the\r\nspin-statistics theorem:\r\n\r\nFermions () inhabit antisymmetric\r\nwavefunctions\r\nBosons () populate symmetric\r\nwavefunctions\r\n\r\nIf there’re two particles in one system, fermions are constrained by\r\nPauli exclusion principle while bosons don’t:\r\nFermions:\r\n\n  \n This state vanishes when\r\n, manifesting the Pauli exclusion principle that prevents\r\nfermions from sharing quantum states.\r\nBosons:\r\n\n  \n When , this state\r\nreinforces itself, doubling the amplitude for bosons to\r\ncoalesce in identical configurations.\r\nAnd now we expand the system to  particles. The whole\r\nstate space becomes products of  Hilbert spaces, called\r\nFock space: \n  \n\r\nIn this Fock space, joint state of all particles can be represented\r\nas derminant or permanent.\r\nFor fermions: \n  \n The determinant’s antisymmetry ensures\r\nwavefunction sign-flips under particle exchange while automatically\r\nenforcing exclusion when states coincide.\r\nFor bosons: \n  \n\nWhere  is to replace all negative sign with positive.\n Summing over all permutations\r\n constructs a wavefunction invariant under particle\r\nexchange, allowing unlimited occupation of single-particle states.\r\nThis symmetry dichotomy orchestrates quantum many-body phenomena—from\r\nthe stability of matter enforced by fermionic exclusion to the coherent\r\nquantum phases enabled by bosonic condensation.\r\n3.1.2 Second Quantization\r\nSuppose we have six particles distributed among single-particle\r\nstates labeled by momentum or energy eigenstates \n\n\r\nImagine:\r\n\r\nParticle 1 in state \r\nParticle 2,3 in state \r\nParticle 4,5,6 in state \r\n\r\nThe corresponding product state is: \n\n\r\nand we must then symmetrize (for bosons) or antisymmetrize (for\r\nfermions) the product state, which is tedious and cumbersome. However,\r\nat the end of this process, we only care about the number of particles\r\nin each single-particle state. In this case, we combine the same states\r\nand denote only occupation numbers: \n\n\r\nwhere  represents number of particles on state\r\n. For example, the configuration above will be denotes as\r\n\n\n\r\nFor bosons,  can take any non-negative integers.\r\nHowever, fermions must not violate Pauli. So  of fermions\r\ncan only take 0 or 1.\r\n3.1.3 Creation and\r\nAnnihilation Operators\r\nWe are now ready to introduce the natural operators that act on these\r\nstates ( |n_1, n_2, : ) creation and annihilation operators. These\r\noperators do exactly what their names suggest:\r\n: creates a particle in the single-particle state \r\n: annihilates a particle from the state \r\nTheir action on the occupation number basis is defined as\r\nfollows:\r\n\r\n\r\nThese definitions ensure the proper normalization of states and\r\npreserve the orthonormality of the Fock basis.\r\nThe algebra of these operators depends on the statistics of the\r\nparticles. Traditionally we use b̂, b̂† for\r\nbosons, and f̂, f̂† for\r\nfermions. They have the following properties:\r\n\r\nFor bosons:\r\n\n      \n    \n    \r\nFor fermions:\r\n\n    \n    \n    \r\n\r\nHere,  denotes the commutator, and  the\r\nanticommutator. We further define number operators:\r\n for Bosons\r\n for Fermions\r\nTo verify, just operate this operator on a state , you\r\nwill finally get a eigenvalue of n.\r\nWe can easily show the following commutation relations:\r\n* For Bosons:\r\n\n     \n    \n     * For Fermions:\r\n\n      \n    \n    \r\nThese operators provide a powerful and elegant language for quantum\r\nmany-body systems. They:\r\n\r\nSimplify the construction of many-body states and\r\noperators\r\nEncode particle statistics algebraically\r\nConstitute the foundation for quantum field theory and quantum\r\nmany-body theory\r\n\r\nFor example, consider the operator:\r\n\r\nwhich is entirely natural in the second quantized language. It is a\r\nlinear operator on Fock space, and when applied to the vacuum, it yields\r\na valid physical state:\r\n\r\nWe can use creation operators to construct the occupation number\r\nbasis states from the vacuum state: \n\n\r\n3.2 Free Fermi Gas\r\n3.2.1 Thermodynamic Properties\r\n\nFor a macroscopic free Fermi gas, \nthe vast number of particles necessitates the use of second quantization to represent the system's state; \nthis formalism inherently incorporates de Broglie's wave-particle duality, \nwhere particles exhibit both wave-like character (plane waves) and granularity. \nTo determine the total energy of this non-interacting gas, \nwe use the second-quantized Hamiltonian\n\nNote that each  has more than 1 states, \nfor we must take spins into consideration. \nIn general, with spin of , \neach energy contains  states. \nThe  is called the degeneracy.\n\r\n\nBefore proceeding, it is important to understand when quantum effects\nbecome important for a gas. At high temperatures or low densities, the gas\nbehaves classically, and quantum statistics can be ignored. However, when\nthe thermal de Broglie wavelength \n\nbecomes comparable to the mean interparticle spacing, \nthe thermal wave packets of individual particles overlap significantly, \nand hence quantum effects can no longer be neglected. \nThe mean inter-particle spacing  is roughly , \nwhere  is the particle number density. Therefore quantum degeneracy\nsets in when\n\nThis finally gives the critical temperature where \nquantum effect cannot be simply neglected. \nFor fermions, it is called Fermi temperature. \nOr, in this part,  is normalized to 1, \nso temperature and energy have the same unit. \nThis value can also be Fermi energy:\n\nWhen , quantum effects play en important role, and vice versa.  \n\r\n\nWe continue our research on Fermi gas with grand canonical ensemble \nbecause the particle number does not remain constant. \nIn a quantum system, \nthe grand partition function is calculated by trace:\n\nwhere . \nThe trace takes all possible eigenvalues of  and . \nAnd for fermions, \n. \nHence, \n\nSo, \n\nPluging in  will get the probability of  taking 0 and 1.\nWe can now compute the average occupation number of a given single particle\nstate .\n\n\r\nThis is called the Fermi-Dirac distribution.\r\n\nFurthermore, we can obtain the grand canonical potential\n\nSum over the spins to simpltfy:\n\nUsing the grand potential, we can derive all thermodynamic quantities:\n\r\n\r\nThe average particle number:\n\n\r\nThe internal energy:\n\n\r\nThe pressure:\n\n\r\n\r\n3.2.2 Density of States (DOS)\r\n\nIn the thermodynamic limit, \nwhere the volume  becomes very large, \nthe allowed momenta  form a dense set. \nIt is then convenient to replace the sum over discrete \n with an integral over continuous momentum space:\n\nwhere  is the volume per discrete \n-point in -space \n(because the spacing between adjacent -points in each dimension is \n under periodic boundary conditions, \nmaking the volume per state in -space \n. \nThe reciprocal of this gives the prefactor).\n\r\n\nWhen considering energy-dependent physical quantities \n(e.g., calculating the partition function, \ntotal energy, or heat capacity in statistical mechanics), \nit is further useful to introduce the concept of the density of states (DOS). \nThe density of states  is defined as the number of states per unit energy interval, \nmeaning  represents the number of states with energy between \n and . \nThis allows us to convert the momentum integral into an energy integral, \nsimplifying calculations.\n\r\n\nTo derive , \nwe utilize the properties of isotropic systems (like the free electron gas), \nwhere the energy  depends only on the magnitude of the wavevector , \nvia the relation . \nStarting from the momentum space integral, \nthe differential number of states is written as:\n\nIn spherical coordinates, \n. \nIntegrating over the angular part gives:\n\nthus:\n\nTransforming variables using the energy-momentum relation:\n\nso:\n\nSubstituting into :\n\nThe density of states is defined as , hence:\n\nThis is often written in the simplified form:\n\nUsing the density of states, any sum over  (if the summed term depends only on the energy ) can be transformed into an integral over energy:\n\nThis is highly efficient when dealing with thermodynamic quantities \n(like the Fermi-Dirac distribution for an electron gas), \nas it reduces the dimensionality of the integral and highlights the energy dependence.\n\r\nWith the density of states we can rewrite the thermodynamic\r\nquantities:\r\n\r\nGrand potential \n  \n\r\nParticle number \n  \n\r\nInternal energy \n  \n\r\n\r\n\nWe can integrate  by parts and finally show\n\nwhich supplies an exact relation between the pressure and the internal\nenergy density:\n\n\r\n3.2.3 Zero Temperature Limit\r\n\nWhen the temperature approaches 0, \nthe Fermi-Dirac distribution function reduces to a unit-step function about energy . \n\n\nAnd the chemical potential at  is defined as Fermi energy\n\nTherefore, at , all single-particle states with energy less than  are\noccupied, and all states with energy greater than  are empty.\nThis configuration forms a Fermi-sphere in  space, \nwhose radius is , making the states on the surface to be .\n\n\n\n\nFermi Sphere, \n\n\nThen the particle number is given by\n\nSubstitude in the density of states in the last section, \n\nSolving for , \nwe find\n\nFor electrons, , \nso the  calculated from zero-temperature limit \nagrees with that calculateed from de Brogile wavelength.\n\r\n\nWe may rewrite the density of states using Fermi energy:\n\nThe internal energy becomes\n\nCombine the equations above, we find\n\nwhich says that at zero temperature, average energy per particle is proportional\nto the Fermi energy.\n\r\n\nFinally, the pressure can be obtained\n\nThus, even at zero temperature, the Fermi gas exerts a nonzero pressure,\nknown as the degeneracy pressure. This pressure is due to Pauli’s exclusion\nprinciple. This degeneracy pressure plays a crucial role in the stability\nof dense astrophysical objects such as white dwarfs and neutrons stars.\nThese stars collapse if their degenerate pressures are not strong enough to\nwithstand the negative pressure due to the gravity force.\n\r\n3.2.4 Low Temperature\r\nSommerfeld Expansion\r\n\nAt low but finite temperatures , the Fermi-Dirac distribution is\nno longer a sharp step function. Instead, it gets slightly smeared around\nthe Fermi energy.\nWe will research this small drift with Sommerfeld expansion.\n\nSuppose we wish to compute integrals of the form\n\nThe key insight enabling the Sommerfeld expansion is to recognize that the derivative of the Fermi-Dirac distribution, , acts as a thermally broadened Dirac delta function centered at  with width . This function, defined as\n\nis normalized to unity and becomes sharply peaked at  as . To exploit this behavior, we integrate  by parts after introducing the auxiliary function . This yields\n\nThe boundary terms vanish: At , ; at ,  exponentially. Thus,\n\nIn the low-temperature limit (), the lower integration limit can be extended to  since  decays rapidly for . Expanding  in a Taylor series around ,\n\nand substituting into the integral gives\n\nThe integrals  (with ) vanish for odd  due to antisymmetry. For even , they evaluate to\n\nwhere the dimensionless integrals are related to the Riemann zeta function:\n\nUsing  and , the first few coefficients are\n\nSince  for , the series becomes\n\nThis expansion is asymptotic, with each term proportional to . The dominance of the Fermi surface () at low temperatures ensures rapid convergence when .\n\r\nBuilding upon the Sommerfeld expansion technique, we now\r\nsystematically examine how key thermodynamic quantities of the\r\ndegenerate Fermi gas evolve at low temperatures. The expansion provides\r\na powerful framework for calculating precise temperature-dependent\r\ncorrections to zero-temperature behavior.\r\n\r\nChemical Potential \nThe fundamental connection between particle number and chemical potential emerges from the density of states and Fermi-Dirac distribution:\n\nwhich simplifies to the constraint equation:\n\nApplying the Sommerfeld expansion with  yields:\n\nSolving perturbatively by writing  and expanding in small  and  gives the chemical potential's temperature dependence:\n\nThis reveals that  decreases quadratically from  with increasing temperature, reflecting thermal excitation of fermions just below the Fermi surface.\n\r\nInternal Energy \nThe internal energy expression:\n\nis evaluated using  in the Sommerfeld expansion:\n\nSubstituting the chemical potential  from above and re-expanding in  produces:\n\nThe quadratic increase in  with temperature contrasts sharply with classical gases, arising from Pauli blocking that restricts excitations to a thin shell near .\n\r\nPressure and Compressibility \nUsing the relation  from the grand potential, pressure inherits the temperature dependence of internal energy:\n\nThe isothermal compressibility  then follows from its thermodynamic definition:\n\nDifferentiating  while noting  yields:\n\nThis shows compressibility decreases with temperature, indicating the gas becomes stiffer as thermal excitations populate states above the Fermi surface.\n\r\n\r\n3.3 Free Bose Gas\r\n3.3.1 Thermodynamic Properties\r\n\nIn this lecture, \nwe study a system of non-interacting massive bosonic particles \nconfined in a volume  at temperature  and chemical potential .\nBosons obey Bose-Einstein statistics, meaning that multiple particles can\noccupy the same quantum state. This is in contrast to fermions, which\nobey the Pauli exclusion principle that prevents more than one particle\nfrom occupying the same quantum state.\n\r\nOur research path is the same as that of fermions.\r\n\nStill, in grand canonical ensemble\n\nThen all thermodynamic quantities are given:\n\r\n\r\nGrand potential \n  \n\nIt is instructive to write  as a product of infinitely many grand canonical\npartition functions for one single-particle state systems:\n\nSimilarly, the grand potential can be written as the sum:\n\nThese results tell us that the Bose gas can be understood as an independent\nsum of infinitely many small open systems.\nThese small open systems are in contact with the same bath with temperature \nand chemical potential .\n\r\nParticle number \n  \n   This is known as\r\nBose-Einstein distribution.\r\n\r\nWe also have \n\nMeanwhile, the density of states for fermions remains correct for bosons, \nfor they are derived in the same way.\n\n\r\n3.3.2 High Temperature Limit\r\n\nAt high temperatures or low densities, \nquantum gases behave classically, \nwith quantum statistical effects appearing as small corrections. \nThe key parameter governing these corrections is the degeneracy parameter \n \nwhere  is the particle density and \n\nis the thermal de Broglie wavelength. \nWhen \n, \nquantum statistics can be treated perturbatively. \nFor a unified description of Bose and Fermi gases, \nwe introduce the statistical parameter :  \n\nThis allows us to express the occupation number distribution and grand potential in a unified form. \nThe Bose-Einstein functions () and Fermi-Dirac functions () \nare defined through integral representations that admit power series expansions in the fugacity :  \n  \nwhich converge for . \n\r\n\nWith density of states, \n can be estimated with integral, \n \nwhere .\n\r\n\nParticle number density and energy density then take the compact forms:  \n  \nwhere  is the spin degeneracy. \nThe equation of state follows as  for both statistics. \nTo derive high-temperature expansions, \nwe solve the particle number equation perturbatively for small . \nInverting the series \n\nyields the fugacity expansion:  \n  \nSubstituting this into the pressure and energy expressions and re-expanding in powers of \n gives the quantum corrections to classical behavior:  \n  \nThe chemical potential expansion follows similarly:  \n  \nThe sign difference in the  terms reveals fundamental statistical behavior: \nbosonic exchange symmetry reduces pressure and energy relative to classical predictions, \nwhile fermionic exclusion increases them. \nThis reflects how quantum statistics modify effective interparticle interactions—bosons exhibit \neffective attraction while fermions exhibit effective repulsion compared to distinguishable particles. \nThe accuracy of these expansions is controlled by the smallness of , \nwhich measures the overlap of thermal wave packets and vanishes in the classical limit.\n\r\n3.3.3 Bose-Einstein\r\nCondensation\r\n\nLet's go back to the Bose-Einstein distribution. \n\nIn order for average occupation number  to be positive, the chemical\npotential must be less than the energy level . \nThis must hold for every single-particle state. Hence we have\n\nwhere  is the single particle ground state.\nFor a free Bose gas confined\nin a box, we have\n\nHence we have\n\nIf , the particle number in the ground state apparently diverges. \nFor a system with finite particle number and finite volume, of course, \nsuch a divergence cannot happen. \nWe will have to invoke other physical conditions to determine the actual number of particles in the\nground state. \nAs we will show next, \nin three dimensions, it may happen that a macroscopic fraction of particles occupy the ground state, \na phenomenon called Bose-Einstein condensation (BEC).\n\r\n\nFor simplicity, we consider spinless bosons, so . \nUsing the expansion, \nwe have\n\nSince , fugacity .\nWhen  increases from 0 to 1, \nBose-Einstein function approaches a finite value, \nrepresented by Riemann Zeta function:\n\nThere is a conflict here, \nConsider a fixed temperature , \nand gradually increase the number of particles in the system.\nThe number density  will gradually reach the satuation point:\n\nHowever, the LHS has no upper boundary while RHS has.\n\r\n\nWe can also fix particle number, \nthe same, \nwe can find a critical temperature .\nfor , the equation has no solution.\n\nCurrently we have no idea why the particle number has a upper boundary. \nThis equation lead to a critical number .\nMaybe we need a leap of faith.\n\r\n\nThe leap is completed by Einstein. \nAt the critical point , we have , so\n. \nBut the Bose distribution tells us the occupation number of the ground state () is\n\nThis divergence is an alarm that we can not really approximate the discrete sum as a continuous integral.\nThe particle number in the ground state must be treated separately. \nThis is where Einstein's genius at play!\n\r\n\nEinstein finally re-explained the integral-approximated part of summation:\n\nIf we use the integral of density of states to estimate the sum.\n\nThe density of states vanishes as . \nMeanwhile the ground states has zero energy. \nHence, the integral does not take the ground state into account.\nThis is how our faith leap:\n\r\nWe consider it as only the particles of excited states\r\ninstead of all bosons, and bosons have finite excited states. When the\r\nparticles exceeds the upper bound of excited states number, no matter\r\nthe energy, the exceeded part must be filled in the ground state. They\r\ncondensed in the ground state and do not contribute to the property of\r\nthe entire system. This phenomenon is called Bose-Einstein\r\ncondensation.\r\n\nUnder this perspective, \nwe can reinterpret the equations above:\n\nThis is the maximum capacity of all excited states. \nUnder this number, \nparticles tend to fill in the excited states.\nBeyond this number, \nother particles must go to the ground state. \n\r\n\nThe total number\n\nAccording to this interpretation, the occupation number of the ground state\nbecomes macroscopic|proportional to the system volume.\n\r\n\nWe can further interpret number density with temperature, \nfor de Brogile wavelength is related to . \nWhen , BEC occurs, then\n\nCorrespondingly, \n\n\nWhen , no BEC, , and the two equations no longer hold.\n\r\nBecause of the condensation, the system is divided into two phases: -\r\nCondensate: A macroscopic number of particles occupy the ground\nstate . These particles have zero kinetic energy and zero mo-\nmentum. As a result, they contribute nothing to the energy, pressure,\nor entropy of the system.\n\r\n\r\nThermal Cloud: All other particles are distributed among excited\nstates with , described by the usual Bose-Einstein distribu-\ntion with . These particles determine all thermodynamic\nproperties energy, pressure, and heat capacity.\n\r\n\r\n\nWhen , the Bose gas exists entirely in the thermal phase with no condensate (). The fugacity , and all thermodynamic properties are governed by the thermal cloud—particles distributed across excited states () according to the Bose-Einstein distribution. For spinless bosons (), the key relations are:\n\r\n\n\n\r\n\nHere, , , and  denote the number density, internal energy density, and pressure of the thermal cloud. To determine the equation of state, solve the first equation for  and substitute into the expressions for  and .\n\r\n\r\nIsothermal Compressibility\nThe thermal cloud exhibits critical behavior near . The isothermal compressibility  quantifies its response to pressure changes at fixed temperature:\n\n\n\nWith  fixed, variations in  and  arise solely from changes in  via :\n\n\n\nUsing the identity , these become:\n\n\n\nIn logarithmic form ():\n\n\n\nThe ratio yields :\n\n\n\nwhere  is fixed by . As ,  and  diverges while  remains finite. Consequently,  diverges:\n\n\n\r\n\r\n\nThis divergence signifies critical fluctuations—a hallmark of continuous phase transitions. The thermal cloud becomes infinitely compressible near , reflecting long-range correlations and heightened sensitivity to external pressure, despite other thermodynamic properties remaining analytic.\n\r\n3.4 Trapped in a Potential\r\n\nThe behavior of quantum gases confined in external potentials exhibits fundamental differences from free-space systems, with harmonic traps  serving as a cornerstone for cold-atom experiments. For bosons in such isotropic traps, the discrete single-particle spectrum  \n  \ngenerates a density of states scaling as , diverging from the free-space  dependence. This restructuring dramatically alters Bose-Einstein condensation (BEC): the critical temperature scales as  \n  \nweaker than the  scaling in uniform systems. Below , the condensate fraction follows  \n  \ndecaying faster than the free-space  law due to the trap’s spectral geometry. The condensate—localized in the ground state—contributes zero energy and momentum, while thermodynamics is governed by the thermal cloud, which exhibits diverging compressibility  near the transition.  \n\r\n\nFor fermions in identical traps, degeneracy emerges when . The Fermi energy at zero temperature,  \n  \nand ground-state energy  reflect the virial theorem’s balance between kinetic and potential energies. At low temperatures , the Sommerfeld expansion yields  \n  \nmirroring free fermions but with rescaled exponents. Confinement redistributes phase space, altering scaling relations—degeneracy pressure now follows  versus  in free space.  \n\r\n\nIn the semiclassical regime , discrete sums converge to phase-space integrals:  \n  \nThis reveals universal scaling laws such as  \n  \nfor -dimensional traps, demonstrating how confinement reshapes thermodynamics while preserving quantum degeneracy phenomena. Cold-atom experiments directly validate these predictions, probing the interplay between quantum statistics and spatial constraints.\n\r\n3.5 Massless Quantum Gas\r\n3.5.1 Density States\r\n  \nMassless bosonic particles such as photons and phonons exhibit a linear dispersion relation , where  is the propagation speed (speed of light for photons, speed of sound for phonons). To derive the energy density of states  in -dimensional space, consider the number of quantum states in a spherical shell of radius  and thickness  in -space. The volume element is proportional to the surface area of a -dimensional sphere:  \n  \nis the surface area of a unit sphere. Substituting the linear dispersion  and  yields:  \n  \nFor photons in three dimensions (), including two polarization states, . For phonons in 3D crystals, accounting for three acoustic branches (two transverse, one longitudinal), .  \n\r\n3.5.2 Blackbody Radiation\r\n  \nThe electromagnetic field in thermal equilibrium is modeled as a gas of non-interacting photons with zero chemical potential. Each mode of frequency  behaves as a quantum harmonic oscillator with average energy . Using the photon density of states, the spectral energy density per unit volume is:  \n  \nIntegrating over all frequencies gives the \r\ntotal energy density , scaling as  (Stefan-Boltzmann law). The pressure  of the photon gas satisfies , consistent with massless relativistic gases.  \n\nThe radiative\r\nenergy flux  emitted by a black surface relates to energy density via , where  is the Stefan-Boltzmann constant. Historically, Planck’s quantization hypothesis resolved the ultraviolet catastrophe in classical electrodynamics and laid the foundation for quantum theory. The cosmic microwave background (CMB), observed at , provides a cosmological validation of blackbody radiation theory.  \n\r\n3.5.3 Lattice Vibration\r\n  \nIn solids, lattice vibrations are quantized as phonons with linear dispersion . The Debye model approximates the crystal as an isotropic continuum with a cutoff frequency  ensuring  modes (for  atoms in 3D):  \n  \nThe internal energy  is:  \n  \nwhere  is the Debye temperature. The heat capacity  exhibits distinct limits:  \n\r\n\r\nAt high temperatures (),  (Dulong-Petit law).  \r\nAt low temperatures (),  due to phonon modes behaving as massless bosons.  \n\nIn metals, low-temperature specific heat combines electronic and phonon contributions: , where  probes the Fermi surface and  reflects phonon properties. The Debye model succeeds by capturing the finite mode density and linear dispersion of acoustic phonons, while Einstein’s model (single-frequency oscillators) fails at low  due to its exponential suppression of .  \n\r\n\r\nFinally submitted on July 1st.\r\nStructure frozen on July 1st.\r\nArchieved on July 1st.\r\n","categories":["physics"]}]